"""
Moteur principal de l'IA personnelle
G√®re l'orchestration entre les diff√©rents modules
"""

import logging
from typing import Dict, Any, Optional, List
from pathlib import Path

from .config import AI_CONFIG
from .conversation import ConversationManager
from models.custom_ai_model import CustomAIModel
from models.conversation_memory import ConversationMemory
from processors.pdf_processor import PDFProcessor
from processors.docx_processor import DOCXProcessor
from processors.code_processor import CodeProcessor
from generators.document_generator import DocumentGenerator
from models.generators import CodeGenerator
from utils.logger import setup_logger
from utils.file_manager import FileManager

class AIEngine:
    """
    Moteur principal de l'IA personnelle
    """
    
    def __init__(self, config: Optional[Dict] = None):
        """
        Initialise le moteur IA
        
        Args:
            config: Configuration personnalis√©e (optionnel)
        """
        self.config = config or AI_CONFIG
        self.logger = setup_logger("AIEngine")
        
        # Initialisation des composants
        self.file_manager = FileManager()
        
        # Initialiser session_context AVANT tout le reste
        self.session_context = {
            "documents_processed": [],
            "code_files_processed": [],
            "last_document_type": None,
            "current_document": None
        }

        # Initialisation de la m√©moire et du gestionnaire de conversations
        self.conversation_memory = ConversationMemory()
        self.conversation_manager = ConversationManager(memory=self.conversation_memory)
        
        # Mod√®le IA local personnalis√© avec m√©moire de conversation (100% autonome)
        try:
            from models.custom_ai_model import CustomAIModel
            self.local_ai = CustomAIModel(conversation_memory=self.conversation_memory)
            self.model = self.local_ai  # Alias pour compatibilit√© avec l'interface graphique
            self.logger.info("‚úÖ Mod√®le IA local avec m√©moire initialis√©")
            # Initialisation du gestionnaire de LLM
            self.llm_manager = self.local_ai  # Pour compatibilit√© avec les fonctions qui utilisent llm_manager
        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors de l'initialisation du mod√®le IA : {e}")
            # Fallback sur l'ancien syst√®me
            self.local_ai = CustomAIModel()
            self.model = self.local_ai
            self.llm_manager = self.local_ai
        
        # Processeurs
        self.pdf_processor = PDFProcessor()
        self.docx_processor = DOCXProcessor()
        self.code_processor = CodeProcessor()
        
        # G√©n√©rateurs
        self.document_generator = DocumentGenerator()
        self.code_generator = CodeGenerator()
        
        self.logger.info("Moteur IA initialis√© avec succ√®s")
    
    def process_text(self, text: str) -> str:
        """
        Traite un texte en utilisant le mod√®le IA local personnalis√©
        
        Args:
            text: Texte √† traiter
            
        Returns:
            R√©ponse de l'IA sous forme de texte
        """
        try:
            self.logger.info(f"Processing text: {text[:50]}...")
            
            # Utilisation du mod√®le IA local 100% autonome
            response = self.local_ai.generate_response(text)
            
            self.logger.info(f"AI model response: {response[:50]}...")
            
            # On sauvegarde l'√©change sans utiliser add_message qui cause des erreurs
            try:
                # Tente d'utiliser add_exchange qui est une autre m√©thode disponible
                self.conversation_manager.add_exchange(text, {"message": response})
            except:
                # Si √ßa √©choue aussi, on ne fait rien - l'important est de ne pas planter
                self.logger.warning("Impossible de sauvegarder la conversation")
            
            return response
        
        except Exception as e:
            self.logger.error(f"Erreur dans process_text: {e}")
            self.logger.warning("Utilisation du fallback response")
            fallback_response = self._generate_fallback_response(text)
            return fallback_response
    
    def _generate_simple_function(self, text: str) -> str:
        """G√©n√®re une fonction simple bas√©e sur la demande"""
        if "factorielle" in text.lower():
            return """üîß Fonction g√©n√©r√©e :

```python
def factorielle(n):
    \"\"\"
    Calcule la factorielle d'un nombre
    
    Args:
        n (int): Nombre dont on veut la factorielle
        
    Returns:
        int: Factorielle de n
    \"\"\"
    if n < 0:
        raise ValueError("La factorielle n'est pas d√©finie pour les nombres n√©gatifs")
    elif n == 0 or n == 1:
        return 1
    else:
        return n * factorielle(n - 1)

# Exemple d'utilisation
print(factorielle(5))  # Output: 120
```"""
        elif "fibonacci" in text.lower():
            return """üîß Fonction g√©n√©r√©e :

```python
def fibonacci(n):
    \"\"\"
    Calcule le n-i√®me nombre de Fibonacci
    
    Args:
        n (int): Position dans la s√©quence
        
    Returns:
        int: n-i√®me nombre de Fibonacci
    \"\"\"
    if n <= 0:
        return 0
    elif n == 1:
        return 1
    else:
        return fibonacci(n-1) + fibonacci(n-2)

# Exemple d'utilisation
for i in range(10):
    print(f"F({i}) = {fibonacci(i)}")
```"""
        else:
            return f"""üîß Fonction g√©n√©r√©e bas√©e sur votre demande :

```python
def ma_fonction():
    \"\"\"
    Fonction g√©n√©r√©e automatiquement
    Bas√©e sur : {text}
    \"\"\"
    # TODO: Impl√©tez votre logique ici
    print("Fonction cr√©√©e avec succ√®s !")
    return True

# Exemple d'utilisation
ma_fonction()
```
üí° Pour une g√©n√©ration plus pr√©cise, d√©crivez exactement ce que doit faire la fonction."""
    
    def _generate_simple_class(self, text: str) -> str:
        """G√©n√®re une classe simple bas√©e sur la demande"""
        if "personne" in text.lower() or "person" in text.lower():
            return """üèóÔ∏è Classe g√©n√©r√©e :

```python
class Personne:
    \"\"\"
    Classe repr√©sentant une personne
    \"\"\"
    
    def __init__(self, nom, age):
        \"\"\"
        Initialise une nouvelle personne
        
        Args:
            nom (str): Nom de la personne
            age (int): √Çge de la personne
        \"\"\"
        self.nom = nom
        self.age = age
    
    def se_presenter(self):
        \"\"\"Pr√©sente la personne\"\"\"
        return f"Bonjour, je suis {self.nom} et j'ai {self.age} ans."
    
    def avoir_anniversaire(self):
        \"\"\"Incr√©mente l'√¢ge d'un an\"\"\"
        self.age += 1
        return f"Joyeux anniversaire ! {self.nom} a maintenant {self.age} ans."

# Exemple d'utilisation
personne = Personne("Alice", 25)
print(personne.se_presenter())
print(personne.avoir_anniversaire())
```"""
        else:
            return f"""üèóÔ∏è Classe g√©n√©r√©e bas√©e sur votre demande :

```python
class MaClasse:
    \"\"\"
    Classe g√©n√©r√©e automatiquement
    Bas√©e sur : {text}
    \"\"\"
    
    def __init__(self):
        \"\"\"Initialise la classe\"\"\"
        self.nom = "MaClasse"
        self.active = True
    
    def action(self):
        \"\"\"M√©thode d'action principale\"\"\"
        if self.active:
            return "Action ex√©cut√©e avec succ√®s !"
        return "Classe inactive"
    
    def __str__(self):
        \"\"\"Repr√©sentation string de la classe\"\"\"
        return f"{self.nom} - Active: {self.active}"

# Exemple d'utilisation
objet = MaClasse()
print(objet)
print(objet.action())
```

üí° Pour une g√©n√©ration plus pr√©cise, d√©crivez les attributs et m√©thodes souhait√©s."""
    
    def _generate_code_from_text(self, text: str) -> str:
        """G√©n√®re du code bas√© sur une description textuelle"""
        return f"""üíª Code g√©n√©r√© bas√© sur votre demande :

```python
# Code bas√© sur : {text}

def solution():
    \"\"\"
    Solution g√©n√©r√©e automatiquement
    \"\"\"
    # TODO: Impl√©tez votre solution ici
    print("Code g√©n√©r√© avec succ√®s !")
    
    # Exemple de logique de base
    resultat = "Mission accomplie"
    return resultat

# Ex√©cution
if __name__ == "__main__":
    print(solution())
```

üí° Pour du code plus sp√©cifique, donnez plus de d√©tails sur ce que vous voulez accomplir."""
    
    def _get_help_text(self) -> str:
        """Retourne le texte d'aide"""
        return """ü§ñ Aide - My AI Personal Assistant

üìù **G√©n√©ration de code :**
‚Ä¢ "g√©n√®re une fonction pour calculer la factorielle"
‚Ä¢ "cr√©e une classe Personne avec nom et √¢ge"
‚Ä¢ "g√©n√®re du code pour lire un fichier CSV"

üìä **Traitement de documents :**
‚Ä¢ Utilisez les boutons pour traiter des PDF/DOCX
‚Ä¢ Glissez-d√©posez vos fichiers

üíª **Analyse de code :**
‚Ä¢ Utilisez le bouton "Process Code" pour analyser vos fichiers

‚ùì **Questions g√©n√©rales :**
‚Ä¢ Posez vos questions en langage naturel
‚Ä¢ Je peux vous aider avec la programmation Python

üí° **Conseils :**
‚Ä¢ Soyez sp√©cifique dans vos demandes
‚Ä¢ N'h√©sitez pas √† demander des exemples
‚Ä¢ Utilisez "aide" pour revoir cette aide"""

    def _generate_fallback_response(self, text: str) -> str:
        """
        G√©n√®re une r√©ponse de fallback en cas d'erreur du mod√®le principal
        
        Args:
            text: Texte de l'utilisateur
            
        Returns:
            R√©ponse de fallback naturelle
        """
        text_lower = text.lower()
        
        # Salutations
        if any(word in text_lower for word in ["salut", "bonjour", "hello", "hi", "bonsoir"]):
            return "Salut ! Comment √ßa va ? En quoi puis-je t'aider ?"
        
        # Questions d'aide
        if "help" in text_lower or "aide" in text_lower:
            return "Je peux t'aider avec la g√©n√©ration de code Python, l'analyse de documents, et r√©pondre √† tes questions techniques. Que veux-tu faire ?"
        
        # Demandes de code
        elif "g√©n√©r" in text_lower or "cr√©er" in text_lower or "fonction" in text_lower or "classe" in text_lower:
            if "fonction" in text_lower:
                return self._generate_simple_function(text)
            elif "classe" in text_lower:
                return self._generate_simple_class(text)
            else:
                return "Je peux g√©n√©rer du code pour toi ! Tu veux une fonction ou une classe ? Dis-moi ce que tu veux cr√©er."
        
        # Questions sur l'IA
        elif any(phrase in text_lower for phrase in ["qui es-tu", "que fais-tu", "qu'est-ce que tu fais"]):
            return "Je suis ton assistant IA local. Je peux coder, analyser des documents et r√©pondre √† tes questions. Qu'est-ce qui t'int√©resse ?"
        
        # R√©ponse g√©n√©rale naturelle
        else:
            return f"Je vois ! Et en quoi puis-je t'aider avec √ßa ? Tu veux que je g√©n√®re du code, que je t'explique quelque chose, ou autre chose ?"

    async def process_message(self, message: str) -> str:
        """
        Traite un message de mani√®re asynchrone
        
        Args:
            message: Message de l'utilisateur
            
        Returns:
            R√©ponse de l'IA
        """
        return self.process_text(message)

    def get_conversation_history(self) -> List[Dict[str, str]]:
        """
        R√©cup√®re l'historique de conversation
        
        Returns:
            Liste des messages
        """
        return self.conversation_manager.get_history()

    def clear_conversation(self):
        """
        Clear the conversation history and memory
        """
        self.conversation_manager.clear()
        self.conversation_memory.clear()
        
        # R√©initialiser aussi le session_context
        self.session_context = {
            "documents_processed": [],
            "code_files_processed": [],
            "last_document_type": None,
            "current_document": None
        }
        
        self.logger.info("Conversation cleared")

    def initialize_llm(self) -> bool:
        """Initialise les mod√®les LLM - Mode entreprise local uniquement"""
        try:
            # En mode entreprise, nous utilisons uniquement le mod√®le local
            return hasattr(self, 'local_ai') and self.local_ai is not None
        except Exception as e:
            self.logger.error(f"Erreur initialisation LLM: {e}")
            return False

    async def process_query(self, query: str, context: Optional[Dict] = None) -> Dict[str, Any]:
        """
        Traite une requ√™te utilisateur
        
        Args:
            query: Question/demande de l'utilisateur
            context: Contexte additionnel (fichiers, historique, etc.)
            
        Returns:
            R√©ponse structur√©e de l'IA
        """
        try:
            self.logger.info(f"Traitement de la requ√™te: {query[:100]}...")
            
            # Analyse de la requ√™te
            query_type = self._analyze_query_type(query)
            
            # Pr√©paration du contexte
            full_context = self._prepare_context(query, context)
            
            # Traitement selon le type
            if query_type == "conversation":
                response = await self._handle_conversation(query, full_context)
            elif query_type == "file_processing":
                response = await self._handle_file_processing(query, full_context)
            elif query_type == "code_generation":
                response = await self._handle_code_generation(query, full_context)
            elif query_type == "document_generation":
                response = await self._handle_document_generation(query, full_context)
            else:
                response = await self._handle_general_query(query, full_context)
            
            # Sauvegarde dans l'historique
            self.conversation_manager.add_exchange(query, response)
            
            return response
            
        except Exception as e:
            self.logger.error(f"Erreur lors du traitement: {e}")
            return {
                "type": "error",
                "message": f"D√©sol√©, une erreur s'est produite: {str(e)}",
                "success": False
            }
    
    def _analyze_query_type(self, query: str) -> str:
        """
        Analyse le type de requ√™te
        
        Args:
            query: Requ√™te utilisateur
            
        Returns:
            Type de requ√™te identifi√©
        """
        query_lower = query.lower()
        
        # PRIORIT√â 1 : V√©rifier d'abord les questions d'identit√©/capacit√©s (AVANT documents)
        identity_keywords = ["qui es-tu", "qui es tu", "qui √™tes vous", "comment tu t'appelles", "ton nom", "tu es qui", "tu es quoi"]
        capability_keywords = ["que peux tu", "que sais tu", "tes capacit√©s", "tu peux faire", "que fais-tu", "comment vas tu", "comment √ßa va"]
        
        if any(keyword in query_lower for keyword in identity_keywords + capability_keywords):
            return "conversation"  # Questions sur l'IA elle-m√™me
        
        # PRIORIT√â 2 : V√©rifier si c'est un texte incompr√©hensible/al√©atoire
        if len(query_lower) > 20 and not any(c.isspace() for c in query_lower[:20]):
            # Si plus de 20 caract√®res sans espaces = probablement du charabia
            return "conversation"
        
        # PRIORIT√â 3 : V√©rifier si on a des documents et si la question les concerne SP√âCIFIQUEMENT
        if hasattr(self.local_ai, 'conversation_memory'):
            stored_docs = self.local_ai.conversation_memory.get_document_content()
            if stored_docs:
                # Mots-cl√©s SP√âCIFIQUES pour questions sur les documents
                doc_question_keywords = [
                    "r√©sume", "explique", "analyse", "qu'est-ce que", "que dit", 
                    "contenu", "parle", "traite", "sujet", "doc", "document", 
                    "pdf", "docx", "fichier", "code"
                ]
                if any(keyword in query_lower for keyword in doc_question_keywords):
                    return "file_processing"
        
        # PRIORIT√â 4 : Mots-cl√©s pour la g√©n√©ration de code (NOUVEAU code, pas analyse)
        # Distinguer entre questions th√©oriques et demandes de g√©n√©ration
        question_words = ["comment", "qu'est-ce que", "c'est quoi", "que signifie", "explique", "expliquer"]
        is_theoretical_question = any(qword in query_lower for qword in question_words)
        
        code_generation_keywords = ["g√©n√®re", "cr√©e", "√©cris", "d√©veloppe", "programme", "script", "fonction", "classe"]
        if any(keyword in query_lower for keyword in code_generation_keywords):
            # Si c'est une question th√©orique (ex: "comment cr√©er une liste ?"), laisser le CustomAIModel s'en occuper
            if is_theoretical_question:
                return "general"  # Laisser le CustomAIModel traiter
            else:
                return "code_generation"  # Vraie demande de g√©n√©ration
        
        # PRIORIT√â 5 : Mots-cl√©s pour la g√©n√©ration de documents
        doc_keywords = ["cr√©er", "g√©n√©rer", "rapport", "r√©diger", "documenter"]
        if any(keyword in query_lower for keyword in doc_keywords):
            return "document_generation"
        
        return "conversation"
    
    def _prepare_context(self, query: str, context: Optional[Dict]) -> Dict[str, Any]:
        """
        Pr√©pare le contexte complet pour la requ√™te
        """
        full_context = {
            "query": query,
            "conversation_history": self.conversation_manager.get_recent_history(),
            "timestamp": self.file_manager.get_timestamp(),
            "user_context": context or {}
        }
        
        # Ajouter les documents stock√©s dans la m√©moire
        if hasattr(self.local_ai, 'conversation_memory'):
            stored_docs = self.local_ai.conversation_memory.stored_documents
            if stored_docs:
                full_context["stored_documents"] = stored_docs
                full_context["document_order"] = self.local_ai.conversation_memory.document_order
                self.logger.info(f"Contexte enrichi avec {len(stored_docs)} documents stock√©s")
        
        return full_context
    
    async def _handle_conversation(self, query: str, context: Dict) -> Dict[str, Any]:
        """
        G√®re les conversations g√©n√©rales
        """
        try:
            # Si on a des documents, les inclure dans le contexte
            prompt = query
            if 'stored_documents' in context and context['stored_documents']:
                prompt = f"""Contexte des documents disponibles:
                
"""
                for doc_name in context['stored_documents'].keys():
                    prompt += f"- {doc_name}\n"
                
                prompt += f"\nQuestion: {query}\n\nR√©ponds en tenant compte du contexte si pertinent."
            
            response = self.local_ai.generate_response(prompt)
            
            return {
                "type": "conversation",
                "message": response,
                "success": True
            }
        except Exception as e:
            self.logger.error(f"Erreur conversation: {e}")
            return {
                "type": "conversation",
                "message": f"Erreur lors de la conversation: {str(e)}",
                "success": False
            }
    
    async def _handle_file_processing(self, query: str, context: Dict) -> Dict[str, Any]:
        """
        G√®re le traitement de fichiers et questions sur les documents
        """
        try:
            query_lower = query.lower()
            
            # D√©terminer quel document sp√©cifique est demand√©
            target_document = None
            all_docs = context.get('stored_documents', {})
            document_order = context.get('document_order', [])
            
            # Log pour debug
            self.logger.info(f"Query: '{query}' | Documents disponibles: {list(all_docs.keys())}")
            
            # Recherche de mots-cl√©s sp√©cifiques au type de document - LOGIQUE RENFORC√âE

            # D√©finir les flags de r√©f√©rence num√©rique AVANT leur utilisation
            is_first_requested = any(term in query_lower for term in ['premier', '1er', '1√®re', 'premi√®re', 'document 1', 'le 1'])
            is_second_requested = any(term in query_lower for term in ['deuxi√®me', '2√®me', '2eme', 'seconde', 'document 2', 'le 2'])
            is_last_requested = any(term in query_lower for term in ['dernier', 'derni√®re', 'last'])
            
            # 1. RECHERCHE EXPLICITE par mention du nom exact du fichier
            for doc_name in all_docs.keys():
                doc_name_clean = doc_name.lower().replace('.pdf', '').replace('.docx', '').replace('.doc', '').replace('.py', '')
                # Construire le prompt avec le document cibl√© - PROTECTION RENFORC√âE
                if target_document and target_document in all_docs:
                    doc_data = all_docs[target_document]
                    if isinstance(doc_data, dict) and 'content' in doc_data:
                        doc_content = doc_data['content']
                    else:
                        doc_content = str(doc_data)

                    # Provide more content for more detailed summaries
                    max_len = 8000
                    self.logger.info(f"üìÑ DOCUMENT S√âLECTIONN√â POUR TRAITEMENT: {target_document}")
                    self.logger.info(f"üìä Taille contenu: {len(doc_content)} caract√®res")

                    prompt = f"""üö® R√àGLE ABSOLUE ET OBLIGATOIRE üö®
 JE TE DONNE ACC√àS √Ä UN SEUL DOCUMENT. TU NE DOIS ANALYSER QUE CE DOCUMENT.
 IGNORE COMPL√àTEMENT TOUTE R√âF√âRENCE √Ä D'AUTRES DOCUMENTS.

 üéØ DOCUMENT UNIQUE √Ä ANALYSER: {target_document}
 ÔøΩ QUESTION: {query}

 üìÑ CONTENU DU DOCUMENT \"{target_document}\":
 {doc_content[:max_len]}"""
                    if len(doc_content) > max_len:
                        prompt += "\n[... contenu tronqu√© pour √©conomiser l'espace ...]"

                    prompt += f"""

 üîí INSTRUCTIONS STRICTES:
 1. R√©ponds UNIQUEMENT sur le contenu ci-dessus du fichier \"{target_document}\"
 2. Commence ta r√©ponse par \"üìÑ DOCX analys√© : {target_document}\"
 3. NE MENTIONNE AUCUN AUTRE DOCUMENT
 4. Base-toi EXCLUSIVEMENT sur le contenu fourni ci-dessus
 5. Si tu vois une r√©f√©rence √† un autre document, IGNORE-LA COMPL√àTEMENT
 6. Fais un r√©sum√© TR√àS D√âTAILL√â, structur√©, avec une introduction, un d√©veloppement en plusieurs points, et une conclusion riche. Utilise des listes, des titres en gras, et mets en valeur les mots importants.
 """
                    # Log pour debug final
                    self.logger.info(f"üîç PROMPT g√©n√©r√© pour {target_document} (d√©but): {prompt[:300]}...")

                    # PROTECTION FINALE : cr√©er un contexte isol√© avec SEULEMENT le document cibl√©
                    isolated_context = {
                        'stored_documents': {target_document: all_docs[target_document]},
                        'document_order': [target_document],
                        'conversation_history': context.get('conversation_history', [])
                    }

                    # G√©n√©rer la r√©ponse avec le contexte isol√©
                    response = self.local_ai.generate_response(prompt)
                    # "doc" isol√© sans contexte PDF
                    query_words = query_lower.split()
                    if 'doc' in query_words or 'du doc' in query_lower or 'le doc' in query_lower:
                        docx_docs = [doc for doc in document_order if doc.lower().endswith(('.docx', '.doc'))]
                        
                        if docx_docs:
                            if is_first_requested and len(docx_docs) >= 1:
                                target_document = docx_docs[0]
                                self.logger.info(f"PREMIER DOC s√©lectionn√©: {target_document}")
                            else:
                                target_document = docx_docs[-1]
                                self.logger.info(f"DOCX s√©lectionn√© par 'doc' isol√©: {target_document}")
                
                # D√©tection CODE
                elif any(term in query_lower for term in ['code', 'py', 'python', 'script']):
                    code_docs = [doc for doc in document_order if doc.lower().endswith(('.py', '.js', '.ts', '.java', '.cpp', '.c', '.html', '.css', '.json'))]
                    
                    if code_docs:
                        if is_first_requested and len(code_docs) >= 1:
                            target_document = code_docs[0]
                            self.logger.info(f"PREMIER CODE s√©lectionn√©: {target_document}")
                        else:
                            target_document = code_docs[-1]
                            self.logger.info(f"Code s√©lectionn√© par type: {target_document}")
                
                # GESTION G√âN√âRALE DES R√âF√âRENCES NUM√âRIQUES (tous types confondus)
                elif is_first_requested or is_second_requested or is_last_requested:
                    if is_first_requested and len(document_order) >= 1:
                        target_document = document_order[0]
                        self.logger.info(f"PREMIER DOCUMENT (tous types) s√©lectionn√©: {target_document}")
                    elif is_second_requested and len(document_order) >= 2:
                        target_document = document_order[1]
                        self.logger.info(f"DEUXI√àME DOCUMENT (tous types) s√©lectionn√©: {target_document}")
                    elif is_last_requested and document_order:
                        target_document = document_order[-1]
                        self.logger.info(f"DERNIER DOCUMENT (tous types) s√©lectionn√©: {target_document}")
            
            # 3. Si pas de type sp√©cifique mais demande d'action (r√©sum√©, analyse, etc.)
            if not target_document:
                action_keywords = ['r√©sume', 'r√©sum√©', 'analyse', 'contenu', 'explique', 'd√©cris', 'que dit', 'que contient']
                if any(keyword in query_lower for keyword in action_keywords):
                    if document_order:
                        target_document = document_order[-1]
                        self.logger.info(f"Dernier document s√©lectionn√© pour action: {target_document}")
            
            # NOUVEAU : Log d√©taill√© pour debug - V√âRIFIE LA S√âLECTION
            self.logger.info(f"=== DEBUG S√âLECTION DOCUMENT ===")
            self.logger.info(f"Query originale: '{query}'")
            self.logger.info(f"Query lowercase: '{query_lower}'")
            self.logger.info(f"Documents disponibles: {list(all_docs.keys())}")
            self.logger.info(f"Ordre documents: {document_order}")
            self.logger.info(f"Document cibl√© FINAL: {target_document}")
            self.logger.info(f"=== FIN DEBUG ===")
            
            if not target_document:
                self.logger.warning(f"Aucun document sp√©cifique d√©tect√© dans: '{query_lower}'")
            
            # Si aucun document sp√©cifique n'est mentionn√©, prendre le plus r√©cent
            if not target_document and document_order:
                target_document = document_order[-1]
                self.logger.info(f"Document le plus r√©cent s√©lectionn√©: {target_document}")
            
            # Construire le prompt avec le document cibl√© - PROTECTION RENFORC√âE
            if target_document and target_document in all_docs:
                doc_data = all_docs[target_document]
                if isinstance(doc_data, dict) and 'content' in doc_data:
                    doc_content = doc_data['content']
                else:
                    doc_content = str(doc_data)
                
                # PROTECTION SUPPL√âMENTAIRE : v√©rifier que c'est bien le bon document
                self.logger.info(f"üìÑ DOCUMENT S√âLECTIONN√â POUR TRAITEMENT: {target_document}")
                self.logger.info(f"üìä Taille contenu: {len(doc_content)} caract√®res")
                
                # PROMPT ULTRA-SP√âCIFIQUE avec BLOCAGE des autres documents
                prompt = f"""üö® R√àGLE ABSOLUE ET OBLIGATOIRE üö®
JE TE DONNE ACC√àS √Ä UN SEUL DOCUMENT. TU NE DOIS ANALYSER QUE CE DOCUMENT.
IGNORE COMPL√àTEMENT TOUTE R√âF√âRENCE √Ä D'AUTRES DOCUMENTS.

üéØ DOCUMENT UNIQUE √Ä ANALYSER: {target_document}
ÔøΩ QUESTION: {query}

üìÑ CONTENU DU DOCUMENT "{target_document}":
{doc_content[:3000]}"""
                
                if len(doc_content) > 3000:
                    prompt += "\n[... contenu tronqu√© pour √©conomiser l'espace ...]"
                    
                prompt += f"""

üîí INSTRUCTIONS STRICTES:
1. R√©ponds UNIQUEMENT sur le contenu ci-dessus du fichier "{target_document}"
2. Commence ta r√©ponse par "üìÑ DOCX analys√© : {target_document}"
3. NE MENTIONNE AUCUN AUTRE DOCUMENT
4. Base-toi EXCLUSIVEMENT sur le contenu fourni ci-dessus
5. Si tu vois une r√©f√©rence √† un autre document, IGNORE-LA COMPL√àTEMENT
"""
                
                # Log pour debug final
                self.logger.info(f"üîç PROMPT g√©n√©r√© pour {target_document} (d√©but): {prompt[:300]}...")
                
                # PROTECTION FINALE : cr√©er un contexte isol√© avec SEULEMENT le document cibl√©
                isolated_context = {
                    'stored_documents': {target_document: all_docs[target_document]},
                    'document_order': [target_document],
                    'conversation_history': context.get('conversation_history', [])
                }
                
                # G√©n√©rer la r√©ponse avec le contexte isol√©
                response = self.local_ai.generate_response(prompt)
                
            else:
                # Fallback: utiliser tous les documents disponibles
                self.logger.warning("Aucun document cibl√© trouv√©, utilisation de tous les documents")
                prompt = f"""Question: {query}

Contexte des documents disponibles:"""
                
                if 'stored_documents' in context:
                    for doc_name, doc_data in context['stored_documents'].items():
                        if isinstance(doc_data, dict) and 'content' in doc_data:
                            doc_content = doc_data['content']
                        else:
                            doc_content = str(doc_data)
                        
                        prompt += f"\n\n--- Document: {doc_name} ---\n{doc_content[:2000]}"
                        if len(doc_content) > 2000:
                            prompt += "\n[... contenu tronqu√© ...]"
                
                prompt += f"\n\nR√©ponds √† la question en te basant sur le contenu des documents ci-dessus."
                
                # G√©n√©rer la r√©ponse
                response = self.local_ai.generate_response(prompt)
            
            # NE PLUS ajouter de pr√©fixe ici car custom_ai_model s'en charge d√©j√†
            return {
                "type": "file_processing",
                "message": response,
                "success": True
            }
        except Exception as e:
            self.logger.error(f"Erreur traitement fichier: {e}")
            return {
                "type": "file_processing",
                "message": "Traitement de fichier en cours...",
                "success": False
            }
    
    async def _handle_code_generation(self, query: str, context: Dict) -> Dict[str, Any]:
        """
        G√®re la g√©n√©ration de code
        """
        try:
            # G√©n√©rer le code sans await (methode sync)
            code = self.code_generator.generate_code(query, context)
            
            # Cr√©er un message d'accompagnement intelligent
            language = self._detect_code_language(query)
            accompaniment = self._create_code_accompaniment(query, language)
            
            # Formater la r√©ponse compl√®te
            full_response = f"{accompaniment}\n\n```{language}\n{code}\n```"
            
            return {
                "type": "code_generation",
                "code": code,
                "message": full_response,
                "success": True
            }
        except Exception as e:
            self.logger.error(f"Erreur g√©n√©ration code: {e}")
            return {
                "type": "code_generation", 
                "message": f"‚ùå Erreur lors de la g√©n√©ration de code: {str(e)}",
                "success": False
            }
    
    async def _handle_document_generation(self, query: str, context: Dict) -> Dict[str, Any]:
        """
        G√®re la g√©n√©ration de documents
        """
        try:
            # G√©n√©rer le document sans await (methode sync)
            document = self.document_generator.generate_document(query, context)
            
            return {
                "type": "document_generation",
                "document": document,
                "message": "Document g√©n√©r√© avec succ√®s",
                "success": True
            }
        except Exception as e:
            self.logger.error(f"Erreur g√©n√©ration document: {e}")
            return {
                "type": "document_generation",
                "message": f"Erreur lors de la g√©n√©ration de document: {str(e)}",
                "success": False
            }
    
    async def _handle_general_query(self, query: str, context: Dict) -> Dict[str, Any]:
        """
        G√®re les requ√™tes g√©n√©rales
        """
        try:
            # Construire le prompt avec contexte si disponible
            prompt = query
            if 'stored_documents' in context and context['stored_documents']:
                prompt = f"""Contexte des documents disponibles:
                
"""
                for doc_name in context['stored_documents'].keys():
                    prompt += f"- {doc_name}\n"
                
                prompt += f"\nQuestion: {query}\n\nR√©ponds en tenant compte du contexte si pertinent."
            
            response = self.local_ai.generate_response(prompt)
            
            return {
                "type": "general",
                "message": response,
                "success": True
            }
        except Exception as e:
            self.logger.error(f"Erreur requ√™te g√©n√©rale: {e}")
            return {
                "type": "general",
                "message": f"Erreur lors du traitement: {str(e)}",
                "success": False
            }
    
    def _detect_code_language(self, query: str) -> str:
        """D√©tecte le langage de programmation demand√©"""
        query_lower = query.lower()
        
        if any(word in query_lower for word in ["python", "py"]):
            return "python"
        elif any(word in query_lower for word in ["javascript", "js", "node"]):
            return "javascript"
        elif any(word in query_lower for word in ["html", "page web", "site"]):
            return "html"
        elif any(word in query_lower for word in ["css", "style"]):
            return "css"
        else:
            return "python"  # Par d√©faut
    
    def _create_code_accompaniment(self, query: str, language: str) -> str:
        """Cr√©e un message d'accompagnement intelligent pour le code"""
        query_lower = query.lower()
        
        # Messages sp√©cifiques selon le type de code demand√©
        if any(word in query_lower for word in ["factorielle", "factorial"]):
            return f"üî¢ **Code pour calculer une factorielle en {language.capitalize()}**\n\nVoici une impl√©mentation efficace avec gestion des cas d'erreur :"
        
        elif any(word in query_lower for word in ["hello", "world", "bonjour"]):
            return f"üëã **Programme Hello World en {language.capitalize()}**\n\nLe classique pour d√©buter :"
        
        elif any(word in query_lower for word in ["fibonacci", "fibo"]):
            return f"üåÄ **S√©quence de Fibonacci en {language.capitalize()}**\n\nCode optimis√© pour g√©n√©rer la suite :"
        
        elif any(word in query_lower for word in ["tri", "sort", "trier"]):
            return f"üìä **Algorithme de tri en {language.capitalize()}**\n\nImpl√©mentation d'un tri efficace :"
        
        elif any(word in query_lower for word in ["classe", "class", "objet"]):
            return f"üèóÔ∏è **Classe en {language.capitalize()}**\n\nStructure orient√©e objet :"
        
        elif any(word in query_lower for word in ["fonction", "function", "def"]):
            return f"‚öôÔ∏è **Fonction en {language.capitalize()}**\n\nCode modulaire et r√©utilisable :"
        
        elif any(word in query_lower for word in ["api", "web", "serveur"]):
            return f"üåê **Code pour API/Web en {language.capitalize()}**\n\nStructure pour service web :"
        
        else:
            return f"üíª **Code g√©n√©r√© en {language.capitalize()}**\n\nVoici une impl√©mentation pour votre demande :"
    
    def get_status(self) -> Dict[str, Any]:
        """
        Retourne le statut du moteur IA
        """
        return {
            "engine": "running",
            "mode": "enterprise_local",
            "llm_status": "local_custom_model_only",
            "conversation_count": len(self.conversation_manager.history) if hasattr(self.conversation_manager, 'history') else 0,
            "local_ai_stats": self.local_ai.get_stats() if hasattr(self.local_ai, 'get_stats') else {},
            "config": self.config
        }
