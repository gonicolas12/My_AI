"""
Moteur principal de l'IA personnelle
G√®re l'orchestration entre les diff√©rents modules
"""

import logging
from typing import Dict, Any, Optional, List
from pathlib import Path

from .config import AI_CONFIG
from .conversation import ConversationManager
from models.custom_ai_model import CustomAIModel
from models.conversation_memory import ConversationMemory
from processors.pdf_processor import PDFProcessor
from processors.docx_processor import DOCXProcessor
from processors.code_processor import CodeProcessor
from generators.document_generator import DocumentGenerator
from models.generators import CodeGenerator
from utils.logger import setup_logger
from utils.file_manager import FileManager

class AIEngine:
    """
    Moteur principal de l'IA personnelle
    """
    
    def __init__(self, config: Optional[Dict] = None):
        """
        Initialise le moteur IA
        
        Args:
            config: Configuration personnalis√©e (optionnel)
        """
        self.config = config or AI_CONFIG
        self.logger = setup_logger("AIEngine")
        
        # Initialisation des composants
        self.file_manager = FileManager()
        
        # Initialiser session_context AVANT tout le reste
        self.session_context = {
            "documents_processed": [],
            "code_files_processed": [],
            "last_document_type": None,
            "current_document": None
        }

        # Initialisation de la m√©moire et du gestionnaire de conversations
        self.conversation_memory = ConversationMemory()
        self.conversation_manager = ConversationManager(memory=self.conversation_memory)
        
        # Mod√®le IA local personnalis√© avec m√©moire de conversation (100% autonome)
        try:
            from models.custom_ai_model import CustomAIModel
            self.local_ai = CustomAIModel(conversation_memory=self.conversation_memory)
            self.model = self.local_ai  # Alias pour compatibilit√© avec l'interface graphique
            self.logger.info("‚úÖ Mod√®le IA local avec m√©moire initialis√©")
            # Initialisation du gestionnaire de LLM
            self.llm_manager = self.local_ai  # Pour compatibilit√© avec les fonctions qui utilisent llm_manager
        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors de l'initialisation du mod√®le IA : {e}")
            # Fallback sur l'ancien syst√®me
            self.local_ai = CustomAIModel()
            self.model = self.local_ai
            self.llm_manager = self.local_ai
        
        # Processeurs
        self.pdf_processor = PDFProcessor()
        self.docx_processor = DOCXProcessor()
        self.code_processor = CodeProcessor()
        
        # G√©n√©rateurs
        self.document_generator = DocumentGenerator()
        self.code_generator = CodeGenerator()
        
        self.logger.info("Moteur IA initialis√© avec succ√®s")
    
    def process_text(self, text: str) -> str:
        """
        Traite un texte en utilisant le mod√®le IA local personnalis√©
        
        Args:
            text: Texte √† traiter
            
        Returns:
            R√©ponse de l'IA sous forme de texte
        """
        try:
            self.logger.info(f"Processing text: {text[:50]}...")
            
            # Utilisation du mod√®le IA local 100% autonome
            response = self.local_ai.generate_response(text)
            
            self.logger.info(f"AI model response: {response[:50]}...")
            
            # On sauvegarde l'√©change sans utiliser add_message qui cause des erreurs
            try:
                # Tente d'utiliser add_exchange qui est une autre m√©thode disponible
                self.conversation_manager.add_exchange(text, {"message": response})
            except:
                # Si √ßa √©choue aussi, on ne fait rien - l'important est de ne pas planter
                self.logger.warning("Impossible de sauvegarder la conversation")
            
            return response
        
        except Exception as e:
            self.logger.error(f"Erreur dans process_text: {e}")
            self.logger.warning("Utilisation du fallback response")
            fallback_response = self._generate_fallback_response(text)
            return fallback_response
    
    def _generate_simple_function(self, text: str) -> str:
        """G√©n√®re une fonction simple bas√©e sur la demande"""
        if "factorielle" in text.lower():
            return """üîß Fonction g√©n√©r√©e :

```python
def factorielle(n):
    \"\"\"
    Calcule la factorielle d'un nombre
    
    Args:
        n (int): Nombre dont on veut la factorielle
        
    Returns:
        int: Factorielle de n
    \"\"\"
    if n < 0:
        raise ValueError("La factorielle n'est pas d√©finie pour les nombres n√©gatifs")
    elif n == 0 or n == 1:
        return 1
    else:
        return n * factorielle(n - 1)

# Exemple d'utilisation
print(factorielle(5))  # Output: 120
```"""
        elif "fibonacci" in text.lower():
            return """üîß Fonction g√©n√©r√©e :

```python
def fibonacci(n):
    \"\"\"
    Calcule le n-i√®me nombre de Fibonacci
    
    Args:
        n (int): Position dans la s√©quence
        
    Returns:
        int: n-i√®me nombre de Fibonacci
    \"\"\"
    if n <= 0:
        return 0
    elif n == 1:
        return 1
    else:
        return fibonacci(n-1) + fibonacci(n-2)

# Exemple d'utilisation
for i in range(10):
    print(f"F({i}) = {fibonacci(i)}")
```"""
        else:
            return f"""üîß Fonction g√©n√©r√©e bas√©e sur votre demande :

```python
def ma_fonction():
    \"\"\"
    Fonction g√©n√©r√©e automatiquement
    Bas√©e sur : {text}
    \"\"\"
    # TODO: Impl√©tez votre logique ici
    print("Fonction cr√©√©e avec succ√®s !")
    return True

# Exemple d'utilisation
ma_fonction()
```
üí° Pour une g√©n√©ration plus pr√©cise, d√©crivez exactement ce que doit faire la fonction."""
    
    def _generate_simple_class(self, text: str) -> str:
        """G√©n√®re une classe simple bas√©e sur la demande"""
        if "personne" in text.lower() or "person" in text.lower():
            return """üèóÔ∏è Classe g√©n√©r√©e :

```python
class Personne:
    \"\"\"
    Classe repr√©sentant une personne
    \"\"\"
    
    def __init__(self, nom, age):
        \"\"\"
        Initialise une nouvelle personne
        
        Args:
            nom (str): Nom de la personne
            age (int): √Çge de la personne
        \"\"\"
        self.nom = nom
        self.age = age
    
    def se_presenter(self):
        \"\"\"Pr√©sente la personne\"\"\"
        return f"Bonjour, je suis {self.nom} et j'ai {self.age} ans."
    
    def avoir_anniversaire(self):
        \"\"\"Incr√©mente l'√¢ge d'un an\"\"\"
        self.age += 1
        return f"Joyeux anniversaire ! {self.nom} a maintenant {self.age} ans."

# Exemple d'utilisation
personne = Personne("Alice", 25)
print(personne.se_presenter())
print(personne.avoir_anniversaire())
```"""
        else:
            return f"""üèóÔ∏è Classe g√©n√©r√©e bas√©e sur votre demande :

```python
class MaClasse:
    \"\"\"
    Classe g√©n√©r√©e automatiquement
    Bas√©e sur : {text}
    \"\"\"
    
    def __init__(self):
        \"\"\"Initialise la classe\"\"\"
        self.nom = "MaClasse"
        self.active = True
    
    def action(self):
        \"\"\"M√©thode d'action principale\"\"\"
        if self.active:
            return "Action ex√©cut√©e avec succ√®s !"
        return "Classe inactive"
    
    def __str__(self):
        \"\"\"Repr√©sentation string de la classe\"\"\"
        return f"{self.nom} - Active: {self.active}"

# Exemple d'utilisation
objet = MaClasse()
print(objet)
print(objet.action())
```

üí° Pour une g√©n√©ration plus pr√©cise, d√©crivez les attributs et m√©thodes souhait√©s."""
    
    def _generate_code_from_text(self, text: str) -> str:
        """G√©n√®re du code bas√© sur une description textuelle"""
        return f"""üíª Code g√©n√©r√© bas√© sur votre demande :

```python
# Code bas√© sur : {text}

def solution():
    \"\"\"
    Solution g√©n√©r√©e automatiquement
    \"\"\"
    # TODO: Impl√©tez votre solution ici
    print("Code g√©n√©r√© avec succ√®s !")
    
    # Exemple de logique de base
    resultat = "Mission accomplie"
    return resultat

# Ex√©cution
if __name__ == "__main__":
    print(solution())
```

üí° Pour du code plus sp√©cifique, donnez plus de d√©tails sur ce que vous voulez accomplir."""
    
    def _get_help_text(self) -> str:
        """Retourne le texte d'aide"""
        return """ü§ñ Aide - My AI Personal Assistant

üìù **G√©n√©ration de code :**
‚Ä¢ "g√©n√®re une fonction pour calculer la factorielle"
‚Ä¢ "cr√©e une classe Personne avec nom et √¢ge"
‚Ä¢ "g√©n√®re du code pour lire un fichier CSV"

üìä **Traitement de documents :**
‚Ä¢ Utilisez les boutons pour traiter des PDF/DOCX
‚Ä¢ Glissez-d√©posez vos fichiers

üíª **Analyse de code :**
‚Ä¢ Utilisez le bouton "Process Code" pour analyser vos fichiers

‚ùì **Questions g√©n√©rales :**
‚Ä¢ Posez vos questions en langage naturel
‚Ä¢ Je peux vous aider avec la programmation Python

üí° **Conseils :**
‚Ä¢ Soyez sp√©cifique dans vos demandes
‚Ä¢ N'h√©sitez pas √† demander des exemples
‚Ä¢ Utilisez "aide" pour revoir cette aide"""

    def _generate_fallback_response(self, text: str) -> str:
        """
        G√©n√®re une r√©ponse de fallback en cas d'erreur du mod√®le principal
        
        Args:
            text: Texte de l'utilisateur
            
        Returns:
            R√©ponse de fallback naturelle
        """
        text_lower = text.lower()
        
        # Salutations
        if any(word in text_lower for word in ["salut", "bonjour", "hello", "hi", "bonsoir"]):
            return "Salut ! Comment √ßa va ? En quoi puis-je t'aider ?"
        
        # Questions d'aide
        if "help" in text_lower or "aide" in text_lower:
            return "Je peux t'aider avec la g√©n√©ration de code Python, l'analyse de documents, et r√©pondre √† tes questions techniques. Que veux-tu faire ?"
        
        # Demandes de code
        elif "g√©n√©r" in text_lower or "cr√©er" in text_lower or "fonction" in text_lower or "classe" in text_lower:
            if "fonction" in text_lower:
                return self._generate_simple_function(text)
            elif "classe" in text_lower:
                return self._generate_simple_class(text)
            else:
                return "Je peux g√©n√©rer du code pour toi ! Tu veux une fonction ou une classe ? Dis-moi ce que tu veux cr√©er."
        
        # Questions sur l'IA
        elif any(phrase in text_lower for phrase in ["qui es-tu", "que fais-tu", "qu'est-ce que tu fais"]):
            return "Je suis ton assistant IA local. Je peux coder, analyser des documents et r√©pondre √† tes questions. Qu'est-ce qui t'int√©resse ?"
        
        # R√©ponse g√©n√©rale naturelle
        else:
            return f"Je vois ! Et en quoi puis-je t'aider avec √ßa ? Tu veux que je g√©n√®re du code, que je t'explique quelque chose, ou autre chose ?"

    async def process_message(self, message: str) -> str:
        """
        Traite un message de mani√®re asynchrone
        
        Args:
            message: Message de l'utilisateur
            
        Returns:
            R√©ponse de l'IA
        """
        return self.process_text(message)

    def get_conversation_history(self) -> List[Dict[str, str]]:
        """
        R√©cup√®re l'historique de conversation
        
        Returns:
            Liste des messages
        """
        return self.conversation_manager.get_history()

    def clear_conversation(self):
        """
        Clear the conversation history and memory
        """
        self.conversation_manager.clear()
        self.conversation_memory.clear()
        
        # R√©initialiser aussi le session_context
        self.session_context = {
            "documents_processed": [],
            "code_files_processed": [],
            "last_document_type": None,
            "current_document": None
        }
        
        self.logger.info("Conversation cleared")

    def initialize_llm(self) -> bool:
        """Initialise les mod√®les LLM - Mode entreprise local uniquement"""
        try:
            # En mode entreprise, nous utilisons uniquement le mod√®le local
            return hasattr(self, 'local_ai') and self.local_ai is not None
        except Exception as e:
            self.logger.error(f"Erreur initialisation LLM: {e}")
            return False

    async def process_query(self, query: str, context: Optional[Dict] = None) -> Dict[str, Any]:
        """
        Traite une requ√™te utilisateur
        
        Args:
            query: Question/demande de l'utilisateur
            context: Contexte additionnel (fichiers, historique, etc.)
            
        Returns:
            R√©ponse structur√©e de l'IA
        """
        try:
            self.logger.info(f"Traitement de la requ√™te: {query[:100]}...")
            
            # Analyse de la requ√™te
            query_type = self._analyze_query_type(query)
            
            # Pr√©paration du contexte
            full_context = self._prepare_context(query, context)
            
            # Traitement selon le type
            if query_type == "conversation":
                response = await self._handle_conversation(query, full_context)
            elif query_type == "file_processing":
                response = await self._handle_file_processing(query, full_context)
            elif query_type == "code_generation":
                response = await self._handle_code_generation(query, full_context)
            elif query_type == "document_generation":
                response = await self._handle_document_generation(query, full_context)
            else:
                response = await self._handle_general_query(query, full_context)
            
            # Sauvegarde dans l'historique
            self.conversation_manager.add_exchange(query, response)
            
            return response
            
        except Exception as e:
            self.logger.error(f"Erreur lors du traitement: {e}")
            return {
                "type": "error",
                "message": f"D√©sol√©, une erreur s'est produite: {str(e)}",
                "success": False
            }
    
    def _analyze_query_type(self, query: str) -> str:
        """
        Analyse le type de requ√™te
        
        Args:
            query: Requ√™te utilisateur
            
        Returns:
            Type de requ√™te identifi√©
        """
        query_lower = query.lower()
        
        # V√©rifier d'abord si on a des documents et si la question les concerne
        if hasattr(self.local_ai, 'conversation_memory'):
            stored_docs = self.local_ai.conversation_memory.get_document_content()
            if stored_docs:
                # Mots-cl√©s pour questions sur les documents
                doc_question_keywords = [
                    "r√©sume", "explique", "analyse", "qu'est-ce que", "que dit", 
                    "contenu", "parle", "traite", "sujet", "doc", "document", 
                    "pdf", "docx", "fichier", "code"
                ]
                if any(keyword in query_lower for keyword in doc_question_keywords):
                    return "file_processing"
        
        # Mots-cl√©s pour la g√©n√©ration de code (NOUVEAU code, pas analyse)
        code_generation_keywords = ["g√©n√®re", "cr√©e", "√©cris", "d√©veloppe", "programme", "script", "fonction", "classe"]
        if any(keyword in query_lower for keyword in code_generation_keywords):
            return "code_generation"
        
        # Mots-cl√©s pour la g√©n√©ration de documents
        doc_keywords = ["cr√©er", "g√©n√©rer", "rapport", "r√©diger", "documenter"]
        if any(keyword in query_lower for keyword in doc_keywords):
            return "document_generation"
        
        return "conversation"
    
    def _prepare_context(self, query: str, context: Optional[Dict]) -> Dict[str, Any]:
        """
        Pr√©pare le contexte complet pour la requ√™te
        """
        full_context = {
            "query": query,
            "conversation_history": self.conversation_manager.get_recent_history(),
            "timestamp": self.file_manager.get_timestamp(),
            "user_context": context or {}
        }
        
        # Ajouter les documents stock√©s dans la m√©moire
        if hasattr(self.local_ai, 'conversation_memory'):
            stored_docs = self.local_ai.conversation_memory.stored_documents
            if stored_docs:
                full_context["stored_documents"] = stored_docs
                full_context["document_order"] = self.local_ai.conversation_memory.document_order
                self.logger.info(f"Contexte enrichi avec {len(stored_docs)} documents stock√©s")
        
        return full_context
    
    async def _handle_conversation(self, query: str, context: Dict) -> Dict[str, Any]:
        """
        G√®re les conversations g√©n√©rales
        """
        try:
            # Si on a des documents, les inclure dans le contexte
            prompt = query
            if 'stored_documents' in context and context['stored_documents']:
                prompt = f"""Contexte des documents disponibles:
                
"""
                for doc_name in context['stored_documents'].keys():
                    prompt += f"- {doc_name}\n"
                
                prompt += f"\nQuestion: {query}\n\nR√©ponds en tenant compte du contexte si pertinent."
            
            response = self.local_ai.generate_response(prompt)
            
            return {
                "type": "conversation",
                "message": response,
                "success": True
            }
        except Exception as e:
            self.logger.error(f"Erreur conversation: {e}")
            return {
                "type": "conversation",
                "message": f"Erreur lors de la conversation: {str(e)}",
                "success": False
            }
    
    async def _handle_file_processing(self, query: str, context: Dict) -> Dict[str, Any]:
        """
        G√®re le traitement de fichiers et questions sur les documents
        """
        try:
            query_lower = query.lower()
            
            # D√©terminer quel document sp√©cifique est demand√©
            target_document = None
            all_docs = context.get('stored_documents', {})
            document_order = context.get('document_order', [])
            
            # Log pour debug
            self.logger.info(f"Query: '{query}' | Documents disponibles: {list(all_docs.keys())}")
            
            # Recherche de mots-cl√©s sp√©cifiques au type de document
            if 'pdf' in query_lower:
                # Chercher le PDF le plus r√©cent
                for doc_name in reversed(document_order):
                    if doc_name.lower().endswith('.pdf'):
                        target_document = doc_name
                        self.logger.info(f"PDF s√©lectionn√©: {target_document}")
                        break
            elif 'docx' in query_lower:
                # Recherche sp√©cifique pour DOCX
                for doc_name in reversed(document_order):
                    if doc_name.lower().endswith('.docx'):
                        target_document = doc_name
                        self.logger.info(f"DOCX s√©lectionn√©: {target_document}")
                        break
            elif 'doc' in query_lower and 'pdf' not in query_lower:
                # "doc" sans mention de PDF = chercher DOCX
                for doc_name in reversed(document_order):
                    if doc_name.lower().endswith('.docx'):
                        target_document = doc_name
                        self.logger.info(f"Document DOCX s√©lectionn√©: {target_document}")
                        break
            elif 'code' in query_lower or 'py' in query_lower:
                # Chercher le fichier de code le plus r√©cent
                for doc_name in reversed(document_order):
                    if doc_name.lower().endswith(('.py', '.js', '.ts', '.java', '.cpp', '.c')):
                        target_document = doc_name
                        self.logger.info(f"Code s√©lectionn√©: {target_document}")
                        break
            
            # Si aucun document sp√©cifique n'est mentionn√©, prendre le plus r√©cent
            if not target_document and document_order:
                target_document = document_order[-1]
                self.logger.info(f"Document le plus r√©cent s√©lectionn√©: {target_document}")
            
            # Construire le prompt avec le document cibl√©
            if target_document and target_document in all_docs:
                doc_data = all_docs[target_document]
                if isinstance(doc_data, dict) and 'content' in doc_data:
                    doc_content = doc_data['content']
                else:
                    doc_content = str(doc_data)
                
                prompt = f"""Question: {query}

Document analys√©: {target_document}

Contenu du document:
{doc_content[:3000]}"""
                
                if len(doc_content) > 3000:
                    prompt += "\n[... contenu tronqu√© ...]"
                    
                prompt += f"\n\nR√©ponds √† la question en te basant uniquement sur le contenu du document '{target_document}' ci-dessus."
            else:
                # Fallback: utiliser tous les documents disponibles
                self.logger.warning("Aucun document cibl√© trouv√©, utilisation de tous les documents")
                prompt = f"""Question: {query}

Contexte des documents disponibles:"""
                
                if 'stored_documents' in context:
                    for doc_name, doc_data in context['stored_documents'].items():
                        if isinstance(doc_data, dict) and 'content' in doc_data:
                            doc_content = doc_data['content']
                        else:
                            doc_content = str(doc_data)
                        
                        prompt += f"\n\n--- Document: {doc_name} ---\n{doc_content[:2000]}"
                        if len(doc_content) > 2000:
                            prompt += "\n[... contenu tronqu√© ...]"
                
                prompt += f"\n\nR√©ponds √† la question en te basant sur le contenu des documents ci-dessus."
            
            # G√©n√©rer la r√©ponse
            response = self.local_ai.generate_response(prompt)
            
            return {
                "type": "file_processing",
                "message": response,
                "success": True
            }
        except Exception as e:
            self.logger.error(f"Erreur traitement fichier: {e}")
            return {
                "type": "file_processing",
                "message": "Traitement de fichier en cours...",
                "success": False
            }
    
    async def _handle_code_generation(self, query: str, context: Dict) -> Dict[str, Any]:
        """
        G√®re la g√©n√©ration de code
        """
        try:
            # G√©n√©rer le code sans await (methode sync)
            code = self.code_generator.generate_code(query, context)
            
            return {
                "type": "code_generation",
                "code": code,
                "message": "Code g√©n√©r√© avec succ√®s",
                "success": True
            }
        except Exception as e:
            self.logger.error(f"Erreur g√©n√©ration code: {e}")
            return {
                "type": "code_generation", 
                "message": f"Erreur lors de la g√©n√©ration de code: {str(e)}",
                "success": False
            }
    
    async def _handle_document_generation(self, query: str, context: Dict) -> Dict[str, Any]:
        """
        G√®re la g√©n√©ration de documents
        """
        try:
            # G√©n√©rer le document sans await (methode sync)
            document = self.document_generator.generate_document(query, context)
            
            return {
                "type": "document_generation",
                "document": document,
                "message": "Document g√©n√©r√© avec succ√®s",
                "success": True
            }
        except Exception as e:
            self.logger.error(f"Erreur g√©n√©ration document: {e}")
            return {
                "type": "document_generation",
                "message": f"Erreur lors de la g√©n√©ration de document: {str(e)}",
                "success": False
            }
    
    async def _handle_general_query(self, query: str, context: Dict) -> Dict[str, Any]:
        """
        G√®re les requ√™tes g√©n√©rales
        """
        try:
            # Construire le prompt avec contexte si disponible
            prompt = query
            if 'stored_documents' in context and context['stored_documents']:
                prompt = f"""Contexte des documents disponibles:
                
"""
                for doc_name in context['stored_documents'].keys():
                    prompt += f"- {doc_name}\n"
                
                prompt += f"\nQuestion: {query}\n\nR√©ponds en tenant compte du contexte si pertinent."
            
            response = self.local_ai.generate_response(prompt)
            
            return {
                "type": "general",
                "message": response,
                "success": True
            }
        except Exception as e:
            self.logger.error(f"Erreur requ√™te g√©n√©rale: {e}")
            return {
                "type": "general",
                "message": f"Erreur lors du traitement: {str(e)}",
                "success": False
            }
    
    def get_status(self) -> Dict[str, Any]:
        """
        Retourne le statut du moteur IA
        """
        return {
            "engine": "running",
            "mode": "enterprise_local",
            "llm_status": "local_custom_model_only",
            "conversation_count": len(self.conversation_manager.history) if hasattr(self.conversation_manager, 'history') else 0,
            "local_ai_stats": self.local_ai.get_stats() if hasattr(self.local_ai, 'get_stats') else {},
            "config": self.config
        }
