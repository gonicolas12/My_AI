
"""
Moteur principal de l'IA personnelle
G√®re l'orchestration entre les diff√©rents modules
"""

import asyncio

import logging
from typing import Dict, Any, Optional, List
from pathlib import Path

from .config import AI_CONFIG
from .conversation import ConversationManager
from models.custom_ai_model import CustomAIModel
from models.conversation_memory import ConversationMemory
from processors.pdf_processor import PDFProcessor
from processors.docx_processor import DOCXProcessor
from processors.code_processor import CodeProcessor
from generators.document_generator import DocumentGenerator
from models.generators import CodeGenerator
from utils.logger import setup_logger
from utils.file_manager import FileManager

class AIEngine:
    """
    Moteur principal de l'IA personnelle
    """
    
    def __init__(self, config: Optional[Dict] = None):
        """
        Initialise le moteur IA
        
        Args:
            config: Configuration personnalis√©e (optionnel)
        """
        self.config = config or AI_CONFIG
        self.logger = setup_logger("AIEngine")
        
        # Initialisation des composants
        self.file_manager = FileManager()

        self.current_request_id = 0
        
        # Initialiser session_context AVANT tout le reste
        self.session_context = {
            "documents_processed": [],
            "code_files_processed": [],
            "last_document_type": None,
            "current_document": None
        }

        # Initialisation de la m√©moire et du gestionnaire de conversations
        self.conversation_memory = ConversationMemory()
        self.conversation_manager = ConversationManager(memory=self.conversation_memory)
        
        # Mod√®le IA local personnalis√© avec m√©moire de conversation (100% autonome)
        try:
            from models.custom_ai_model import CustomAIModel
            from models.ml_faq_model import MLFAQModel
            self.local_ai = CustomAIModel(conversation_memory=self.conversation_memory)
            self.ml_ai = MLFAQModel()  # Mod√®le ML local (TF-IDF)
            # Debug supprim√© : plus de log sur le chargement de la base FAQ/ML
            self.model = self.local_ai  # Alias pour compatibilit√© avec l'interface graphique
            self.logger.info("‚úÖ Mod√®le IA local avec m√©moire initialis√©")
            self.logger.info("‚úÖ Mod√®le ML (TF-IDF) initialis√©")
            # Initialisation du gestionnaire de LLM
            self.llm_manager = self.local_ai  # Pour compatibilit√© avec les fonctions qui utilisent llm_manager
        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors de l'initialisation du mod√®le IA : {e}")
            # Fallback sur l'ancien syst√®me
            self.local_ai = CustomAIModel()
            self.ml_ai = None
            self.model = self.local_ai
            self.llm_manager = self.local_ai
        
        # Processeurs
        self.pdf_processor = PDFProcessor()
        self.docx_processor = DOCXProcessor()
        self.code_processor = CodeProcessor()
        
        # G√©n√©rateurs
        self.document_generator = DocumentGenerator()
        self.code_generator = CodeGenerator()
        
        self.logger.info("Moteur IA initialis√© avec succ√®s")
    
    def process_text(self, text: str) -> str:
        """
        Traite un texte en donnant la priorit√© √† la FAQ ML (TF-IDF), puis utilise la logique avanc√©e (process_query) pour router la demande (explication code, etc).
        """
        try:
            self.logger.info(f"[DEBUG] process_text: question utilisateur brute: {repr(text)}")
            print(f"[AIEngine] Appel FAQ pour: '{text}'")
            # 1. Tenter la FAQ ML d'abord
            response_ml = None
            if self.ml_ai is not None:
                try:
                    self.logger.info(f"[DEBUG] Passage de la question √† FAQ/ML: {repr(text)}")
                    response_ml = self.ml_ai.predict(text)
                    self.logger.info(f"[DEBUG] ML model response: {repr(response_ml)}")
                except Exception as e:
                    self.logger.warning(f"Erreur mod√®le ML: {e}")
            else:
                print(f"[AIEngine] Mod√®le FAQ ML non initialis√©. Passage direct au mod√®le custom.")

            # Priorit√© absolue : si la FAQ locale a une r√©ponse, on la retourne DIRECTEMENT
            if response_ml is not None and str(response_ml).strip():
                print(f"[AIEngine] R√©ponse trouv√©e dans la FAQ locale. Priorit√© absolue. Pas de recherche internet ni d'appel au mod√®le custom.")
                try:
                    self.conversation_manager.add_exchange(text, {"message": response_ml})
                except Exception as e:
                    self.logger.warning(f"Impossible de sauvegarder la conversation: {e}")
                return response_ml

            # Sinon, router via process_query pour b√©n√©ficier de la logique avanc√©e (explication code, etc)
            try:
                # Utilise asyncio pour appeler la m√©thode async
                loop = None
                try:
                    loop = asyncio.get_event_loop()
                except RuntimeError:
                    loop = None
                if loop and loop.is_running():
                    # Si d√©j√† dans un event loop (rare hors notebook), patch avec nest_asyncio si dispo
                    try:
                        import nest_asyncio  # type: ignore
                        nest_asyncio.apply()
                    except ImportError:
                        self.logger.warning("nest_asyncio non install√© : l'appel async peut √©chouer si d√©j√† dans un event loop.")
                    future = self.process_query(text)
                    response = loop.run_until_complete(future)
                else:
                    response = asyncio.run(self.process_query(text))
                # On sauvegarde l'√©change
                try:
                    self.conversation_manager.add_exchange(text, response)
                except Exception as e:
                    self.logger.warning(f"Impossible de sauvegarder la conversation: {e}")
                return response.get("message", "[Aucune r√©ponse g√©n√©r√©e]")
            except Exception as e:
                self.logger.error(f"Erreur lors de l'appel √† process_query: {e}")
                self.logger.warning("Utilisation du fallback response (process_query)")
                fallback_response = self._generate_fallback_response(text)
                return fallback_response
        except Exception as e:
            self.logger.error(f"Erreur dans process_text: {e}")
            self.logger.warning("Utilisation du fallback response (global)")
            fallback_response = self._generate_fallback_response(text)
            return fallback_response

    def _merge_responses(self, response_custom, response_ml):
        """
        Donne la priorit√© √† la FAQ ML : si une r√©ponse MLFAQ existe, elle est utilis√©e, sinon on utilise la r√©ponse custom.
        """
        if response_ml is not None and str(response_ml).strip():
            return str(response_ml)
        return response_custom
    
    def _generate_simple_function(self, text: str) -> str:
        """G√©n√®re une fonction simple bas√©e sur la demande"""
        if "factorielle" in text.lower():
            return """üîß Fonction g√©n√©r√©e :

```python
def factorielle(n):
    \"\"\"
    Calcule la factorielle d'un nombre
    
    Args:
        n (int): Nombre dont on veut la factorielle
        
    Returns:
        int: Factorielle de n
    \"\"\"
    if n < 0:
        raise ValueError("La factorielle n'est pas d√©finie pour les nombres n√©gatifs")
    elif n == 0 or n == 1:
        return 1
    else:
        return n * factorielle(n - 1)

# Exemple d'utilisation
print(factorielle(5))  # Output: 120
```"""
        elif "fibonacci" in text.lower():
            return """üîß Fonction g√©n√©r√©e :

```python
def fibonacci(n):
    \"\"\"
    Calcule le n-i√®me nombre de Fibonacci
    
    Args:
        n (int): Position dans la s√©quence
        
    Returns:
        int: n-i√®me nombre de Fibonacci
    \"\"\"
    if n <= 0:
        return 0
    elif n == 1:
        return 1
    else:
        return fibonacci(n-1) + fibonacci(n-2)

# Exemple d'utilisation
for i in range(10):
    print(f"F({i}) = {fibonacci(i)}")
```"""
        else:
            return f"""üîß Fonction g√©n√©r√©e bas√©e sur votre demande :

```python
def ma_fonction():
    \"\"\"
    Fonction g√©n√©r√©e automatiquement
    Bas√©e sur : {text}
    \"\"\"
    # TODO: Impl√©tez votre logique ici
    print("Fonction cr√©√©e avec succ√®s !")
    return True

# Exemple d'utilisation
ma_fonction()
```
üí° Pour une g√©n√©ration plus pr√©cise, d√©crivez exactement ce que doit faire la fonction."""
    
    def _generate_simple_class(self, text: str) -> str:
        """G√©n√®re une classe simple bas√©e sur la demande"""
        if "personne" in text.lower() or "person" in text.lower():
            return """üèóÔ∏è Classe g√©n√©r√©e :

```python
class Personne:
    \"\"\"
    Classe repr√©sentant une personne
    \"\"\"
    
    def __init__(self, nom, age):
        \"\"\"
        Initialise une nouvelle personne
        
        Args:
            nom (str): Nom de la personne
            age (int): √Çge de la personne
        \"\"\"
        self.nom = nom
        self.age = age
    
    def se_presenter(self):
        \"\"\"Pr√©sente la personne\"\"\"
        return f"Bonjour, je suis {self.nom} et j'ai {self.age} ans."
    
    def avoir_anniversaire(self):
        \"\"\"Incr√©mente l'√¢ge d'un an\"\"\"
        self.age += 1
        return f"Joyeux anniversaire ! {self.nom} a maintenant {self.age} ans."

# Exemple d'utilisation
personne = Personne("Alice", 25)
print(personne.se_presenter())
print(personne.avoir_anniversaire())
```"""
        else:
            return f"""üèóÔ∏è Classe g√©n√©r√©e bas√©e sur votre demande :

```python
class MaClasse:
    \"\"\"
    Classe g√©n√©r√©e automatiquement
    Bas√©e sur : {text}
    \"\"\"
    
    def __init__(self):
        \"\"\"Initialise la classe\"\"\"
        self.nom = "MaClasse"
        self.active = True
    
    def action(self):
        \"\"\"M√©thode d'action principale\"\"\"
        if self.active:
            return "Action ex√©cut√©e avec succ√®s !"
        return "Classe inactive"
    
    def __str__(self):
        \"\"\"Repr√©sentation string de la classe\"\"\"
        return f"{self.nom} - Active: {self.active}"

# Exemple d'utilisation
objet = MaClasse()
print(objet)
print(objet.action())
```

üí° Pour une g√©n√©ration plus pr√©cise, d√©crivez les attributs et m√©thodes souhait√©s."""
    
    def _generate_code_from_text(self, text: str) -> str:
        """G√©n√®re du code bas√© sur une description textuelle"""
        return f"""üíª Code g√©n√©r√© bas√© sur votre demande :

```python
# Code bas√© sur : {text}

def solution():
    \"\"\"
    Solution g√©n√©r√©e automatiquement
    \"\"\"
    # TODO: Impl√©tez votre solution ici
    print("Code g√©n√©r√© avec succ√®s !")
    
    # Exemple de logique de base
    resultat = "Mission accomplie"
    return resultat

# Ex√©cution
if __name__ == "__main__":
    print(solution())
```

üí° Pour du code plus sp√©cifique, donnez plus de d√©tails sur ce que vous voulez accomplir."""
    
    def _get_help_text(self) -> str:
        """Retourne le texte d'aide"""
        return """ü§ñ Aide - My AI Personal Assistant

üìù **G√©n√©ration de code :**
‚Ä¢ "g√©n√®re une fonction pour calculer la factorielle"
‚Ä¢ "cr√©e une classe Personne avec nom et √¢ge"
‚Ä¢ "g√©n√®re du code pour lire un fichier CSV"

üìä **Traitement de documents :**
‚Ä¢ Utilisez les boutons pour traiter des PDF/DOCX
‚Ä¢ Glissez-d√©posez vos fichiers

üíª **Analyse de code :**
‚Ä¢ Utilisez le bouton "Process Code" pour analyser vos fichiers

‚ùì **Questions g√©n√©rales :**
‚Ä¢ Posez vos questions en langage naturel
‚Ä¢ Je peux vous aider avec la programmation Python

üí° **Conseils :**
‚Ä¢ Soyez sp√©cifique dans vos demandes
‚Ä¢ N'h√©sitez pas √† demander des exemples
‚Ä¢ Utilisez "aide" pour revoir cette aide"""

    def _generate_fallback_response(self, text: str) -> str:
        """
        G√©n√®re une r√©ponse de fallback en cas d'erreur du mod√®le principal
        
        Args:
            text: Texte de l'utilisateur
            
        Returns:
            R√©ponse de fallback naturelle
        """
        text_lower = text.lower()
        
        # Salutations
        if any(word in text_lower for word in ["salut", "bonjour", "hello", "hi", "bonsoir"]):
            return "Salut ! Comment √ßa va ? En quoi puis-je t'aider ?"
        
        # Questions d'aide
        if "help" in text_lower or "aide" in text_lower:
            return "Je peux t'aider avec la g√©n√©ration de code Python, l'analyse de documents, et r√©pondre √† tes questions techniques. Que veux-tu faire ?"
        
        # Demandes de code
        elif "g√©n√©r" in text_lower or "cr√©er" in text_lower or "fonction" in text_lower or "classe" in text_lower:
            if "fonction" in text_lower:
                return self._generate_simple_function(text)
            elif "classe" in text_lower:
                return self._generate_simple_class(text)
            else:
                return "Je peux g√©n√©rer du code pour toi ! Tu veux une fonction ou une classe ? Dis-moi ce que tu veux cr√©er."
        
        # Questions sur l'IA
        elif any(phrase in text_lower for phrase in ["qui es-tu", "que fais-tu", "qu'est-ce que tu fais"]):
            return "Je suis ton assistant IA local. Je peux coder, analyser des documents et r√©pondre √† tes questions. Qu'est-ce qui t'int√©resse ?"
        
        # R√©ponse g√©n√©rale naturelle
        else:
            return f"Je vois ! Et en quoi puis-je t'aider avec √ßa ? Tu veux que je g√©n√®re du code, que je t'explique quelque chose, ou autre chose ?"

    async def process_message(self, message: str) -> str:
        """
        Traite un message de mani√®re asynchrone
        
        Args:
            message: Message de l'utilisateur
            
        Returns:
            R√©ponse de l'IA
        """
        return self.process_text(message)

    def get_conversation_history(self) -> List[Dict[str, str]]:
        """
        R√©cup√®re l'historique de conversation
        
        Returns:
            Liste des messages
        """
        return self.conversation_manager.get_history()

    def clear_conversation(self):
        """
        Clear the conversation history and memory
        """
        self.conversation_manager.clear()
        self.conversation_memory.clear()
        
        # R√©initialiser aussi le session_context
        self.session_context = {
            "documents_processed": [],
            "code_files_processed": [],
            "last_document_type": None,
            "current_document": None
        }
        
        self.logger.info("Conversation cleared")

    def initialize_llm(self) -> bool:
        """Initialise les mod√®les LLM - Mode entreprise local uniquement"""
        try:
            # En mode entreprise, nous utilisons uniquement le mod√®le local
            return hasattr(self, 'local_ai') and self.local_ai is not None
        except Exception as e:
            self.logger.error(f"Erreur initialisation LLM: {e}")
            return False

    async def process_query(self, query: str, context: Optional[Dict] = None) -> Dict[str, Any]:
        """
        Traite une requ√™te utilisateur
        
        Args:
            query: Question/demande de l'utilisateur
            context: Contexte additionnel (fichiers, historique, etc.)
            
        Returns:
            R√©ponse structur√©e de l'IA
        """
        try:
            self.logger.info(f"Traitement de la requ√™te: {query[:100]}...")
            print(f"[AIEngine] Appel FAQ pour: '{query}' (async)")
            # 1. Tenter la FAQ ML d'abord
            response_ml = None
            if hasattr(self, 'ml_ai') and self.ml_ai is not None:
                try:
                    response_ml = self.ml_ai.predict(query)
                    self.logger.info(f"ML model response: {str(response_ml)[:50]}...")
                except Exception as e:
                    self.logger.warning(f"Erreur mod√®le ML: {e}")
            if response_ml is not None and str(response_ml).strip():
                # On sauvegarde l'√©change
                try:
                    self.conversation_manager.add_exchange(query, {"message": response_ml})
                except:
                    self.logger.warning("Impossible de sauvegarder la conversation (FAQ async)")
                return {"type": "faq", "message": response_ml, "success": True}

            # 2. Sinon, routage normal
            # Analyse de la requ√™te
            query_type = self._analyze_query_type(query)
            # Pr√©paration du contexte
            full_context = self._prepare_context(query, context)
            # Traitement selon le type
            if query_type == "conversation":
                response = await self._handle_conversation(query, full_context)
            elif query_type == "file_processing":
                response = await self._handle_file_processing(query, full_context)
            elif query_type == "code_generation":
                response = await self._handle_code_generation(query, full_context)
            elif query_type == "document_generation":
                response = await self._handle_document_generation(query, full_context)
            else:
                response = await self._handle_general_query(query, full_context)
            # Sauvegarde dans l'historique
            self.conversation_manager.add_exchange(query, response)
            return response
        except Exception as e:
            self.logger.error(f"Erreur lors du traitement: {e}")
            return {
                "type": "error",
                "message": f"D√©sol√©, une erreur s'est produite: {str(e)}",
                "success": False
            }
    
    def _analyze_query_type(self, query: str) -> str:
        """
        Analyse le type de requ√™te
        
        Args:
            query: Requ√™te utilisateur
            
        Returns:
            Type de requ√™te identifi√©
        """
        query_lower = query.lower()
        
        # PRIORIT√â 1 : V√©rifier d'abord les questions d'identit√©/capacit√©s (AVANT documents)
        identity_keywords = ["qui es-tu", "qui es tu", "qui √™tes vous", "comment tu t'appelles", "ton nom", "tu es qui", "tu es quoi"]
        capability_keywords = ["que peux tu", "que sais tu", "tes capacit√©s", "tu peux faire", "que fais-tu", "comment vas tu", "comment √ßa va"]
        
        if any(keyword in query_lower for keyword in identity_keywords + capability_keywords):
            return "conversation"  # Questions sur l'IA elle-m√™me
        
        # PRIORIT√â 2 : V√©rifier si c'est un texte incompr√©hensible/al√©atoire
        if len(query_lower) > 20 and not any(c.isspace() for c in query_lower[:20]):
            # Si plus de 20 caract√®res sans espaces = probablement du charabia
            return "conversation"
        
        # PRIORIT√â 3 : V√©rifier si on a des documents et si la question les concerne SP√âCIFIQUEMENT
        if hasattr(self.local_ai, 'conversation_memory'):
            stored_docs = self.local_ai.conversation_memory.get_document_content()
            if stored_docs:
                # Mots-cl√©s SP√âCIFIQUES pour questions sur les documents
                doc_question_keywords = [
                    "r√©sume", "explique", "analyse", "qu'est-ce que", "que dit", 
                    "contenu", "parle", "traite", "sujet", "doc", "document", 
                    "pdf", "docx", "fichier", "code"
                ]
                if any(keyword in query_lower for keyword in doc_question_keywords):
                    return "file_processing"
        
        # PRIORIT√â 4 : Mots-cl√©s pour la g√©n√©ration de code (NOUVEAU code, pas analyse)
        # Distinguer entre questions th√©oriques et demandes de g√©n√©ration
        question_words = ["comment", "qu'est-ce que", "c'est quoi", "que signifie", "explique", "expliquer"]
        is_theoretical_question = any(qword in query_lower for qword in question_words)
        
        code_generation_keywords = ["g√©n√®re", "cr√©e", "√©cris", "d√©veloppe", "programme", "script", "fonction", "classe"]
        if any(keyword in query_lower for keyword in code_generation_keywords):
            # Si c'est une question th√©orique (ex: "comment cr√©er une liste ?"), laisser le CustomAIModel s'en occuper
            if is_theoretical_question:
                return "general"  # Laisser le CustomAIModel traiter
            else:
                return "code_generation"  # Vraie demande de g√©n√©ration
        
        # PRIORIT√â 5 : Mots-cl√©s pour la g√©n√©ration de documents
        doc_keywords = ["cr√©er", "g√©n√©rer", "rapport", "r√©diger", "documenter"]
        if any(keyword in query_lower for keyword in doc_keywords):
            return "document_generation"
        
        return "conversation"
    
    def _prepare_context(self, query: str, context: Optional[Dict]) -> Dict[str, Any]:
        """
        Pr√©pare le contexte complet pour la requ√™te
        """
        full_context = {
            "query": query,
            "conversation_history": self.conversation_manager.get_recent_history(),
            "timestamp": self.file_manager.get_timestamp(),
            "user_context": context or {}
        }
        
        # Ajouter les documents stock√©s dans la m√©moire
        if hasattr(self.local_ai, 'conversation_memory'):
            stored_docs = self.local_ai.conversation_memory.stored_documents
            if stored_docs:
                full_context["stored_documents"] = stored_docs
                full_context["document_order"] = self.local_ai.conversation_memory.document_order
                self.logger.info(f"Contexte enrichi avec {len(stored_docs)} documents stock√©s")
        
        return full_context
    
    async def _handle_conversation(self, query: str, context: Dict) -> Dict[str, Any]:
        """
        G√®re les conversations g√©n√©rales
        """
        try:
            # Si on a des documents, les inclure dans le contexte
            prompt = query
            if 'stored_documents' in context and context['stored_documents']:
                prompt = f"""Contexte des documents disponibles:
                
"""
                for doc_name in context['stored_documents'].keys():
                    prompt += f"- {doc_name}\n"
                
                prompt += f"\nQuestion: {query}\n\nR√©ponds en tenant compte du contexte si pertinent."
            
            response = self.local_ai.generate_response(prompt)
            
            return {
                "type": "conversation",
                "message": response,
                "success": True
            }
        except Exception as e:
            self.logger.error(f"Erreur conversation: {e}")
            return {
                "type": "conversation",
                "message": f"Erreur lors de la conversation: {str(e)}",
                "success": False
            }
    
    async def _handle_file_processing(self, query: str, context: Dict) -> Dict[str, Any]:
        """
        G√®re le traitement de fichiers et questions sur les documents - VERSION CORRIG√âE
        """
        try:
            query_lower = query.lower()
            
            # R√©cup√©rer les documents disponibles
            all_docs = context.get('stored_documents', {})
            document_order = context.get('document_order', [])
            
            # NOUVELLE LOGIQUE : D√©tecter le type de document demand√© VS disponible
            requested_type = self._detect_requested_document_type(query_lower)
            available_types = self._get_available_document_types(all_docs)
            
            print(f"[DEBUG] Type demand√©: {requested_type}")
            print(f"[DEBUG] Types disponibles: {available_types}")
            
            # V√âRIFICATION DE COH√âRENCE
            if requested_type and requested_type not in available_types:
                return self._generate_type_mismatch_response(requested_type, available_types, all_docs)
            
            # D√©terminer le document cible selon le type demand√©
            target_document = None
            
            if requested_type:
                # Chercher un document du type demand√©
                target_document = self._find_document_by_type(all_docs, document_order, requested_type)
            
            # Gestion des r√©f√©rences num√©riques (premier, deuxi√®me, etc.)
            is_first_requested = any(term in query_lower for term in ['premier', '1er', '1√®re', 'premi√®re', 'document 1', 'le 1'])
            is_second_requested = any(term in query_lower for term in ['deuxi√®me', '2√®me', '2eme', 'seconde', 'document 2', 'le 2'])
            is_last_requested = any(term in query_lower for term in ['dernier', 'derni√®re', 'last'])
            
            if is_first_requested and len(document_order) >= 1:
                target_document = document_order[0]
            elif is_second_requested and len(document_order) >= 2:
                target_document = document_order[1]
            elif is_last_requested and document_order:
                target_document = document_order[-1]
            
            # Si aucun document sp√©cifique trouv√©, prendre le plus r√©cent
            if not target_document and document_order:
                target_document = document_order[-1]
            
            # Traitement sp√©cial pour les fichiers Python (explication d√©taill√©e)
            if target_document and target_document.lower().endswith('.py'):
                if any(word in query_lower for word in ['explique', 'analyse', 'code']):
                    return self._handle_python_code_explanation(target_document, all_docs)
            
            # Traitement standard pour les autres documents
            if target_document and target_document in all_docs:
                doc_data = all_docs[target_document]
                doc_content = doc_data.get('content') if isinstance(doc_data, dict) else str(doc_data)
                
                if doc_content:
                    # G√©n√©rer le r√©sum√© appropri√© selon le type de document
                    doc_type = self._determine_document_type(target_document)
                    
                    # D√©l√©guer la cr√©ation du r√©sum√© au mod√®le IA local
                    response = self.local_ai._create_universal_summary(doc_content, target_document, doc_type)
                    
                    return {
                        "type": "file_processing",
                        "message": response,
                        "success": True
                    }
            
            return {
                "type": "file_processing",
                "message": "Aucun document appropri√© trouv√© pour traiter votre demande.",
                "success": False
            }
            
        except Exception as e:
            print(f"‚ùå Erreur traitement fichier: {e}")
            return {
                "type": "file_processing",
                "message": f"Erreur lors du traitement : {str(e)}",
                "success": False
            }

    def _detect_requested_document_type(self, query_lower: str) -> Optional[str]:
        """
        D√©tecte le type de document sp√©cifiquement demand√© dans la requ√™te
        
        Returns:
            'pdf', 'docx', 'code', ou None si pas sp√©cifique
        """
        # D√©tection PDF
        if any(term in query_lower for term in ['pdf', 'le pdf', 'du pdf', 'ce pdf']):
            return 'pdf'
        
        # D√©tection DOCX/Word
        if any(term in query_lower for term in ['docx', 'doc', 'word', 'le docx', 'du docx', 'le doc', 'du doc']):
            return 'docx'
        
        # D√©tection Code
        if any(term in query_lower for term in ['code', 'py', 'python', 'script', 'programme', 'le code', 'du code']):
            return 'code'
        
        return None

    def _get_available_document_types(self, all_docs: Dict) -> List[str]:
        """
        D√©termine les types de documents disponibles
        
        Returns:
            Liste des types disponibles: ['pdf', 'docx', 'code']
        """
        available_types = []
        
        for doc_name, doc_data in all_docs.items():
            doc_type = self._determine_document_type(doc_name, doc_data)
            if doc_type not in available_types:
                available_types.append(doc_type)
        
        return available_types

    def _determine_document_type(self, doc_name: str, doc_data: Any = None) -> str:
        """
        D√©termine le type d'un document
        
        Returns:
            'pdf', 'docx', ou 'code'
        """
        doc_name_lower = doc_name.lower()
        
        if doc_name_lower.endswith('.pdf'):
            return 'pdf'
        elif doc_name_lower.endswith(('.docx', '.doc')):
            return 'docx'
        elif doc_name_lower.endswith(('.py', '.js', '.html', '.css', '.java', '.cpp', '.c')):
            return 'code'
        elif isinstance(doc_data, dict) and doc_data.get('type') == 'code':
            return 'code'
        else:
            # Par d√©faut, consid√©rer comme document texte
            return 'docx'

    def _find_document_by_type(self, all_docs: Dict, document_order: List, requested_type: str) -> Optional[str]:
        """
        Trouve un document du type demand√©
        
        Returns:
            Nom du document trouv√© ou None
        """
        # Chercher dans l'ordre inverse (le plus r√©cent en premier)
        for doc_name in reversed(document_order):
            if doc_name in all_docs:
                doc_type = self._determine_document_type(doc_name, all_docs[doc_name])
                if doc_type == requested_type:
                    return doc_name
        
        return None

    def _generate_type_mismatch_response(self, requested_type: str, available_types: List[str], all_docs: Dict) -> Dict[str, Any]:
        """
        G√©n√®re une r√©ponse quand le type demand√© n'est pas disponible
        """
        # Mappage pour un langage plus naturel
        type_names = {
            'pdf': 'fichier PDF',
            'docx': 'document Word/DOCX', 
            'code': 'fichier de code'
        }
        
        requested_name = type_names.get(requested_type, requested_type)
        
        # Construire la liste des documents disponibles
        available_docs = []
        for doc_name in all_docs.keys():
            doc_type = self._determine_document_type(doc_name, all_docs[doc_name])
            type_name = type_names.get(doc_type, doc_type)
            available_docs.append(f"‚Ä¢ **{doc_name}** ({type_name})")
        
        available_names = [type_names.get(t, t) for t in available_types]
        
        response = f"‚ùå **Document non trouv√©**\n\n"
        response += f"Vous demandez l'analyse d'un **{requested_name}**, mais je n'ai pas ce type de document en m√©moire.\n\n"
        
        if available_docs:
            response += f"üìÅ **Documents actuellement disponibles :**\n"
            response += "\n".join(available_docs)
            response += f"\n\nüí° **Suggestion :** Reformulez votre demande en utilisant le bon type :\n"
            
            if 'pdf' in available_types:
                response += f"‚Ä¢ \"r√©sume le PDF\" ou \"explique le PDF\"\n"
            if 'docx' in available_types:
                response += f"‚Ä¢ \"r√©sume le DOCX\" ou \"explique le document\"\n"
            if 'code' in available_types:
                response += f"‚Ä¢ \"explique le code\" ou \"analyse le Python\"\n"
        else:
            response += f"Aucun document n'est actuellement en m√©moire."
        
        return {
            "type": "file_processing",
            "message": response,
            "success": False
        }

    def _handle_python_code_explanation(self, target_document: str, all_docs: Dict) -> Dict[str, Any]:
        """
        G√®re l'explication d√©taill√©e des fichiers Python
        """
        try:
            doc_data = all_docs.get(target_document, {})
            doc_content = doc_data.get('content') if isinstance(doc_data, dict) else str(doc_data)
            
            if not doc_content:
                return {
                    "type": "file_processing", 
                    "message": f"Le fichier {target_document} semble vide.",
                    "success": False
                }
            
            # Cr√©er un fichier temporaire pour l'analyse
            import os, tempfile
            with tempfile.NamedTemporaryFile('w', delete=False, suffix='.py', encoding='utf-8') as tmpf:
                tmpf.write(doc_content)
                tmp_path = tmpf.name
            
            try:
                # Utiliser le code_processor pour l'explication d√©taill√©e
                explanation = self.code_processor.generate_detailed_explanation(
                    tmp_path, 
                    real_file_name=os.path.basename(target_document)
                )
                
                return {
                    "type": "file_processing",
                    "message": explanation,
                    "success": True
                }
                
            finally:
                # Nettoyer le fichier temporaire
                try:
                    os.remove(tmp_path)
                except Exception:
                    pass
                    
        except Exception as e:
            return {
                "type": "file_processing",
                "message": f"Erreur lors de l'explication du code : {str(e)}",
                "success": False
            }
    
    async def _handle_code_generation(self, query: str, context: Dict) -> Dict[str, Any]:
        """
        G√®re la g√©n√©ration de code
        """
        try:
            # G√©n√©rer le code sans await (methode sync)
            code = self.code_generator.generate_code(query, context)
            
            # Cr√©er un message d'accompagnement intelligent
            language = self._detect_code_language(query)
            accompaniment = self._create_code_accompaniment(query, language)
            
            # Formater la r√©ponse compl√®te
            full_response = f"{accompaniment}\n\n```{language}\n{code}\n```"
            
            return {
                "type": "code_generation",
                "code": code,
                "message": full_response,
                "success": True
            }
        except Exception as e:
            self.logger.error(f"Erreur g√©n√©ration code: {e}")
            return {
                "type": "code_generation", 
                "message": f"‚ùå Erreur lors de la g√©n√©ration de code: {str(e)}",
                "success": False
            }
    
    async def _handle_document_generation(self, query: str, context: Dict) -> Dict[str, Any]:
        """
        G√®re la g√©n√©ration de documents
        """
        try:
            # G√©n√©rer le document sans await (methode sync)
            document = self.document_generator.generate_document(query, context)
            
            return {
                "type": "document_generation",
                "document": document,
                "message": "Document g√©n√©r√© avec succ√®s",
                "success": True
            }
        except Exception as e:
            self.logger.error(f"Erreur g√©n√©ration document: {e}")
            return {
                "type": "document_generation",
                "message": f"Erreur lors de la g√©n√©ration de document: {str(e)}",
                "success": False
            }
    
    async def _handle_general_query(self, query: str, context: Dict) -> Dict[str, Any]:
        """
        G√®re les requ√™tes g√©n√©rales
        """
        try:
            # Construire le prompt avec contexte si disponible
            prompt = query
            if 'stored_documents' in context and context['stored_documents']:
                prompt = f"""Contexte des documents disponibles:
                
"""
                for doc_name in context['stored_documents'].keys():
                    prompt += f"- {doc_name}\n"
                
                prompt += f"\nQuestion: {query}\n\nR√©ponds en tenant compte du contexte si pertinent."
            
            response = self.local_ai.generate_response(prompt)
            
            return {
                "type": "general",
                "message": response,
                "success": True
            }
        except Exception as e:
            self.logger.error(f"Erreur requ√™te g√©n√©rale: {e}")
            return {
                "type": "general",
                "message": f"Erreur lors du traitement: {str(e)}",
                "success": False
            }
    
    def _detect_code_language(self, query: str) -> str:
        """D√©tecte le langage de programmation demand√©"""
        query_lower = query.lower()
        
        if any(word in query_lower for word in ["python", "py"]):
            return "python"
        elif any(word in query_lower for word in ["javascript", "js", "node"]):
            return "javascript"
        elif any(word in query_lower for word in ["html", "page web", "site"]):
            return "html"
        elif any(word in query_lower for word in ["css", "style"]):
            return "css"
        else:
            return "python"  # Par d√©faut
    
    def _create_code_accompaniment(self, query: str, language: str) -> str:
        """Cr√©e un message d'accompagnement intelligent pour le code"""
        query_lower = query.lower()
        
        # Messages sp√©cifiques selon le type de code demand√©
        if any(word in query_lower for word in ["factorielle", "factorial"]):
            return f"üî¢ **Code pour calculer une factorielle en {language.capitalize()}**\n\nVoici une impl√©mentation efficace avec gestion des cas d'erreur :"
        
        elif any(word in query_lower for word in ["hello", "world", "bonjour"]):
            return f"üëã **Programme Hello World en {language.capitalize()}**\n\nLe classique pour d√©buter :"
        
        elif any(word in query_lower for word in ["fibonacci", "fibo"]):
            return f"üåÄ **S√©quence de Fibonacci en {language.capitalize()}**\n\nCode optimis√© pour g√©n√©rer la suite :"
        
        elif any(word in query_lower for word in ["tri", "sort", "trier"]):
            return f"üìä **Algorithme de tri en {language.capitalize()}**\n\nImpl√©mentation d'un tri efficace :"
        
        elif any(word in query_lower for word in ["classe", "class", "objet"]):
            return f"üèóÔ∏è **Classe en {language.capitalize()}**\n\nStructure orient√©e objet :"
        
        elif any(word in query_lower for word in ["fonction", "function", "def"]):
            return f"‚öôÔ∏è **Fonction en {language.capitalize()}**\n\nCode modulaire et r√©utilisable :"
        
        elif any(word in query_lower for word in ["api", "web", "serveur"]):
            return f"üåê **Code pour API/Web en {language.capitalize()}**\n\nStructure pour service web :"
        
        else:
            return f"üíª **Code g√©n√©r√© en {language.capitalize()}**\n\nVoici une impl√©mentation pour votre demande :"
    
    def get_status(self) -> Dict[str, Any]:
        """
        Retourne le statut du moteur IA
        """
        return {
            "engine": "running",
            "mode": "enterprise_local",
            "llm_status": "local_custom_model_only",
            "conversation_count": len(self.conversation_manager.history) if hasattr(self.conversation_manager, 'history') else 0,
            "local_ai_stats": self.local_ai.get_stats() if hasattr(self.local_ai, 'get_stats') else {},
            "config": self.config
        }
