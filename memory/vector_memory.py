"""
Vector Memory - Gestionnaire de m√©moire vectorielle avec recherche s√©mantique
Remplace le million_token_context_manager avec de vraies capacit√©s ML
Supporte ChromaDB et FAISS, tokenization correcte, chiffrement AES-256
"""

import hashlib
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional
import re

# Note: Le mode offline HuggingFace est g√©r√© intelligemment dans core.shared
# Il t√©l√©charge automatiquement le mod√®le au premier lancement si n√©cessaire

try:
    import chromadb

    CHROMADB_AVAILABLE = True
except ImportError:
    CHROMADB_AVAILABLE = False
    print("‚ö†Ô∏è ChromaDB non disponible. Installez: pip install chromadb")

try:
    from core.shared import get_shared_embedding_model, is_embeddings_available

    EMBEDDINGS_AVAILABLE = is_embeddings_available()
except ImportError:
    EMBEDDINGS_AVAILABLE = False
    print(
        "‚ö†Ô∏è Sentence-transformers non disponible. Installez: pip install sentence-transformers"
    )
    get_shared_embedding_model = lambda: None

try:
    from transformers import AutoTokenizer

    TOKENIZER_AVAILABLE = True
except ImportError:
    TOKENIZER_AVAILABLE = False
    print("‚ö†Ô∏è Transformers non disponible. Installez: pip install transformers")

try:
    from cryptography.fernet import Fernet
    import base64

    ENCRYPTION_AVAILABLE = True
except ImportError:
    ENCRYPTION_AVAILABLE = False
    print("‚ö†Ô∏è Cryptography non disponible. Installez: pip install cryptography")


class VectorMemory:
    """
    Gestionnaire de m√©moire vectorielle avec recherche s√©mantique

    Fonctionnalit√©s:
    - Tokenization correcte (transformers)
    - Embeddings s√©mantiques (sentence-transformers)
    - Stockage vectoriel (ChromaDB/FAISS)
    - Chiffrement AES-256 (optionnel)
    - Recherche par similarit√© cosinus
    """

    def __init__(
        self,
        max_tokens: int = 1_000_000,
        chunk_size: int = 512,
        chunk_overlap: int = 50,
        embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2",
        storage_dir: str = "memory/vector_store",
        enable_encryption: bool = False,
        encryption_key: Optional[str] = None,
    ):
        """
        Initialise le gestionnaire de m√©moire vectorielle

        Args:
            max_tokens: Limite maximale de tokens (1M par d√©faut)
            chunk_size: Taille des chunks en tokens
            chunk_overlap: Chevauchement entre chunks
            embedding_model: Mod√®le d'embeddings √† utiliser
            storage_dir: R√©pertoire de stockage
            enable_encryption: Activer le chiffrement AES-256
            encryption_key: Cl√© de chiffrement (g√©n√©r√©e si None)
        """
        self.max_tokens = max_tokens
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.current_tokens = 0

        # Configuration du stockage
        self.storage_dir = Path(storage_dir)
        self.storage_dir.mkdir(parents=True, exist_ok=True)

        # Chiffrement
        self.enable_encryption = enable_encryption and ENCRYPTION_AVAILABLE
        if self.enable_encryption:
            self._init_encryption(encryption_key)

        # Tokenizer (vrai comptage de tokens)
        if TOKENIZER_AVAILABLE:
            try:
                self.tokenizer = AutoTokenizer.from_pretrained("gpt2")
                print("‚úÖ Tokenizer GPT-2 charg√©")
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur chargement tokenizer: {e}")
                self.tokenizer = None
        else:
            self.tokenizer = None

        # Mod√®le d'embeddings partag√© (d√©j√† charg√© au d√©marrage dans core.shared)
        self.embedding_model = get_shared_embedding_model()

        # Base vectorielle ChromaDB
        if CHROMADB_AVAILABLE:
            try:
                self.chroma_client = chromadb.PersistentClient(
                    path=str(self.storage_dir / "chroma_db")
                )

                # Collections pour diff√©rents types de donn√©es
                self.conversation_collection = (
                    self.chroma_client.get_or_create_collection(
                        name="conversations", metadata={"hnsw:space": "cosine"}
                    )
                )

                self.document_collection = self.chroma_client.get_or_create_collection(
                    name="documents", metadata={"hnsw:space": "cosine"}
                )

                print("‚úÖ ChromaDB initialis√©")
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur ChromaDB: {e}")
                self.chroma_client = None
                self.conversation_collection = None
                self.document_collection = None
        else:
            self.chroma_client = None
            self.conversation_collection = None
            self.document_collection = None

        # M√©tadonn√©es et statistiques
        self.documents = {}
        self.stats = {
            "documents_added": 0,
            "chunks_created": 0,
            "total_tokens": 0,
            "last_updated": None,
            "encryption_enabled": self.enable_encryption,
        }

        print(f"‚úÖ VectorMemory initialis√© (max: {max_tokens:,} tokens)")

    def _init_encryption(self, encryption_key: Optional[str] = None):
        """Initialise le syst√®me de chiffrement AES-256"""
        if not ENCRYPTION_AVAILABLE:
            print("‚ö†Ô∏è Chiffrement d√©sactiv√©: cryptography non disponible")
            self.enable_encryption = False
            return

        key_file = self.storage_dir / ".encryption_key"

        if encryption_key:
            # Utiliser la cl√© fournie
            key_bytes = encryption_key.encode()
        elif key_file.exists():
            # Charger la cl√© existante
            with open(key_file, "rb") as f:
                key_bytes = f.read()
        else:
            # G√©n√©rer une nouvelle cl√©
            key_bytes = Fernet.generate_key()
            with open(key_file, "wb") as f:
                f.write(key_bytes)
            print("üîê Nouvelle cl√© de chiffrement g√©n√©r√©e")

        self.cipher = Fernet(key_bytes)
        print("‚úÖ Chiffrement AES-256 activ√©")

    def _encrypt(self, text: str) -> str:
        """Chiffre un texte"""
        if not self.enable_encryption:
            return text
        encrypted = self.cipher.encrypt(text.encode())
        return base64.b64encode(encrypted).decode()

    def _decrypt(self, encrypted_text: str) -> str:
        """D√©chiffre un texte"""
        if not self.enable_encryption:
            return encrypted_text
        encrypted = base64.b64decode(encrypted_text.encode())
        return self.cipher.decrypt(encrypted).decode()

    def count_tokens(self, text: str) -> int:
        """
        Compte le nombre r√©el de tokens (pas de mots)

        Args:
            text: Texte √† analyser

        Returns:
            Nombre de tokens
        """
        if self.tokenizer:
            # Vrai comptage avec tokenizer
            tokens = self.tokenizer.encode(text, add_special_tokens=False)
            return len(tokens)
        else:
            # Fallback: approximation (1 mot ‚âà 0.75 tokens)
            words = text.split()
            return int(len(words) * 0.75)

    def split_into_chunks(self, text: str) -> List[str]:
        """
        Divise le texte en chunks avec chevauchement

        Args:
            text: Texte √† diviser

        Returns:
            Liste de chunks
        """
        if self.tokenizer:
            # D√©coupage bas√© sur les vrais tokens
            tokens = self.tokenizer.encode(text, add_special_tokens=False)
            chunks = []

            start = 0
            while start < len(tokens):
                end = min(start + self.chunk_size, len(tokens))
                chunk_tokens = tokens[start:end]
                chunk_text = self.tokenizer.decode(chunk_tokens)
                chunks.append(chunk_text)

                # Avancer avec chevauchement
                start += self.chunk_size - self.chunk_overlap

            return chunks
        else:
            # Fallback: d√©coupage par mots
            words = text.split()
            chunks = []
            word_chunk_size = int(self.chunk_size / 0.75)  # Approximation
            word_overlap = int(self.chunk_overlap / 0.75)

            start = 0
            while start < len(words):
                end = min(start + word_chunk_size, len(words))
                chunk_words = words[start:end]
                chunks.append(" ".join(chunk_words))
                start += word_chunk_size - word_overlap

            return chunks

    def add_document(
        self,
        content: str,
        document_name: str = "",
        metadata: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        Ajoute un document √† la m√©moire vectorielle

        Args:
            content: Contenu du document
            document_name: Nom du document
            metadata: M√©tadonn√©es additionnelles

        Returns:
            Informations sur l'ajout
        """
        try:
            # G√©n√©rer ID unique
            doc_id = self._generate_document_id(content, document_name)

            # V√©rifier doublon
            if doc_id in self.documents:
                return {
                    "document_id": doc_id,
                    "status": "duplicate",
                    "chunks_created": 0,
                    "tokens_added": 0,
                }

            # Compter tokens
            total_tokens = self.count_tokens(content)

            # V√©rifier capacit√©
            if self.current_tokens + total_tokens > self.max_tokens:
                self._cleanup_old_documents(total_tokens)

            # Diviser en chunks
            chunks = self.split_into_chunks(content)

            # G√©n√©rer embeddings et stocker
            chunk_ids = []
            embeddings_list = []

            for i, chunk_text in enumerate(chunks):
                chunk_id = f"{doc_id}_chunk_{i}"
                chunk_tokens = self.count_tokens(chunk_text)

                # Chiffrer si activ√©
                stored_text = (
                    self._encrypt(chunk_text) if self.enable_encryption else chunk_text
                )

                # G√©n√©rer embedding
                if self.embedding_model:
                    embedding = self.embedding_model.encode(chunk_text).tolist()
                    embeddings_list.append(embedding)
                else:
                    embedding = None

                # Stocker dans ChromaDB
                if self.document_collection and embedding:
                    chunk_metadata = {
                        "document_id": doc_id,
                        "document_name": document_name,
                        "chunk_index": i,
                        "tokens": chunk_tokens,
                        "created": datetime.now().isoformat(),
                        "encrypted": self.enable_encryption,
                        **(metadata or {}),
                    }

                    self.document_collection.add(
                        ids=[chunk_id],
                        embeddings=[embedding],
                        documents=[stored_text],
                        metadatas=[chunk_metadata],
                    )

                chunk_ids.append(chunk_id)
                self.current_tokens += chunk_tokens

            # Enregistrer m√©tadonn√©es document
            self.documents[doc_id] = {
                "name": document_name or f"Document_{len(self.documents)}",
                "chunks": chunk_ids,
                "total_tokens": total_tokens,
                "created": datetime.now().isoformat(),
                "preview": content[:200] + "..." if len(content) > 200 else content,
                "metadata": metadata,
            }

            # Mettre √† jour statistiques
            self.stats["documents_added"] += 1
            self.stats["chunks_created"] += len(chunks)
            self.stats["total_tokens"] = self.current_tokens
            self.stats["last_updated"] = datetime.now().isoformat()

            return {
                "document_id": doc_id,
                "document_name": document_name,
                "chunks_created": len(chunks),
                "tokens_added": total_tokens,
                "status": "success",
            }

        except Exception as e:
            return {"error": str(e), "status": "error"}

    def search_similar(
        self, query: str, n_results: int = 5, collection_type: str = "document"
    ) -> List[Dict[str, Any]]:
        """
        Recherche s√©mantique par similarit√©

        Args:
            query: Requ√™te de recherche
            n_results: Nombre de r√©sultats
            collection_type: "document" ou "conversation"

        Returns:
            Liste de r√©sultats avec scores
        """
        if not self.embedding_model:
            print("‚ö†Ô∏è Recherche s√©mantique non disponible (embeddings d√©sactiv√©s)")
            return []

        collection = (
            self.document_collection
            if collection_type == "document"
            else self.conversation_collection
        )

        if not collection:
            return []

        try:
            # G√©n√©rer embedding de la requ√™te
            query_embedding = self.embedding_model.encode(query).tolist()

            # Rechercher dans ChromaDB
            results = collection.query(
                query_embeddings=[query_embedding], n_results=n_results
            )

            # Formater les r√©sultats
            formatted_results = []
            if results and results["ids"] and len(results["ids"][0]) > 0:
                for i, chunk_id in enumerate(results["ids"][0]):
                    # D√©chiffrer si n√©cessaire
                    content = results["documents"][0][i]
                    if results["metadatas"][0][i].get("encrypted", False):
                        content = self._decrypt(content)

                    formatted_results.append(
                        {
                            "chunk_id": chunk_id,
                            "content": content,
                            "metadata": results["metadatas"][0][i],
                            "distance": (
                                results["distances"][0][i]
                                if "distances" in results
                                else None
                            ),
                        }
                    )

            return formatted_results

        except Exception as e:
            print(f"‚ö†Ô∏è Erreur recherche: {e}")
            return []

    def get_relevant_context(
        self, query: str, max_chunks: int = 10, collection_type: str = "document"
    ) -> str:
        """
        R√©cup√®re le contexte le plus pertinent (API compatible avec ancien syst√®me)

        Args:
            query: Requ√™te de recherche
            max_chunks: Nombre maximum de chunks
            collection_type: Type de collection

        Returns:
            Contexte consolid√©
        """
        results = self.search_similar(
            query, n_results=max_chunks, collection_type=collection_type
        )

        if not results:
            return "Aucun contexte pertinent trouv√©."

        context_parts = []
        for res in results:
            metadata = res["metadata"]
            doc_name = metadata.get("document_name", "Document")
            chunk_idx = metadata.get("chunk_index", 0)

            context_parts.append(
                f"--- {doc_name} (Chunk {chunk_idx}) ---\n" f"{res['content']}\n"
            )

        return "\n".join(context_parts)

    def _generate_document_id(self, content: str, name: str) -> str:
        """G√©n√®re un ID unique pour un document"""
        content_hash = hashlib.md5(content.encode()).hexdigest()[:8]
        name_clean = re.sub(r"[^a-zA-Z0-9]", "_", name)[:20]
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        return f"{name_clean}_{content_hash}_{timestamp}"

    def _cleanup_old_documents(self, tokens_needed: int):
        """Nettoie les anciens documents pour faire de la place"""
        docs_by_date = sorted(self.documents.items(), key=lambda x: x[1]["created"])

        tokens_freed = 0
        for doc_id, doc_info in docs_by_date:
            if tokens_freed >= tokens_needed:
                break

            # Supprimer chunks de ChromaDB
            if self.document_collection:
                self.document_collection.delete(ids=doc_info["chunks"])

            tokens_freed += doc_info["total_tokens"]
            self.current_tokens -= doc_info["total_tokens"]
            del self.documents[doc_id]

        print(f"üßπ Nettoyage: {tokens_freed:,} tokens lib√©r√©s")

    def get_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques"""
        return {
            **self.stats,
            "current_tokens": self.current_tokens,
            "documents_count": len(self.documents),
            "max_tokens": self.max_tokens,
            "usage_percent": (self.current_tokens / self.max_tokens) * 100,
            "embeddings_enabled": self.embedding_model is not None,
            "chromadb_enabled": self.chroma_client is not None,
            "tokenizer": "transformers" if self.tokenizer else "fallback",
        }

    def clear_all(self):
        """Vide toute la m√©moire"""
        if self.document_collection:
            # ChromaDB ne permet pas de clear directement, on supprime et recr√©e
            try:
                self.chroma_client.delete_collection("documents")
                self.document_collection = self.chroma_client.create_collection(
                    name="documents", metadata={"hnsw:space": "cosine"}
                )
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur clear documents: {e}")

        if self.conversation_collection:
            try:
                self.chroma_client.delete_collection("conversations")
                self.conversation_collection = self.chroma_client.create_collection(
                    name="conversations", metadata={"hnsw:space": "cosine"}
                )
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur clear conversations: {e}")

        self.documents = {}
        self.current_tokens = 0
        self.stats = {
            "documents_added": 0,
            "chunks_created": 0,
            "total_tokens": 0,
            "last_updated": datetime.now().isoformat(),
            "encryption_enabled": self.enable_encryption,
        }

        print("üßπ M√©moire vid√©e")

    def cleanup(self):
        """
        Nettoie proprement les ressources (ChromaDB, threads, etc.)
        Doit √™tre appel√© avant de terminer le programme pour √©viter le blocage
        """
        try:
            print("üßπ Nettoyage des ressources VectorMemory...")

            # Fermer ChromaDB proprement
            if self.chroma_client:
                try:
                    # ChromaDB PersistentClient n'a pas de m√©thode close() explicite
                    # mais on peut forcer la lib√©ration des ressources
                    self.chroma_client = None
                    self.document_collection = None
                    self.conversation_collection = None
                except Exception as e:
                    print(f"‚ö†Ô∏è Erreur fermeture ChromaDB: {e}")

            # Lib√©rer le mod√®le d'embeddings
            if self.embedding_model:
                self.embedding_model = None

            # Lib√©rer le tokenizer
            if self.tokenizer:
                self.tokenizer = None

            print("‚úÖ Ressources VectorMemory lib√©r√©es")

        except Exception as e:
            print(f"‚ö†Ô∏è Erreur cleanup VectorMemory: {e}")


if __name__ == "__main__":
    # Tests
    print("üß™ Test VectorMemory")

    memory = VectorMemory(max_tokens=100000, chunk_size=256, enable_encryption=True)

    # Test ajout document
    TEST_CONTENT = (
        "Python est un langage de programmation puissant et facile √† apprendre. " * 50
    )
    result = memory.add_document(TEST_CONTENT, "Test Python")
    print(f"‚úÖ Document ajout√©: {result}")

    # Test recherche
    if memory.embedding_model:
        search_results = memory.search_similar("Python programmation")
        print(f"‚úÖ Recherche: {len(search_results)} r√©sultats")

        CONTEXT = memory.get_relevant_context("Python")
        print(f"‚úÖ Contexte: {len(CONTEXT)} caract√®res")

    # Stats
    stats = memory.get_stats()
    print(f"üìä Stats: {stats}")
