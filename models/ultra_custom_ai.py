"""
UltraCustomAIModel - Syst√®me IA avec contexte 1M tokens
Mod√®le personnalis√© avec capacit√©s √©tendues de contexte
"""

import os
import sys
import re
from typing import Dict, Any, List, Optional
from pathlib import Path

# Ajout du chemin racine pour les imports
current_dir = Path(__file__).parent.parent
if str(current_dir) not in sys.path:
    sys.path.insert(0, str(current_dir))

try:
    from models.million_token_context_manager import MillionTokenContextManager
except ImportError:
    # Fallback si le gestionnaire n'est pas encore cr√©√©
    MillionTokenContextManager = None

# Import du calculateur intelligent pour le syst√®me Ultra
try:
    from utils.intelligent_calculator import intelligent_calculator
    CALCULATOR_AVAILABLE = True
except ImportError:
    CALCULATOR_AVAILABLE = False
    print("‚ö†Ô∏è Calculateur intelligent non disponible pour Ultra")

# Import des processeurs avanc√©s pour le syst√®me Ultra
try:
    from processors.pdf_processor import PDFProcessor
    from processors.docx_processor import DOCXProcessor
    from processors.code_processor import CodeProcessor
    ADVANCED_PROCESSORS_AVAILABLE = True
    print("üîß Processeurs avanc√©s charg√©s pour Ultra")
except ImportError as e:
    ADVANCED_PROCESSORS_AVAILABLE = False
    print(f"‚ö†Ô∏è Processeurs avanc√©s non disponibles: {e}")

class UltraCustomAIModel:
    """
    Mod√®le IA Ultra avec contexte √©tendu de 1 million de tokens
    """
    
    def __init__(self, base_ai_engine=None):
        """
        Initialise le mod√®le Ultra
        
        Args:
            base_ai_engine: Moteur IA de base √† √©tendre
        """
        self.base_ai = base_ai_engine
        self.is_ultra_available = True
        self.context_stats = {
            'total_tokens': 0,
            'documents_processed': 0,
            'chunks_created': 0,
            'max_context_length': 1000000  # 1M tokens
        }
        
        # Initialiser le gestionnaire de contexte 1M tokens
        if MillionTokenContextManager:
            self.context_manager = MillionTokenContextManager()
        else:
            self.context_manager = None
            print("‚ö†Ô∏è Gestionnaire 1M tokens non disponible, mode de base activ√©")
        
        # Initialisation des processeurs avanc√©s pour Ultra
        if ADVANCED_PROCESSORS_AVAILABLE:
            self.pdf_processor = PDFProcessor()
            self.docx_processor = DOCXProcessor()
            self.code_processor = CodeProcessor()
            print("üîß Processeurs Ultra initialis√©s: PDF, DOCX, Code")
        else:
            self.pdf_processor = None
            self.docx_processor = None
            self.code_processor = None
            print("‚ö†Ô∏è Processeurs Ultra non disponibles")
    
    def generate_response(self, user_input: str, context: str = "", **kwargs) -> Dict[str, Any]:
        """
        G√©n√®re une r√©ponse en utilisant le contexte √©tendu
        
        Args:
            user_input: Message de l'utilisateur
            context: Contexte additionnel
            **kwargs: Param√®tres suppl√©mentaires
            
        Returns:
            Dict contenant la r√©ponse et les statistiques
        """
        try:
            # üßÆ PRIORIT√â 1: V√©rification si c'est un calcul (m√™me en mode Ultra)
            if CALCULATOR_AVAILABLE and intelligent_calculator.is_calculation_request(user_input):
                print(f"üßÆ Calcul Ultra d√©tect√©: {user_input}")
                calc_result = intelligent_calculator.calculate(user_input)
                calc_response = intelligent_calculator.format_response(calc_result)
                
                return {
                    'response': calc_response,
                    'success': True,
                    'calculation': True,
                    'ultra_mode': True,
                    'ultra_stats': self.get_context_stats(),
                    'calc_details': calc_result
                }
            
            # üìÑ PRIORIT√â 2: V√©rification si c'est une demande de r√©sum√© de document
            user_lower = user_input.lower()
            is_document_request = any(keyword in user_lower for keyword in [
                'r√©sume', 'r√©sumer', 'r√©sum√©', 'summary', 'summarize', 
                'pdf', 'docx', 'document', 'fichier', 'analyze', 'analyse'
            ])
            
            if is_document_request:
                print(f"üìÑ [ULTRA] Demande de traitement de document d√©tect√©e")
                
                # Chercher dans le contexte stock√© AVANT tout
                relevant_context = self._search_in_stored_context(user_input)
                
                if relevant_context:
                    print(f"üß† [ULTRA] Documents trouv√©s, d√©l√©gation au custom_ai_model pour traitement avanc√©")
                    
                    # D√âL√âGUER au custom_ai_model pour le traitement avanc√©
                    if self.base_ai and hasattr(self.base_ai, 'local_ai'):
                        try:
                            # Pr√©parer le prompt avec le contexte trouv√©
                            enhanced_prompt = f"CONTEXTE ULTRA (1M tokens):\n{relevant_context}\n\nQUESTION: {user_input}"
                            
                            # Utiliser la m√©thode de traitement de document du custom_ai_model
                            custom_response = self.base_ai.local_ai.generate_response(enhanced_prompt)
                            
                            return {
                                'response': custom_response,
                                'success': True,
                                'ultra_mode': True,
                                'context_used': True,
                                'processing_method': 'custom_ai_model_advanced',
                                'context_length': len(relevant_context),
                                'ultra_stats': self.get_context_stats()
                            }
                            
                        except Exception as e:
                            print(f"‚ö†Ô∏è [ULTRA] Erreur d√©l√©gation custom_ai_model: {e}")
                            # Fallback vers traitement Ultra simple
                            pass
                
                # Si pas de documents dans Ultra mais c'est une demande de document,
                # v√©rifier la m√©moire classique directement
                elif self.base_ai and hasattr(self.base_ai, 'local_ai'):
                    print(f"üîç [ULTRA] Pas de docs en Ultra, v√©rification m√©moire classique...")
                    
                    try:
                        # Laisser le custom_ai_model g√©rer compl√®tement
                        custom_response = self.base_ai.local_ai.generate_response(user_input)
                        
                        return {
                            'response': custom_response,
                            'success': True,
                            'ultra_mode': True,
                            'context_used': False,
                            'processing_method': 'custom_ai_model_fallback',
                            'ultra_stats': self.get_context_stats()
                        }
                        
                    except Exception as e:
                        print(f"‚ö†Ô∏è [ULTRA] Erreur fallback custom_ai_model: {e}")
            
            # Construire le contexte √©tendu AVANT de traiter
            extended_context = self._build_extended_context(context)
            
            # Chercher dans le contexte stock√© AVANT d'utiliser l'IA
            relevant_context = self._search_in_stored_context(user_input)
            
            # Si on trouve des informations pertinentes dans le contexte stock√©
            if relevant_context:
                print(f"üß† [ULTRA] Utilisation du contexte stock√© pour: {user_input[:50]}...")
                print(f"üìä [ULTRA] Contexte trouv√©: {len(relevant_context)} caract√®res")
                
                # Construire une r√©ponse √† partir du contexte stock√©
                context_response = self._generate_from_context(user_input, relevant_context)
                if context_response:
                    return {
                        'response': context_response,
                        'success': True,
                        'ultra_mode': True,
                        'context_used': True,
                        'context_length': len(relevant_context),
                        'ultra_stats': self.get_context_stats()
                    }
                else:
                    # Si la g√©n√©ration √©choue, retourner au moins le contexte brut
                    print("‚ö†Ô∏è [ULTRA] G√©n√©ration √©chou√©e, retour du contexte brut")
                    return {
                        'response': f"üìö **Documents trouv√©s dans la m√©moire Ultra:**\n\n{relevant_context[:2000]}{'...' if len(relevant_context) > 2000 else ''}",
                        'success': True,
                        'ultra_mode': True,
                        'context_used': True,
                        'context_length': len(relevant_context),
                        'ultra_stats': self.get_context_stats()
                    }
            else:
                print(f"‚ùå [ULTRA] Aucun contexte pertinent trouv√© pour: {user_input[:50]}")
                # V√©rifier si on a des documents en m√©moire
                if hasattr(self, 'documents_storage') and self.documents_storage:
                    doc_count = len(self.documents_storage)
                    doc_names = [doc.get('name', 'Sans nom') for doc in self.documents_storage.values()]
                    print(f"üìö [ULTRA] Documents disponibles ({doc_count}): {', '.join(doc_names[:3])}")
                    
                    # Retourner une r√©ponse informative sur les documents disponibles
                    return {
                        'response': f"üìö **J'ai {doc_count} document(s) en m√©moire Ultra**: {', '.join(doc_names[:5])}{'...' if len(doc_names) > 5 else ''}\n\nVous pouvez me demander de r√©sumer un document sp√©cifique ou poser une question plus pr√©cise sur le contenu.",
                        'success': True,
                        'ultra_mode': True,
                        'context_used': False,
                        'ultra_stats': self.get_context_stats()
                    }
                else:
                    print("üì≠ [ULTRA] Aucun document en m√©moire")
            
            # Si pas de contexte pertinent, utiliser le moteur IA de base avec le contexte √©tendu
            print(f"ü§ñ [ULTRA] Utilisation IA de base avec contexte √©tendu...")
            
            # Utiliser le moteur IA de base si disponible
            if self.base_ai:
                # Combiner user_input avec le contexte √©tendu
                enhanced_input = f"CONTEXTE: {extended_context}\n\nQUESTION: {user_input}" if extended_context else user_input
                
                # AIEngine utilise process_text au lieu de generate_response
                if hasattr(self.base_ai, 'process_text'):
                    ai_response = self.base_ai.process_text(enhanced_input)
                    response = {
                        'response': ai_response,
                        'success': True
                    }
                elif hasattr(self.base_ai, 'process_message'):
                    # Version async
                    import asyncio
                    ai_response = asyncio.run(self.base_ai.process_message(user_input))
                    response = {
                        'response': ai_response,
                        'success': True
                    }
                else:
                    # Fallback si aucune m√©thode disponible
                    response = {
                        'response': f"M√©thode de traitement non trouv√©e sur {type(self.base_ai).__name__}",
                        'success': False
                    }
            else:
                # R√©ponse de fallback
                response = {
                    'response': f"[ULTRA MODE] Traitement de: {user_input[:100]}...",
                    'success': True
                }
            
            # Ajouter les statistiques Ultra
            if isinstance(response, dict):
                response['ultra_stats'] = self.get_context_stats()
                response['ultra_mode'] = True
                response['context_length'] = len(extended_context)
            
            return response
            
        except Exception as e:
            return {
                'response': f"Erreur Ultra: {str(e)}",
                'success': False,
                'error': str(e),
                'ultra_mode': True
            }
    
    def add_document_to_context(self, document_content: str, document_name: str = "") -> Dict[str, Any]:
        """
        Ajoute un document au contexte 1M tokens
        
        Args:
            document_content: Contenu du document
            document_name: Nom du document
            
        Returns:
            Statistiques d'ajout
        """
        if not self.context_manager:
            return {'success': False, 'error': 'Gestionnaire de contexte indisponible'}
        
        try:
            # Initialiser le stockage de documents si n√©cessaire
            if not hasattr(self, 'documents_storage'):
                self.documents_storage = {}
            
            # Ajouter l'import time si n√©cessaire
            import time
            
            # Stocker le document pour recherche directe
            doc_id = f"doc_{len(self.documents_storage)}_{hash(document_content) % 10000}"
            self.documents_storage[doc_id] = {
                'name': document_name,
                'content': document_content,
                'tokens': len(document_content.split()),
                'added_time': time.time()
            }
            
            print(f"üìö [ULTRA] Document '{document_name}' stock√© avec ID: {doc_id}")
            
            # Ajouter au gestionnaire de contexte
            result = self.context_manager.add_document(document_content, document_name)
            
            # Mettre √† jour les statistiques
            self.context_stats['documents_processed'] += 1
            self.context_stats['total_tokens'] += len(document_content.split())
            
            if 'chunks_created' in result:
                self.context_stats['chunks_created'] += result['chunks_created']
            
            return {
                'success': True,
                'document_name': document_name,
                'chunk_ids': [doc_id],  # Ajout des chunk_ids pour l'interface
                'chunks_created': result.get('chunks_created', 1),
                'tokens_added': len(document_content.split()),
                'total_context_tokens': self.context_stats['total_tokens']
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': f'Erreur lors de l\'ajout du document: {str(e)}'
            }
    
    def add_file_to_context(self, file_path: str) -> Dict[str, Any]:
        """
        Ajoute un fichier au contexte en utilisant les processeurs avanc√©s
        
        Args:
            file_path: Chemin vers le fichier
            
        Returns:
            Statistiques d'ajout avec contenu analys√©
        """
        import os
        
        if not os.path.exists(file_path):
            return {'success': False, 'error': f'Fichier introuvable: {file_path}'}
        
        file_extension = os.path.splitext(file_path)[1].lower()
        file_name = os.path.basename(file_path)
        
        try:
            processed_content = ""
            analysis_info = ""
            
            # Traitement PDF avec processeur avanc√©
            if file_extension == '.pdf' and self.pdf_processor:
                print(f"üìÑ [ULTRA] Traitement PDF avanc√©: {file_name}")
                pdf_content = self.pdf_processor.read_pdf(file_path)
                if pdf_content.get('success'):
                    processed_content = pdf_content['content']
                    pages = pdf_content.get('pages', 'N/A')
                    # S'assurer que pages est une string
                    pages_str = str(pages) if pages is not None else 'N/A'
                    analysis_info = f"Pages: {pages_str}, Caract√®res: {len(processed_content)}"
                else:
                    return {'success': False, 'error': f'Erreur PDF: {pdf_content.get("error", "Inconnue")}'}
            
            # Traitement DOCX avec processeur avanc√©  
            elif file_extension in ['.docx', '.doc'] and self.docx_processor:
                print(f"üìù [ULTRA] Traitement DOCX avanc√©: {file_name}")
                docx_content = self.docx_processor.read_docx(file_path)
                if docx_content.get('success'):
                    processed_content = docx_content['content']
                    paragraphs = docx_content.get('paragraphs', 'N/A')
                    # S'assurer que paragraphs est une string
                    paragraphs_str = str(paragraphs) if paragraphs is not None else 'N/A'
                    analysis_info = f"Paragraphes: {paragraphs_str}, Caract√®res: {len(processed_content)}"
                else:
                    return {'success': False, 'error': f'Erreur DOCX: {docx_content.get("error", "Inconnue")}'}
            
            # Traitement code avec processeur avanc√©
            elif file_extension in ['.py', '.js', '.java', '.cpp', '.c', '.cs', '.php', '.rb', '.go', '.rs', '.ts', '.jsx', '.tsx'] and self.code_processor:
                print(f"üíª [ULTRA] Traitement code avanc√©: {file_name}")
                code_content = self.code_processor.read_code(file_path)
                if code_content.get('success'):
                    processed_content = code_content['content']
                    language = code_content.get('language', 'D√©tect√©')
                    lines = code_content.get('lines', 'N/A')
                    # S'assurer que language et lines sont des strings
                    language_str = str(language) if language is not None else 'D√©tect√©'
                    lines_str = str(lines) if lines is not None else 'N/A'
                    analysis_info = f"Langage: {language_str}, Lignes: {lines_str}"
                else:
                    return {'success': False, 'error': f'Erreur code: {code_content.get("error", "Inconnue")}'}
            
            # Traitement texte basique pour autres formats
            else:
                print(f"üìÑ [ULTRA] Traitement texte basique: {file_name}")
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    processed_content = f.read()
                analysis_info = f"Fichier texte, Caract√®res: {len(processed_content)}"
            
            # Ajouter le contenu analys√© au contexte
            # S'assurer que processed_content est une string
            if not isinstance(processed_content, str):
                processed_content = str(processed_content)
            
            # S'assurer que analysis_info est une string
            if not isinstance(analysis_info, str):
                analysis_info = str(analysis_info)
            
            enhanced_content = f"--- DOCUMENT: {file_name} ---\n"
            enhanced_content += f"Type: {file_extension}, {analysis_info}\n"
            enhanced_content += f"Chemin: {file_path}\n\n"
            enhanced_content += processed_content
            
            print(f"üîç [DEBUG] enhanced_content type: {type(enhanced_content)}, length: {len(enhanced_content)}")
            
            # Utiliser la m√©thode standard pour ajouter au contexte
            result = self.add_document_to_context(enhanced_content, file_name)
            
            if result['success']:
                result['analysis_info'] = analysis_info
                result['file_type'] = file_extension
                result['processor_used'] = 'advanced' if file_extension in ['.pdf', '.docx', '.doc'] or (file_extension in ['.py', '.js', '.java'] and self.code_processor) else 'basic'
                print(f"‚úÖ [ULTRA] Fichier '{file_name}' trait√© et ajout√© au contexte Ultra")
            
            return result
            
        except Exception as e:
            return {
                'success': False,
                'error': f'Erreur lors du traitement du fichier: {str(e)}'
            }
    
    def add_code_to_context(self, code_content: str, file_name: str = "") -> Dict[str, Any]:
        """
        Ajoute du code au contexte avec analyse syntaxique
        
        Args:
            code_content: Contenu du code
            file_name: Nom du fichier
            
        Returns:
            Statistiques d'ajout
        """
        if not self.context_manager:
            return {'success': False, 'error': 'Gestionnaire de contexte indisponible'}
        
        try:
            # Pr√©parer le contenu avec m√©tadonn√©es de code
            enriched_content = f"# FICHIER: {file_name}\n\n{code_content}"
            
            result = self.context_manager.add_document(enriched_content, f"CODE: {file_name}")
            
            # Mettre √† jour les statistiques
            self.context_stats['documents_processed'] += 1
            self.context_stats['total_tokens'] += len(code_content.split())
            
            return {
                'success': True,
                'file_name': file_name,
                'chunk_ids': [f"code_{hash(code_content) % 10000}"],  # Ajout des chunk_ids pour l'interface
                'chunks_created': result.get('chunks_created', 1),
                'tokens_added': len(code_content.split()),
                'total_context_tokens': self.context_stats['total_tokens']
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': f'Erreur lors de l\'ajout du code: {str(e)}'
            }
    
    def _build_extended_context(self, base_context: str = "") -> str:
        """
        Construit le contexte √©tendu en utilisant le gestionnaire 1M tokens
        """
        if not self.context_manager:
            return base_context
        
        try:
            # R√©cup√©rer le contexte du gestionnaire
            extended_context = self.context_manager.get_relevant_context(base_context)
            
            # Combiner avec le contexte de base
            if base_context:
                return f"{base_context}\n\n--- CONTEXTE √âTENDU ---\n{extended_context}"
            else:
                return extended_context
                
        except Exception as e:
            print(f"Erreur lors de la construction du contexte √©tendu: {e}")
            return base_context
    
    def _search_in_stored_context(self, query: str) -> str:
        """
        Recherche dans le contexte stock√© pour des informations pertinentes
        Consulte d'abord le syst√®me Ultra, puis la m√©moire classique en fallback
        """
        print(f"üîç [DEBUG] _search_in_stored_context appel√© avec query: '{query}'")
        
        # 1. Recherche dans le syst√®me Ultra
        if not hasattr(self, 'documents_storage'):
            self.documents_storage = {}
        
        print(f"üîç [DEBUG] Documents Ultra disponibles: {len(self.documents_storage)}")
        if self.documents_storage:
            for doc_id, doc_info in self.documents_storage.items():
                print(f"   - {doc_id}: {doc_info.get('name', 'Sans nom')}")
        
        relevant_content = ""
        query_lower = query.lower()
        
        # Mots-cl√©s pour r√©sum√©/document
        summary_keywords = ['r√©sume', 'r√©sumer', 'r√©sum√©', 'pdf', 'docx', 'document', 'fichier', 'contenu', 'ce', 'le', 'la', 'du', 'de la']
        
        # Recherche dans le syst√®me Ultra d'abord
        if self.documents_storage:
            print(f"üîç [ULTRA] Recherche dans syst√®me Ultra ({len(self.documents_storage)} docs)...")
            
            # Si c'est une demande de r√©sum√©, retourner TOUS les documents Ultra
            if any(keyword in query_lower for keyword in summary_keywords):
                print(f"üîç [ULTRA] Demande de r√©sum√© d√©tect√©e, chargement de tous les documents Ultra...")
                for doc_id, doc_info in self.documents_storage.items():
                    relevant_content += f"\n--- DOCUMENT: {doc_info.get('name', doc_id)} ---\n"
                    relevant_content += doc_info.get('content', '')
                    relevant_content += "\n"
                
                if relevant_content.strip():
                    print(f"‚úÖ [ULTRA] Retour de tous les docs Ultra: {len(relevant_content)} caract√®res")
                    return relevant_content
            
            # Recherche par mots-cl√©s dans le contenu Ultra
            keywords = [word for word in query_lower.split() if len(word) > 2]
            print(f"üîç [DEBUG] Mots-cl√©s recherche: {keywords}")
            
            for doc_id, doc_info in self.documents_storage.items():
                content = doc_info.get('content', '')
                # Recherche simple de mots-cl√©s
                if any(keyword in content.lower() for keyword in keywords):
                    print(f"‚úÖ [ULTRA] Correspondance trouv√©e dans {doc_info.get('name', doc_id)}")
                    relevant_content += f"\n--- DOCUMENT: {doc_info.get('name', doc_id)} ---\n"
                    relevant_content += content
                    relevant_content += "\n"
            
            if relevant_content.strip():
                print(f"‚úÖ [ULTRA] Trouv√© dans syst√®me Ultra: {len(relevant_content)} caract√®res")
                return relevant_content
        
        # 2. Fallback vers la m√©moire classique si rien trouv√© en Ultra
        print(f"üîç [FALLBACK] Recherche dans m√©moire classique...")
        print(f"üîç [DEBUG] base_ai disponible: {self.base_ai is not None}")
        
        if self.base_ai:
            print(f"üîç [DEBUG] base_ai type: {type(self.base_ai)}")
            print(f"üîç [DEBUG] hasattr local_ai: {hasattr(self.base_ai, 'local_ai')}")
            
            if hasattr(self.base_ai, 'local_ai'):
                print(f"üîç [DEBUG] local_ai type: {type(self.base_ai.local_ai)}")
                print(f"üîç [DEBUG] hasattr conversation_memory: {hasattr(self.base_ai.local_ai, 'conversation_memory')}")
                
                if hasattr(self.base_ai.local_ai, 'conversation_memory'):
                    memory = self.base_ai.local_ai.conversation_memory
                    print(f"üîç [DEBUG] conversation_memory type: {type(memory)}")
                    print(f"üîç [DEBUG] hasattr stored_documents: {hasattr(memory, 'stored_documents')}")
                    
                    if hasattr(memory, 'stored_documents'):
                        print(f"üîç [DEBUG] stored_documents: {memory.stored_documents}")
                        print(f"üîç [DEBUG] stored_documents type: {type(memory.stored_documents)}")
                        print(f"üîç [DEBUG] memory instance id: {id(memory)}")
                        print(f"üîç [DEBUG] self.base_ai.local_ai id: {id(self.base_ai.local_ai) if self.base_ai and hasattr(self.base_ai, 'local_ai') else 'N/A'}")
                        
                        if memory.stored_documents:
                            print(f"üìö [FALLBACK] Trouv√© {len(memory.stored_documents)} docs dans m√©moire classique")
                            for doc_name in memory.stored_documents.keys():
                                print(f"   - {doc_name}")
                        else:
                            print(f"üì≠ [FALLBACK] stored_documents vide ou None")
                    else:
                        print(f"‚ùå [FALLBACK] Pas d'attribut stored_documents")
                else:
                    print(f"‚ùå [FALLBACK] Pas d'attribut conversation_memory")
            else:
                print(f"‚ùå [FALLBACK] Pas d'attribut local_ai")
        
        if self.base_ai and hasattr(self.base_ai, 'local_ai') and hasattr(self.base_ai.local_ai, 'conversation_memory'):
            try:
                memory = self.base_ai.local_ai.conversation_memory
                
                # V√©rifier les documents stock√©s dans la m√©moire classique
                if hasattr(memory, 'stored_documents') and memory.stored_documents:
                    print(f"üìö [FALLBACK] Trouv√© {len(memory.stored_documents)} docs dans m√©moire classique")
                    
                    # Pour les demandes de r√©sum√©, retourner tous les documents
                    if any(keyword in query_lower for keyword in summary_keywords):
                        for doc_name, doc_content in memory.stored_documents.items():
                            relevant_content += f"\n--- DOCUMENT: {doc_name} ---\n"
                            relevant_content += doc_content
                            relevant_content += "\n"
                        
                        if relevant_content.strip():
                            print(f"‚úÖ [FALLBACK] R√©sum√© depuis m√©moire classique: {len(relevant_content)} caract√®res")
                            return relevant_content
                    
                    # Recherche par mots-cl√©s dans la m√©moire classique
                    keywords = [word for word in query_lower.split() if len(word) > 2]
                    for doc_name, doc_content in memory.stored_documents.items():
                        # Recherche plus permissive
                        if (any(keyword in doc_content.lower() for keyword in keywords) or 
                            any(keyword in doc_name.lower() for keyword in keywords)):
                            relevant_content += f"\n--- DOCUMENT: {doc_name} ---\n"
                            relevant_content += doc_content
                            relevant_content += "\n"
                    
                    # Si pas de correspondance par mots-cl√©s, retourner TOUT si c'est une demande g√©n√©rale
                    if not relevant_content.strip() and (
                        'document' in query_lower or 'fichier' in query_lower or 
                        'r√©sume' in query_lower or 'analyse' in query_lower
                    ):
                        print(f"üìÑ [FALLBACK] Aucune correspondance sp√©cifique, retour de tous les documents")
                        for doc_name, doc_content in memory.stored_documents.items():
                            relevant_content += f"\n--- DOCUMENT: {doc_name} ---\n"
                            relevant_content += doc_content
                            relevant_content += "\n"
                    
                    if relevant_content.strip():
                        print(f"‚úÖ [FALLBACK] Trouv√© dans m√©moire classique: {len(relevant_content)} caract√®res")
                        return relevant_content
                
            except Exception as e:
                print(f"‚ö†Ô∏è [FALLBACK] Erreur acc√®s m√©moire classique: {e}")
                import traceback
                traceback.print_exc()
        
        print(f"‚ùå Aucun document trouv√© dans aucun syst√®me")
        return ""
    
    def _generate_from_context(self, question: str, context: str) -> str:
        """
        G√©n√®re une r√©ponse intelligente √† partir du contexte stock√© avec analyse avanc√©e
        """
        question_lower = question.lower()
        context_lower = context.lower()
        
        # Demande de r√©sum√© d√©tect√©e
        if any(keyword in question_lower for keyword in ['r√©sume', 'r√©sumer', 'r√©sum√©', 'r√©sumer', 'synth√®se', 'synthese']):
            print(f"üìã [ULTRA] G√©n√©ration de r√©sum√© intelligent avec processeurs avanc√©s...")
            
            if context.strip():
                # Extraire les informations principales avec analyse de type
                lines = [line.strip() for line in context.split('\n') if line.strip()]
                
                # Identifier les sections de documents avec m√©tadonn√©es
                documents_found = []
                current_doc = None
                current_content = []
                current_metadata = {}
                
                for line in lines:
                    if line.startswith('--- DOCUMENT:'):
                        if current_doc and current_content:
                            documents_found.append({
                                'name': current_doc,
                                'content': '\n'.join(current_content),
                                'metadata': current_metadata.copy()
                            })
                        current_doc = line.replace('--- DOCUMENT:', '').strip()
                        current_content = []
                        current_metadata = {}
                    elif line.startswith('Type:'):
                        # Extraire les m√©tadonn√©es du processeur
                        current_metadata['type_info'] = line.replace('Type:', '').strip()
                    elif line.startswith('Chemin:'):
                        current_metadata['file_path'] = line.replace('Chemin:', '').strip()
                    elif current_doc:
                        current_content.append(line)
                
                # Ajouter le dernier document
                if current_doc and current_content:
                    documents_found.append({
                        'name': current_doc,
                        'content': '\n'.join(current_content),
                        'metadata': current_metadata.copy()
                    })
                
                # G√©n√©rer un r√©sum√© intelligent bas√© sur le type de document
                if documents_found:
                    summary = "üìÑ **Analyse intelligente des documents en m√©moire:**\n\n"
                    
                    for doc in documents_found:
                        summary += f"**üìã {doc['name']}**\n"
                        
                        # Analyser le type de document
                        type_info = doc['metadata'].get('type_info', '')
                        content = doc['content']
                        
                        # Analyse sp√©cialis√©e selon le type AVEC r√©sum√© avanc√©
                        if '.pdf' in type_info:
                            summary += self._analyze_pdf_content(content, type_info)
                            # Ajouter un r√©sum√© intelligent du contenu
                            intelligent_summary = self._create_intelligent_summary(content, doc['name'], 'PDF')
                            summary += f"‚Ä¢ **R√©sum√© intelligent**: {intelligent_summary}\n"
                        elif '.docx' in type_info or '.doc' in type_info:
                            summary += self._analyze_docx_content(content, type_info)
                            intelligent_summary = self._create_intelligent_summary(content, doc['name'], 'DOCX')
                            summary += f"‚Ä¢ **R√©sum√© intelligent**: {intelligent_summary}\n"
                        elif any(ext in type_info for ext in ['.py', '.js', '.java', '.cpp', '.c']):
                            summary += self._analyze_code_content(content, type_info)
                            intelligent_summary = self._create_intelligent_summary(content, doc['name'], 'Code')
                            summary += f"‚Ä¢ **Analyse du code**: {intelligent_summary}\n"
                        else:
                            summary += self._analyze_text_content(content, type_info)
                            intelligent_summary = self._create_intelligent_summary(content, doc['name'], 'Text')
                            summary += f"‚Ä¢ **R√©sum√© du contenu**: {intelligent_summary}\n"
                        
                        summary += "\n"
                    
                    return summary
        
        # Recherche sp√©cifique pour des codes/informations (existant)
        if "code secret" in question_lower:
            import re
            code_patterns = [
                r'code secret[^:]*:\s*([A-Z_0-9]+)',
                r'code[^:]*:\s*([A-Z_0-9]+)',
                r'ULTRA_TEST_[A-Z0-9_]+'
            ]
            
            for pattern in code_patterns:
                matches = re.findall(pattern, context, re.IGNORECASE)
                if matches:
                    return f"Le code secret de test est: {matches[0]}"
        
        # Recherche de num√©ros magiques (existant)
        if "num√©ro magique" in question_lower:
            import re
            magic_numbers = re.findall(r'num√©ro magique[^:]*:\s*(\d+)', context, re.IGNORECASE)
            if magic_numbers:
                return f"Le num√©ro magique est: {magic_numbers[0]}"
        
        # Recherche de phrases uniques
        if "phrase unique" in question_lower:
            import re
            unique_phrases = re.findall(r'phrase unique[^:]*:\s*"([^"]+)"', context, re.IGNORECASE)
            if unique_phrases:
                return f"La phrase unique est: \"{unique_phrases[0]}\""
        
        # Si aucune correspondance sp√©cifique, retourner un extrait pertinent
        if context.strip():
            lines = context.split('\n')
            relevant_lines = [line for line in lines if any(word in line.lower() for word in question_lower.split() if len(word) > 2)]
            if relevant_lines:
                return f"D'apr√®s les documents en m√©moire: {' '.join(relevant_lines[:3])}"
            
            # Si aucune ligne pertinente, retourner un r√©sum√© du contexte
            lines_clean = [line.strip() for line in lines if line.strip() and not line.startswith('---')]
            if lines_clean:
                # Prendre les premi√®res lignes significatives
                content_sample = ' '.join(lines_clean[:5])
                if len(content_sample) > 200:
                    content_sample = content_sample[:200] + "..."
                return f"üìö **Contenu disponible:** {content_sample}\n\nPouvez-vous pr√©ciser votre question ?"
        
        # Fallback final - ne jamais retourner None
        return "üìö J'ai du contenu en m√©moire mais je n'ai pas trouv√© d'information sp√©cifique pour r√©pondre √† votre question. Pouvez-vous reformuler ou √™tre plus pr√©cis ?"
    
    def _analyze_pdf_content(self, content: str, type_info: str) -> str:
        """Analyse intelligente du contenu PDF"""
        lines = content.split('\n')
        text_lines = [line.strip() for line in lines if line.strip() and not line.startswith('Type:') and not line.startswith('Chemin:')]
        
        analysis = f"üìÑ **Document PDF** - {type_info}\n"
        analysis += f"‚Ä¢ **Contenu**: {len(text_lines)} lignes de texte\n"
        
        # Identifier les sections importantes
        if text_lines:
            # Premier paragraphe comme r√©sum√©
            first_paragraph = ' '.join(text_lines[:3])
            if len(first_paragraph) > 100:
                first_paragraph = first_paragraph[:100] + "..."
            analysis += f"‚Ä¢ **D√©but**: {first_paragraph}\n"
            
            # Chercher des titres ou sections
            titles = [line for line in text_lines if len(line) < 50 and line.isupper()]
            if titles:
                analysis += f"‚Ä¢ **Sections d√©tect√©es**: {', '.join(titles[:3])}\n"
        
        return analysis
    
    def _analyze_docx_content(self, content: str, type_info: str) -> str:
        """Analyse intelligente du contenu DOCX"""
        lines = content.split('\n')
        text_lines = [line.strip() for line in lines if line.strip() and not line.startswith('Type:') and not line.startswith('Chemin:')]
        
        analysis = f"üìù **Document Word** - {type_info}\n"
        analysis += f"‚Ä¢ **Structure**: {len(text_lines)} paragraphes\n"
        
        if text_lines:
            # R√©sum√© du contenu
            all_text = ' '.join(text_lines)
            words = all_text.split()
            analysis += f"‚Ä¢ **Taille**: {len(words)} mots\n"
            
            # Premier paragraphe
            if len(words) > 20:
                summary = ' '.join(words[:20]) + "..."
                analysis += f"‚Ä¢ **R√©sum√©**: {summary}\n"
        
        return analysis
    
    def _analyze_code_content(self, content: str, type_info: str) -> str:
        """Analyse intelligente du code source"""
        lines = content.split('\n')
        code_lines = [line for line in lines if line.strip() and not line.startswith('Type:') and not line.startswith('Chemin:')]
        
        analysis = f"üíª **Code Source** - {type_info}\n"
        analysis += f"‚Ä¢ **Lignes**: {len(code_lines)} lignes de code\n"
        
        if code_lines:
            # D√©tecter le langage
            if '.py' in type_info:
                # Analyse Python
                functions = [line.strip() for line in code_lines if line.strip().startswith('def ')]
                classes = [line.strip() for line in code_lines if line.strip().startswith('class ')]
                imports = [line.strip() for line in code_lines if line.strip().startswith('import ') or line.strip().startswith('from ')]
                
                if functions:
                    analysis += f"‚Ä¢ **Fonctions**: {len(functions)} d√©tect√©es\n"
                if classes:
                    analysis += f"‚Ä¢ **Classes**: {len(classes)} d√©tect√©es\n"
                if imports:
                    analysis += f"‚Ä¢ **Imports**: {len(imports)} modules\n"
            
            elif '.js' in type_info or '.ts' in type_info:
                # Analyse JavaScript/TypeScript
                functions = [line.strip() for line in code_lines if 'function' in line or '=>' in line]
                analysis += f"‚Ä¢ **Fonctions**: ~{len(functions)} d√©tect√©es\n"
        
        return analysis
    
    def _analyze_text_content(self, content: str, type_info: str) -> str:
        """Analyse du contenu texte g√©n√©rique"""
        lines = content.split('\n')
        text_lines = [line.strip() for line in lines if line.strip() and not line.startswith('Type:') and not line.startswith('Chemin:')]
        
        analysis = f"üìÑ **Fichier Texte** - {type_info}\n"
        
        if text_lines:
            all_text = ' '.join(text_lines)
            words = all_text.split()
            analysis += f"‚Ä¢ **Contenu**: {len(text_lines)} lignes, {len(words)} mots\n"
            
            # √âchantillon du contenu
            if len(words) > 30:
                sample = ' '.join(words[:30]) + "..."
                analysis += f"‚Ä¢ **Aper√ßu**: {sample}\n"
            else:
                analysis += f"‚Ä¢ **Contenu complet**: {all_text}\n"
        
        return analysis
    
    def _create_intelligent_summary(self, content: str, doc_name: str, doc_type: str) -> str:
        """
        Cr√©e un r√©sum√© intelligent bas√© sur la logique avanc√©e du custom_ai_model
        """
        # Nettoyer le contenu des m√©tadonn√©es
        lines = content.split('\n')
        clean_lines = []
        for line in lines:
            if not line.startswith('Type:') and not line.startswith('Chemin:') and line.strip():
                clean_lines.append(line.strip())
        
        clean_content = '\n'.join(clean_lines)
        
        if len(clean_content) < 50:
            return "Contenu trop court pour analyse."
        
        # Extraire les th√®mes principaux
        words = clean_content.lower().split()
        
        # Mots-cl√©s fr√©quents pour identifier les th√®mes
        important_words = []
        word_count = {}
        
        for word in words:
            word_clean = re.sub(r'[^\w]', '', word)
            if len(word_clean) > 4 and word_clean not in ['cette', 'cette', 'dans', 'avec', 'pour', 'sont', 'mais', 'tout', 'nous', 'vous', 'leur', 'elle', 'ils', 'elles']:
                word_count[word_clean] = word_count.get(word_clean, 0) + 1
        
        # Prendre les mots les plus fr√©quents
        top_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)[:5]
        themes = [word for word, count in top_words if count > 1]
        
        # Extraire des phrases cl√©s
        sentences = [s.strip() for s in clean_content.split('.') if len(s.strip()) > 20]
        key_sentences = sentences[:2] if sentences else []
        
        # Cr√©er le r√©sum√© selon le type
        if doc_type == 'PDF':
            summary = f"Document PDF '{doc_name}' "
            if themes:
                summary += f"traitant de {', '.join(themes[:3])}. "
            if key_sentences:
                summary += f"Points cl√©s: {key_sentences[0][:100]}..."
            else:
                summary += f"Contenu de {len(words)} mots analys√©."
                
        elif doc_type == 'DOCX':
            summary = f"Document Word '{doc_name}' "
            if themes:
                summary += f"couvrant {', '.join(themes[:3])}. "
            if key_sentences:
                summary += f"R√©sum√©: {key_sentences[0][:100]}..."
            else:
                summary += f"Document structur√© de {len(words)} mots."
                
        elif doc_type == 'Code':
            summary = f"Code source '{doc_name}' "
            if themes:
                summary += f"avec √©l√©ments: {', '.join(themes[:3])}. "
            # D√©tecter les fonctions/classes
            functions = len([line for line in clean_lines if 'def ' in line or 'function ' in line])
            classes = len([line for line in clean_lines if 'class ' in line])
            if functions:
                summary += f"Contient {functions} fonction(s)"
            if classes:
                summary += f" et {classes} classe(s)."
            if not functions and not classes:
                summary += f"Script de {len(clean_lines)} lignes."
                
        else:
            summary = f"Fichier texte '{doc_name}' "
            if themes:
                summary += f"abordant {', '.join(themes[:3])}. "
            if key_sentences:
                summary += f"Contenu principal: {key_sentences[0][:100]}..."
            else:
                summary += f"Texte de {len(words)} mots."
        
        return summary


    def get_context_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques du contexte"""
        stats = self.context_stats.copy()
        
        if self.context_manager:
            # Ajouter les stats du gestionnaire de contexte
            manager_stats = self.context_manager.get_stats()
            stats.update(manager_stats)
        
        # Calculer le pourcentage d'utilisation
        current_tokens = stats.get('total_tokens', 0)
        max_tokens = stats.get('max_context_length', 1000000)
        if max_tokens > 0:
            stats['utilization_percent'] = (current_tokens / max_tokens) * 100
        else:
            stats['utilization_percent'] = 0.0
            
        # Ajouter current_tokens pour compatibilit√©
        stats['current_tokens'] = current_tokens
        
        return stats
    
    def clear_context(self):
        """Vide le contexte √©tendu"""
        if self.context_manager:
            self.context_manager.clear_context()
        
        # R√©initialiser les statistiques
        self.context_stats = {
            'total_tokens': 0,
            'documents_processed': 0,
            'chunks_created': 0,
            'max_context_length': 1000000
        }
    
    def is_available(self) -> bool:
        """V√©rifie si le mode Ultra est disponible"""
        return self.is_ultra_available and self.context_manager is not None

# Fonction de compatibilit√© pour l'import
def create_ultra_model(base_ai_engine=None) -> UltraCustomAIModel:
    """Cr√©e une instance du mod√®le Ultra"""
    return UltraCustomAIModel(base_ai_engine)

# Test de disponibilit√©
def is_ultra_system_available() -> bool:
    """V√©rifie si le syst√®me Ultra est disponible"""
    try:
        model = UltraCustomAIModel()
        return model.is_available()
    except Exception:
        return False

if __name__ == "__main__":
    # Test du syst√®me
    print("üöÄ Test du syst√®me UltraCustomAIModel")
    model = UltraCustomAIModel()
    print(f"Ultra disponible: {model.is_available()}")
    print(f"Stats: {model.get_context_stats()}")
