"""
Module de gestion des LLM locaux avec priorit√© √† Ollama.
Support de l'historique de conversation pour un contexte persistant.
"""

from typing import Dict, List

import requests


class LocalLLM:
    """
    Gestionnaire intelligent de LLM Local avec m√©moire de conversation.
    Tente d'utiliser Ollama en priorit√©, sinon g√®re le fallback.
    """

    def __init__(
        self,
        model="my_ai",
        ollama_url="http://localhost:11434/api/generate",
        timeout=600,
    ):
        # On essaie d'abord le mod√®le personnalis√© 'my_ai', sinon fallback sur 'llama3'
        self.model = model
        self.ollama_url = ollama_url
        self.chat_url = ollama_url.replace("/api/generate", "/api/chat")
        self.timeout = timeout  # Timeout configurable
        self.is_ollama_available = self._check_ollama_availability()

        # üß† Historique de conversation pour le contexte
        self.conversation_history: List[Dict[str, str]] = []
        self.max_history_length = 200  # Garder les 200 derniers √©changes

        if self.is_ollama_available:
            # V√©rifier si le mod√®le personnalis√© existe, sinon utiliser llama3
            if not self._check_model_exists(model):
                print(
                    f"‚ö†Ô∏è [LocalLLM] Mod√®le '{model}' non trouv√©. Fallback sur 'llama3'."
                )
                self.model = "llama3"

            print(
                f"‚úÖ [LocalLLM] Ollama d√©tect√© et actif sur {self.ollama_url} (Mod√®le: {self.model})"
            )
            print(
                f"   ‚ÑπÔ∏è  Timeout configur√©: {self.timeout}s (la premi√®re requ√™te peut √™tre lente)"
            )
            print(
                f"   üß† M√©moire de conversation activ√©e (max {self.max_history_length} √©changes)"
            )
        else:
            print(
                "‚ö†Ô∏è [LocalLLM] Ollama non d√©tect√©. Le mode g√©n√©ratif avanc√© sera d√©sactiv√©."
            )

    def _check_model_exists(self, model_name):
        """V√©rifie si le mod√®le existe dans Ollama"""
        try:
            response = requests.get(
                self.ollama_url.replace("/api/generate", "/api/tags"), timeout=2
            )
            if response.status_code == 200:
                models = [m["name"] for m in response.json().get("models", [])]
                # V√©rifie si le mod√®le est dans la liste (avec ou sans tag :latest)
                return any(model_name in m for m in models)
            return False
        except Exception:
            return False

    def _check_ollama_availability(self):
        """V√©rifie si le serveur Ollama r√©pond"""
        try:
            # On tente juste un ping rapide (GET sur la racine ou une API l√©g√®re)
            response = requests.get(
                self.ollama_url.replace("/api/generate", ""), timeout=2
            )
            return response.status_code == 200
        except Exception:
            return False

    def generate(self, prompt, system_prompt=None):
        """
        G√©n√®re une r√©ponse avec contexte de conversation.
        Utilise l'API /api/chat pour maintenir l'historique.
        Retourne None si Ollama n'est pas disponible (pour d√©clencher le fallback).
        """
        if not self.is_ollama_available:
            return None

        # Construire les messages avec historique
        messages = []

        # Ajouter le system prompt s'il existe
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})

        # Ajouter l'historique de conversation
        messages.extend(self.conversation_history)

        # Ajouter le message actuel de l'utilisateur
        messages.append({"role": "user", "content": prompt})

        data = {
            "model": self.model,
            "messages": messages,
            "stream": False,
            "options": {
                "temperature": 0.7,
                "num_ctx": 8192,
                "num_predict": 1024,
            },
        }

        try:
            print(
                f"‚è≥ [LocalLLM] G√©n√©ration avec contexte ({len(self.conversation_history)} messages pr√©c√©dents)..."
            )
            response = requests.post(self.chat_url, json=data, timeout=self.timeout)
            if response.status_code == 200:
                result = response.json()
                assistant_response = result.get("message", {}).get("content", "")

                if assistant_response:
                    # Sauvegarder dans l'historique
                    self.add_to_history("user", prompt)
                    self.add_to_history("assistant", assistant_response)
                    print("‚úÖ [LocalLLM] R√©ponse g√©n√©r√©e et ajout√©e √† l'historique")

                return assistant_response
            else:
                print(f"‚ö†Ô∏è [LocalLLM] Erreur API Ollama: {response.status_code}")
                return None
        except requests.exceptions.Timeout:
            print(
                f"‚ö†Ô∏è [LocalLLM] Timeout apr√®s {self.timeout}s - Le mod√®le est trop lent."
            )
            print(
                "   üí° Conseil: Essayez un mod√®le plus l√©ger ou augmentez le timeout."
            )
            return None
        except Exception as e:
            print(f"‚ö†Ô∏è [LocalLLM] Exception durant la g√©n√©ration: {e}")
            return None

    def add_to_history(self, role: str, content: str):
        """Ajoute un message √† l'historique de conversation"""
        self.conversation_history.append({"role": role, "content": content})

        # Limiter la taille de l'historique
        if len(self.conversation_history) > self.max_history_length * 2:
            # Garder les premiers messages (contexte initial) et les derniers
            self.conversation_history = self.conversation_history[
                -self.max_history_length * 2 :
            ]
            print(
                f"üîÑ [LocalLLM] Historique tronqu√© √† {len(self.conversation_history)} messages"
            )

    def generate_with_image(self, prompt, image_base64, system_prompt=None):
        """
        G√©n√®re une r√©ponse √† partir d'une image en utilisant un mod√®le vision.
        L'API Ollama supporte les images via le champ 'images' dans les messages.

        Args:
            prompt: Le message/question de l'utilisateur sur l'image
            image_base64: L'image encod√©e en base64
            system_prompt: Prompt syst√®me optionnel

        Returns:
            La r√©ponse du mod√®le ou None si erreur
        """
        if not self.is_ollama_available:
            return None

        # Utiliser un mod√®le vision
        vision_model = self._get_vision_model()
        if not vision_model:
            return None

        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})

        # Message utilisateur avec image
        messages.append({
            "role": "user",
            "content": prompt,
            "images": [image_base64]
        })

        data = {
            "model": vision_model,
            "messages": messages,
            "stream": False,
            "options": {
                "temperature": 0.7,
                "num_ctx": 8192,
                "num_predict": 1024,
            },
        }

        try:
            print(f"üñºÔ∏è [LocalLLM] G√©n√©ration avec image via mod√®le vision '{vision_model}'...")
            response = requests.post(self.chat_url, json=data, timeout=self.timeout)
            if response.status_code == 200:
                result = response.json()
                assistant_response = result.get("message", {}).get("content", "")
                if assistant_response:
                    self.add_to_history("user", f"[Image jointe] {prompt}")
                    self.add_to_history("assistant", assistant_response)
                    print("‚úÖ [LocalLLM] R√©ponse vision g√©n√©r√©e")
                return assistant_response
            else:
                print(f"‚ö†Ô∏è [LocalLLM] Erreur API vision: {response.status_code}")
                return None
        except requests.exceptions.Timeout:
            print(f"‚ö†Ô∏è [LocalLLM] Timeout vision apr√®s {self.timeout}s")
            return None
        except Exception as e:
            print(f"‚ö†Ô∏è [LocalLLM] Exception vision: {e}")
            return None

    def generate_stream_with_image(self, prompt, image_base64, system_prompt=None, on_token=None):
        """
        G√©n√®re une r√©ponse avec image en STREAMING.

        Args:
            prompt: Le message de l'utilisateur
            image_base64: L'image encod√©e en base64
            system_prompt: Prompt syst√®me optionnel
            on_token: Callback pour chaque token

        Returns:
            La r√©ponse compl√®te ou None
        """
        if not self.is_ollama_available:
            return None

        vision_model = self._get_vision_model()
        if not vision_model:
            return None

        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})

        messages.append({
            "role": "user",
            "content": prompt,
            "images": [image_base64]
        })

        data = {
            "model": vision_model,
            "messages": messages,
            "stream": True,
            "options": {
                "temperature": 0.7,
                "num_ctx": 8192,
                "num_predict": 1024,
            },
        }

        try:
            print(f"‚ö°üñºÔ∏è [LocalLLM] Streaming vision via '{vision_model}'...")
            full_response = ""

            with requests.post(
                self.chat_url, json=data, timeout=self.timeout, stream=True
            ) as response:
                if response.status_code != 200:
                    print(f"‚ö†Ô∏è [LocalLLM] Erreur API vision: {response.status_code}")
                    return None

                for line in response.iter_lines():
                    if line:
                        try:
                            chunk = line.decode("utf-8")
                            json_chunk = __import__("json").loads(chunk)
                            token = json_chunk.get("message", {}).get("content", "")

                            if token:
                                full_response += token
                                if on_token:
                                    should_continue = on_token(token)
                                    if should_continue is False:
                                        print("üõë [LocalLLM] Vision interrompue")
                                        break

                            if json_chunk.get("done", False):
                                break
                        except Exception as e:
                            print(f"‚ö†Ô∏è [LocalLLM] Erreur parsing vision chunk: {e}")
                            continue

            if full_response:
                self.add_to_history("user", f"[Image jointe] {prompt}")
                self.add_to_history("assistant", full_response)
                print("‚úÖ [LocalLLM] R√©ponse vision streaming compl√®te")

            return full_response

        except requests.exceptions.Timeout:
            print(f"‚ö†Ô∏è [LocalLLM] Timeout vision streaming apr√®s {self.timeout}s")
            return None
        except Exception as e:
            print(f"‚ö†Ô∏è [LocalLLM] Exception vision streaming: {e}")
            return None

    def _get_vision_model(self):
        """D√©tecte et retourne un mod√®le vision disponible dans Ollama"""
        vision_models = ["llava", "llava:13b", "llava:7b", "llama3.2-vision", "bakllava", "moondream"]

        try:
            response = requests.get(
                self.ollama_url.replace("/api/generate", "/api/tags"), timeout=5
            )
            if response.status_code == 200:
                models = response.json().get("models", [])
                available_names = [m.get("name", "").split(":", maxsplit=1)[0] for m in models]

                for vm in vision_models:
                    base_name = vm.split(":", maxsplit=1)[0]
                    if base_name in available_names:
                        # Retourner le nom exact du mod√®le trouv√©
                        for m in models:
                            if m.get("name", "").startswith(base_name):
                                print(f"üñºÔ∏è [LocalLLM] Mod√®le vision trouv√©: {m['name']}")
                                return m["name"]

                print("‚ö†Ô∏è [LocalLLM] Aucun mod√®le vision trouv√©.")
                print("   üí° Installez-en un avec: ollama pull llava")
                print(f"   üìã Mod√®les support√©s: {', '.join(vision_models)}")
                return None
        except Exception as e:
            print(f"‚ö†Ô∏è [LocalLLM] Erreur d√©tection mod√®le vision: {e}")
            return None

    def clear_history(self):
        """Efface l'historique de conversation"""
        self.conversation_history.clear()
        print("üóëÔ∏è [LocalLLM] Historique de conversation effac√©")

    def get_last_user_message(self) -> str:
        """R√©cup√®re le dernier message de l'utilisateur pour le contexte"""
        for msg in reversed(self.conversation_history):
            if msg["role"] == "user":
                return msg["content"]
        return ""

    def get_conversation_context(self, n_messages: int = 5) -> str:
        """R√©cup√®re les n derniers √©changes sous forme de texte pour le contexte"""
        recent = (
            self.conversation_history[-n_messages * 2 :]
            if self.conversation_history
            else []
        )
        context_parts = []
        for msg in recent:
            role = "Utilisateur" if msg["role"] == "user" else "Assistant"
            context_parts.append(f"{role}: {msg['content']}")
        return "\n".join(context_parts)

    def generate_stream(self, prompt, system_prompt=None, on_token=None):
        """
        G√©n√®re une r√©ponse en STREAMING pour une latence minimale.
        Chaque token est envoy√© via le callback on_token(token_text) d√®s qu'il est re√ßu.

        Args:
            prompt: Le message de l'utilisateur
            system_prompt: Prompt syst√®me optionnel
            on_token: Callback appel√© pour chaque token re√ßu (signature: on_token(str) -> bool)
                     Retourne False pour interrompre la g√©n√©ration

        Returns:
            La r√©ponse compl√®te une fois termin√©e, ou None si erreur
        """
        if not self.is_ollama_available:
            return None

        # Construire les messages avec historique
        messages = []

        # Ajouter le system prompt s'il existe
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})

        # Ajouter l'historique de conversation
        messages.extend(self.conversation_history)

        # Ajouter le message actuel de l'utilisateur
        messages.append({"role": "user", "content": prompt})

        data = {
            "model": self.model,
            "messages": messages,
            "stream": True,
            "options": {
                "temperature": 0.7,
                "num_ctx": 8192,
                "num_predict": 1024,
            },
        }

        try:
            print(
                f"‚ö° [LocalLLM] G√©n√©ration STREAMING ({len(self.conversation_history)} messages contexte)..."
            )

            full_response = ""

            # Requ√™te en streaming
            with requests.post(
                self.chat_url, json=data, timeout=self.timeout, stream=True
            ) as response:
                if response.status_code != 200:
                    print(f"‚ö†Ô∏è [LocalLLM] Erreur API Ollama: {response.status_code}")
                    return None

                # Lire les chunks JSON ligne par ligne
                for line in response.iter_lines():
                    if line:
                        try:
                            chunk = line.decode("utf-8")
                            json_chunk = __import__("json").loads(chunk)

                            # Extraire le contenu du token
                            token = json_chunk.get("message", {}).get("content", "")

                            if token:
                                full_response += token

                                # Appeler le callback si fourni
                                if on_token:
                                    should_continue = on_token(token)
                                    if should_continue is False:
                                        print(
                                            "üõë [LocalLLM] G√©n√©ration interrompue par callback"
                                        )
                                        break

                            # V√©rifier si c'est le dernier message
                            if json_chunk.get("done", False):
                                break

                        except Exception as e:
                            print(f"‚ö†Ô∏è [LocalLLM] Erreur parsing chunk: {e}")
                            continue

            if full_response:
                # Sauvegarder dans l'historique
                self.add_to_history("user", prompt)
                self.add_to_history("assistant", full_response)
                print(
                    "‚úÖ [LocalLLM] R√©ponse streaming compl√®te et ajout√©e √† l'historique"
                )

            return full_response

        except requests.exceptions.Timeout:
            print(f"‚ö†Ô∏è [LocalLLM] Timeout apr√®s {self.timeout}s")
            return None
        except Exception as e:
            print(f"‚ö†Ô∏è [LocalLLM] Exception streaming: {e}")
            return None
