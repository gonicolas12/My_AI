"""
Module de gestion des LLM locaux avec priorit√© √† Ollama.
"""
import requests

class LocalLLM:
    """
    Gestionnaire intelligent de LLM Local.
    Tente d'utiliser Ollama en priorit√©, sinon g√®re le fallback.
    """
    def __init__(self, model="my_ai", ollama_url="http://localhost:11434/api/generate", timeout=180):
        # On essaie d'abord le mod√®le personnalis√© 'my_ai', sinon fallback sur 'llama3'
        self.model = model
        self.ollama_url = ollama_url
        self.timeout = timeout  # Timeout configurable (180s par d√©faut)
        self.is_ollama_available = self._check_ollama_availability()

        if self.is_ollama_available:
            # V√©rifier si le mod√®le personnalis√© existe, sinon utiliser llama3
            if not self._check_model_exists(model):
                print(f"‚ö†Ô∏è [LocalLLM] Mod√®le '{model}' non trouv√©. Fallback sur 'llama3'.")
                self.model = "llama3"

            print(f"‚úÖ [LocalLLM] Ollama d√©tect√© et actif sur {self.ollama_url} (Mod√®le: {self.model})")
            print(f"   ‚ÑπÔ∏è  Timeout configur√©: {self.timeout}s (la premi√®re requ√™te peut √™tre lente)")
        else:
            print("‚ö†Ô∏è [LocalLLM] Ollama non d√©tect√©. Le mode g√©n√©ratif avanc√© sera d√©sactiv√©.")

    def _check_model_exists(self, model_name):
        """V√©rifie si le mod√®le existe dans Ollama"""
        try:
            response = requests.get(self.ollama_url.replace("/api/generate", "/api/tags"), timeout=2)
            if response.status_code == 200:
                models = [m['name'] for m in response.json().get('models', [])]
                # V√©rifie si le mod√®le est dans la liste (avec ou sans tag :latest)
                return any(model_name in m for m in models)
            return False
        except Exception:
            return False

    def _check_ollama_availability(self):
        """V√©rifie si le serveur Ollama r√©pond"""
        try:
            # On tente juste un ping rapide (GET sur la racine ou une API l√©g√®re)
            response = requests.get(self.ollama_url.replace("/api/generate", ""), timeout=2)
            return response.status_code == 200
        except Exception:
            return False

    def generate(self, prompt, system_prompt=None):
        """
        G√©n√®re une r√©ponse.
        Retourne None si Ollama n'est pas disponible (pour d√©clencher le fallback).
        """
        if not self.is_ollama_available:
            return None

        full_prompt = prompt
        if system_prompt:
            full_prompt = f"System: {system_prompt}\nUser: {prompt}"

        data = {
            "model": self.model,
            "prompt": full_prompt,
            "stream": False,
            "options": {
                "temperature": 0.7,
                "num_ctx": 4096,  # Contexte √©quilibr√© performance/vitesse
                "num_predict": 1024  # R√©ponses plus compl√®tes
            }
        }

        try:
            print(f"‚è≥ [LocalLLM] G√©n√©ration en cours (timeout: {self.timeout}s)...")
            response = requests.post(self.ollama_url, json=data, timeout=self.timeout)
            if response.status_code == 200:
                return response.json().get('response', '')
            else:
                print(f"‚ö†Ô∏è [LocalLLM] Erreur API Ollama: {response.status_code}")
                return None
        except requests.exceptions.Timeout:
            print(f"‚ö†Ô∏è [LocalLLM] Timeout apr√®s {self.timeout}s - Le mod√®le est trop lent.")
            print("   üí° Conseil: Essayez un mod√®le plus l√©ger (llama3.2) ou augmentez le timeout.")
            return None
        except Exception as e:
            print(f"‚ö†Ô∏è [LocalLLM] Exception durant la g√©n√©ration: {e}")
            return None
