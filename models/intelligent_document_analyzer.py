"""
üß† Analyseur de Documents Intelligent - Sans LLM externe
Architecture modulaire pour comprendre, analyser et r√©pondre sur n'importe quel document

Ce module impl√©mente une vraie intelligence documentaire bas√©e sur:
- Extraction d'entit√©s et de relations
- Graphe de connaissances dynamique
- Analyse syntaxique et s√©mantique
- G√©n√©ration de r√©ponses naturelles
"""

import re
import hashlib
from collections import defaultdict, Counter
from typing import Any, Dict, List, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum


class EntityType(Enum):
    """Types d'entit√©s reconnaissables"""

    PERSON = "person"
    ORGANIZATION = "organization"
    LOCATION = "location"
    DATE = "date"
    TIME = "time"
    NUMBER = "number"
    VERSION = "version"
    TECHNOLOGY = "technology"
    CONCEPT = "concept"
    ACTION = "action"
    PROPERTY = "property"
    VALUE = "value"
    CODE = "code"
    FILE = "file"
    URL = "url"


class RelationType(Enum):
    """Types de relations entre entit√©s"""

    IS_A = "is_a"
    HAS = "has"
    BELONGS_TO = "belongs_to"
    CREATED_BY = "created_by"
    LOCATED_IN = "located_in"
    OCCURRED_AT = "occurred_at"
    VALUE_OF = "value_of"
    USES = "uses"
    PRODUCES = "produces"
    REQUIRES = "requires"
    EQUALS = "equals"
    GREATER_THAN = "greater_than"
    LESS_THAN = "less_than"


@dataclass
class Entity:
    """Repr√©sente une entit√© extraite d'un document"""

    text: str
    entity_type: EntityType
    start_pos: int
    end_pos: int
    confidence: float
    context: str = ""
    normalized_form: str = ""
    metadata: Dict[str, Any] = field(default_factory=dict)

    def __hash__(self):
        return hash((self.text.lower(), self.entity_type))

    def __eq__(self, other):
        if isinstance(other, Entity):
            return (
                self.text.lower() == other.text.lower()
                and self.entity_type == other.entity_type
            )
        return False


@dataclass
class Relation:
    """Repr√©sente une relation entre deux entit√©s"""

    source: Entity
    target: Entity
    relation_type: RelationType
    confidence: float
    context: str = ""
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class Fact:
    """Repr√©sente un fait extrait (triplet sujet-pr√©dicat-objet)"""

    subject: str
    predicate: str
    object: str
    confidence: float
    source_text: str
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class DocumentSection:
    """Repr√©sente une section de document analys√©e"""

    title: str
    content: str
    level: int  # Niveau hi√©rarchique (1=titre principal, 2=sous-titre, etc.)
    entities: List[Entity] = field(default_factory=list)
    facts: List[Fact] = field(default_factory=list)
    keywords: List[str] = field(default_factory=list)
    summary: str = ""
    parent: Optional["DocumentSection"] = None
    children: List["DocumentSection"] = field(default_factory=list)


class IntelligentDocumentAnalyzer:
    """
    üß† Analyseur de documents intelligent sans LLM externe

    Capacit√©s:
    - Extraction d'entit√©s nomm√©es (NER)
    - Construction de graphe de connaissances
    - Analyse syntaxique et s√©mantique
    - R√©ponses en langage naturel
    """

    def __init__(self):
        # Graphe de connaissances: stocke les entit√©s et relations
        self.knowledge_graph: Dict[str, Dict] = {}
        self.entities: List[Entity] = []
        self.relations: List[Relation] = []
        self.facts: List[Fact] = []
        self.sections: List[DocumentSection] = []

        # Index invers√© pour recherche rapide
        self.entity_index: Dict[str, List[Entity]] = defaultdict(list)
        self.fact_index: Dict[str, List[Fact]] = defaultdict(list)
        self.keyword_index: Dict[str, List[Tuple[str, float]]] = defaultdict(list)

        # Patterns pour extraction d'entit√©s
        self._init_extraction_patterns()

        # Vocabulaire s√©mantique
        self._init_semantic_vocabulary()

        # Statistiques du document
        self.document_stats = {
            "total_words": 0,
            "total_sentences": 0,
            "total_sections": 0,
            "entity_counts": Counter(),
            "keyword_frequency": Counter(),
        }

        print("üß† Analyseur de documents intelligent initialis√©")

    def _init_extraction_patterns(self):
        """Initialise les patterns regex pour extraction d'entit√©s"""

        # Patterns pour diff√©rents types d'entit√©s
        self.patterns = {
            EntityType.VERSION: [
                r"(?:version|v|ver|V)\s*[:\s]*(\d+\.\d+(?:\.\d+)?(?:-\w+)?)",
                r'"version"\s*:\s*"(\d+\.\d+(?:\.\d+)?)"',
                r"'version'\s*:\s*'(\d+\.\d+(?:\.\d+)?)'",
                r"(\d+\.\d+\.\d+)",
            ],
            EntityType.NUMBER: [
                r"(\d{1,3}(?:[,\s]\d{3})*(?:\.\d+)?)\s*(?:tokens?|octets?|bytes?|MB|GB|KB|ms|secondes?|minutes?|heures?|%)",
                r"[<>‚â§‚â•]\s*(\d+(?:\.\d+)?)\s*(?:secondes?|ms|s\b)",
                r"(\d+(?:\.\d+)?)\s*(?:secondes?|ms|s\b)",
            ],
            EntityType.DATE: [
                r"(\d{1,2}[/\-\.]\d{1,2}[/\-\.]\d{2,4})",
                r"(\d{4}[/\-\.]\d{1,2}[/\-\.]\d{1,2})",
                r"(?:en\s+)?(\d{4})\b(?!\.\d)",  # Ann√©es seules
                r"(\d{1,2}\s+(?:janvier|f√©vrier|mars|avril|mai|juin|juillet|ao√ªt|septembre|octobre|novembre|d√©cembre)\s+\d{4})",
            ],
            EntityType.PERSON: [
                r"\b([A-Z][a-z√©√®√™√´√†√¢√§√π√ª√º√¥√∂√Æ√Ø√ß]+(?:\s+[A-Z][a-z√©√®√™√´√†√¢√§√π√ª√º√¥√∂√Æ√Ø√ß]+)+)\b",
                r"(?:M\.|Mme|Dr|Pr|Prof)\s+([A-Z][a-z√©√®√™√´√†√¢√§√π√ª√º√¥√∂√Æ√Ø√ß]+(?:\s+[A-Z][a-z√©√®√™√´√†√¢√§√π√ª√º√¥√∂√Æ√Ø√ß]+)?)",
            ],
            EntityType.TECHNOLOGY: [
                r"\b(Python|JavaScript|Java|C\+\+|C#|Ruby|Go|Rust|TypeScript|PHP|Swift|Kotlin)\b",
                r"\b(React|Vue|Angular|Django|Flask|FastAPI|Node\.js|Express|Spring|Laravel)\b",
                r"\b(TensorFlow|PyTorch|scikit-learn|pandas|numpy|Keras|OpenCV)\b",
                r"\b(Docker|Kubernetes|AWS|Azure|GCP|Linux|Windows|MacOS)\b",
                r"\b(SQL|NoSQL|MongoDB|PostgreSQL|MySQL|Redis|Elasticsearch)\b",
                r"\b(Git|GitHub|GitLab|CI/CD|DevOps|Agile|Scrum)\b",
                r"\b(API|REST|GraphQL|WebSocket|HTTP|HTTPS|JSON|XML|YAML)\b",
                r"\b(Machine Learning|Deep Learning|NLP|Computer Vision|IA|AI)\b",
            ],
            EntityType.CODE: [
                r"```[\w]*\n([\s\S]*?)```",
                r"`([^`]+)`",
                r"\b(def\s+\w+|class\s+\w+|function\s+\w+|const\s+\w+|let\s+\w+|var\s+\w+)\b",
            ],
            EntityType.URL: [
                r'(https?://[^\s<>"\']+)',
                r'(www\.[^\s<>"\']+)',
            ],
            EntityType.FILE: [
                r"([a-zA-Z0-9_\-]+\.(?:py|js|ts|html|css|json|yaml|yml|xml|txt|md|pdf|docx|csv))",
            ],
        }

        # Patterns pour extraction de relations/faits
        self.relation_patterns = [
            # "X est Y"
            (r"(.+?)\s+(?:est|sont|√©tait|√©taient)\s+(.+)", RelationType.IS_A),
            # "X a Y"
            (r"(.+?)\s+(?:a|ont|poss√®de|contient)\s+(.+)", RelationType.HAS),
            # "X utilise Y"
            (r"(.+?)\s+(?:utilise|emploie|exploite)\s+(.+)", RelationType.USES),
            # "X produit Y"
            (r"(.+?)\s+(?:produit|g√©n√®re|cr√©e|fournit)\s+(.+)", RelationType.PRODUCES),
            # "X n√©cessite Y"
            (
                r"(.+?)\s+(?:n√©cessite|requiert|demande|exige)\s+(.+)",
                RelationType.REQUIRES,
            ),
            # "X = Y" ou "X : Y"
            (r'"?(\w+)"?\s*[:=]\s*"?([^"]+)"?', RelationType.EQUALS),
            # Comparaisons
            (r"(.+?)\s*[<]\s*(.+)", RelationType.LESS_THAN),
            (r"(.+?)\s*[>]\s*(.+)", RelationType.GREATER_THAN),
        ]

    def _init_semantic_vocabulary(self):
        """Initialise le vocabulaire s√©mantique pour comprendre les questions"""

        # Mots-cl√©s pour identifier le type de question
        self.question_types = {
            "what": ["quel", "quelle", "quels", "quelles", "qu'est-ce", "que", "quoi"],
            "who": ["qui", "par qui", "de qui"],
            "when": ["quand", "√† quelle date", "en quelle ann√©e", "depuis quand"],
            "where": ["o√π", "√† quel endroit", "dans quel"],
            "how": ["comment", "de quelle mani√®re", "par quel moyen"],
            "how_much": ["combien", "quel nombre", "quelle quantit√©", "quel montant"],
            "why": ["pourquoi", "pour quelle raison", "√† cause de quoi"],
            "which": ["lequel", "laquelle", "lesquels", "lesquelles"],
        }

        # Synonymes et √©quivalences s√©mantiques
        self.semantic_equivalences = {
            "version": ["version", "v", "ver", "num√©ro de version", "release"],
            "performance": [
                "performance",
                "vitesse",
                "rapidit√©",
                "temps de r√©ponse",
                "latence",
            ],
            "temps": [
                "temps",
                "dur√©e",
                "secondes",
                "minutes",
                "heures",
                "ms",
                "millisecondes",
            ],
            "capacit√©": ["capacit√©", "taille", "volume", "quantit√©", "nombre"],
            "algorithme": ["algorithme", "algo", "m√©thode", "proc√©dure", "fonction"],
            "langage": ["langage", "language", "langue", "programmation"],
            "cr√©ateur": ["cr√©ateur", "auteur", "inventeur", "fondateur", "d√©veloppeur"],
            "date": ["date", "ann√©e", "jour", "mois", "p√©riode", "moment"],
        }

        # Mots de liaison et structures
        self.connectors = {
            "cause": ["car", "parce que", "puisque", "√©tant donn√© que", "du fait que"],
            "consequence": [
                "donc",
                "ainsi",
                "par cons√©quent",
                "c'est pourquoi",
                "de ce fait",
            ],
            "condition": ["si", "√† condition que", "pourvu que", "dans le cas o√π"],
            "opposition": ["mais", "cependant", "toutefois", "n√©anmoins", "pourtant"],
            "addition": ["et", "de plus", "en outre", "√©galement", "aussi"],
            "exemple": ["par exemple", "notamment", "comme", "tel que", "c'est-√†-dire"],
        }

    def analyze_document(self, content: str, document_name: str = "") -> Dict[str, Any]:
        """
        üîç Analyse compl√®te d'un document

        1. Segmentation en sections
        2. Extraction d'entit√©s
        3. Extraction de faits/relations
        4. Construction du graphe de connaissances
        5. Indexation pour recherche
        """
        print(f"üß† [ANALYZE] D√©but analyse de '{document_name}'...")

        # Reset pour nouveau document (ou fusionner si multi-documents)
        self._prepare_for_analysis()

        # √âtape 1: Segmentation
        sections = self._segment_document(content)
        print(f"üìÑ [ANALYZE] {len(sections)} sections identifi√©es")

        # √âtape 2: Extraction d'entit√©s par section
        all_entities = []
        for section in sections:
            entities = self._extract_entities(section.content)
            section.entities = entities
            all_entities.extend(entities)

            # Extraction de mots-cl√©s
            section.keywords = self._extract_keywords(section.content)

        self.entities = all_entities
        self.sections = sections
        print(f"üè∑Ô∏è [ANALYZE] {len(all_entities)} entit√©s extraites")

        # √âtape 3: Extraction de faits
        facts = self._extract_facts(content)
        self.facts = facts
        print(f"üìä [ANALYZE] {len(facts)} faits extraits")

        # √âtape 4: Construction du graphe
        self._build_knowledge_graph()

        # √âtape 5: Indexation
        self._build_indexes()

        # Statistiques
        self._compute_statistics(content)

        print(
            f"‚úÖ [ANALYZE] Analyse termin√©e - Graphe: {len(self.knowledge_graph)} n≈ìuds"
        )

        return {
            "success": True,
            "sections": len(sections),
            "entities": len(all_entities),
            "facts": len(facts),
            "graph_nodes": len(self.knowledge_graph),
            "stats": self.document_stats,
        }

    def _prepare_for_analysis(self):
        """Pr√©pare les structures pour une nouvelle analyse (r√©initialise l'√©tat)."""
        # R√©initialiser le graphe et les collections d'analyses
        self.knowledge_graph = {}
        self.entities = []
        self.relations = []
        self.facts = []
        self.sections = []

        # R√©initialiser les index invers√©s
        self.entity_index = defaultdict(list)
        self.fact_index = defaultdict(list)
        self.keyword_index = defaultdict(list)

        # R√©initialiser les statistiques documentaires
        self.document_stats = {
            "total_words": 0,
            "total_sentences": 0,
            "total_sections": 0,
            "entity_counts": Counter(),
            "keyword_frequency": Counter(),
        }

    def _segment_document(self, content: str) -> List[DocumentSection]:
        """Segmente le document en sections hi√©rarchiques"""
        sections = []

        # Patterns pour d√©tecter les titres de sections
        title_patterns = [
            (r"^#{1,6}\s+(.+)$", lambda m: len(m.group(0).split()[0])),  # Markdown
            (r"^([A-Z][^.!?]*?)(?:\n|$)", lambda m: 1),  # Titre en majuscules
            (
                r"^(\d+\.(?:\d+\.)*)\s*(.+)$",
                lambda m: m.group(1).count(".") + 1,
            ),  # Num√©rotation
            (
                r"^(?:Chapitre|Section|Partie)\s+(\d+|[IVX]+)[:\s]*(.+)?$",
                lambda m: 1,
            ),  # Titres explicites
        ]

        lines = content.split("\n")
        current_section = None
        current_content = []

        for line in lines:
            is_title = False
            title_text = ""
            level = 1

            # V√©rifier si c'est un titre
            for pattern, level_func in title_patterns:
                match = re.match(pattern, line.strip(), re.MULTILINE)
                if match:
                    # Sauvegarder la section pr√©c√©dente
                    if current_section is not None:
                        current_section.content = "\n".join(current_content).strip()
                        sections.append(current_section)

                    # Nouvelle section
                    title_text = (
                        match.group(1) if match.lastindex >= 1 else line.strip()
                    )
                    level = level_func(match) if callable(level_func) else 1

                    current_section = DocumentSection(
                        title=title_text.strip(), content="", level=level
                    )
                    current_content = []
                    is_title = True
                    break

            if not is_title:
                current_content.append(line)

        # Derni√®re section
        if current_section is not None:
            current_section.content = "\n".join(current_content).strip()
            sections.append(current_section)
        elif current_content:
            # Pas de titre trouv√©, tout est une seule section
            sections.append(
                DocumentSection(
                    title="Contenu principal",
                    content="\n".join(current_content).strip(),
                    level=1,
                )
            )

        return sections

    def _extract_entities(self, text: str) -> List[Entity]:
        """Extrait les entit√©s nomm√©es d'un texte"""
        entities = []

        for entity_type, patterns in self.patterns.items():
            for pattern in patterns:
                try:
                    for match in re.finditer(
                        pattern, text, re.IGNORECASE | re.MULTILINE
                    ):
                        entity_text = (
                            match.group(1) if match.lastindex >= 1 else match.group(0)
                        )

                        # Calculer le contexte (texte autour)
                        start = max(0, match.start() - 50)
                        end = min(len(text), match.end() + 50)
                        context = text[start:end]

                        entity = Entity(
                            text=entity_text.strip(),
                            entity_type=entity_type,
                            start_pos=match.start(),
                            end_pos=match.end(),
                            confidence=0.8,  # Confidence par d√©faut
                            context=context,
                            normalized_form=self._normalize_entity(
                                entity_text, entity_type
                            ),
                        )

                        # √âviter les doublons
                        if entity not in entities:
                            entities.append(entity)
                except Exception:
                    continue

        return entities

    def _normalize_entity(self, text: str, entity_type: EntityType) -> str:
        """Normalise une entit√© pour faciliter la comparaison"""
        text = text.strip().lower()

        if entity_type == EntityType.VERSION:
            # Garder uniquement les chiffres et points
            return re.sub(r"[^0-9.]", "", text)
        elif entity_type == EntityType.NUMBER:
            # Normaliser les nombres
            text = text.replace(",", "").replace(" ", "")
            try:
                return str(float(text))
            except Exception:
                return text
        elif entity_type == EntityType.DATE:
            # Normaliser les dates (format ISO)
            # Simplification: garder tel quel pour l'instant
            return text
        else:
            return text

    def _extract_keywords(self, text: str, top_n: int = 10) -> List[str]:
        """Extrait les mots-cl√©s importants d'un texte (TF-IDF simplifi√©)"""
        # Nettoyage et tokenization
        words = re.findall(r"\b[a-zA-Z√Ä-√ø]{3,}\b", text.lower())

        # Stopwords fran√ßais
        stopwords = {
            "les",
            "des",
            "une",
            "pour",
            "avec",
            "dans",
            "sur",
            "par",
            "est",
            "sont",
            "qui",
            "que",
            "quoi",
            "mais",
            "donc",
            "car",
            "cette",
            "ces",
            "aux",
            "pas",
            "plus",
            "peut",
            "√™tre",
            "fait",
            "faire",
            "ont",
            "√©t√©",
            "comme",
            "tout",
            "tous",
            "aussi",
            "leur",
            "leurs",
            "nous",
            "vous",
            "ils",
            "elle",
            "elles",
            "son",
            "ses",
            "notre",
            "votre",
            "lui",
            "tr√®s",
            "bien",
            "encore",
            "m√™me",
            "sans",
            "entre",
            "apr√®s",
            "avant",
            "sous",
            "chez",
            "peu",
            "trop",
            "autre",
            "autres",
            "celui",
            "celle",
            "ceux",
            "celles",
            "dont",
            "alors",
            "ainsi",
        }

        # Filtrer et compter
        word_freq = Counter(w for w in words if w not in stopwords)

        # Top N mots-cl√©s
        return [word for word, _ in word_freq.most_common(top_n)]

    def _extract_facts(self, text: str) -> List[Fact]:
        """Extrait les faits (triplets sujet-pr√©dicat-objet) du texte"""
        facts = []

        # Diviser en phrases
        sentences = re.split(r"[.!?]+", text)

        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) < 10:
                continue

            # Appliquer les patterns de relation
            for pattern, rel_type in self.relation_patterns:
                try:
                    match = re.search(pattern, sentence, re.IGNORECASE)
                    if match and match.lastindex >= 2:
                        subject = match.group(1).strip()
                        obj = match.group(2).strip()

                        # Nettoyer
                        subject = re.sub(
                            r"^(?:le|la|les|un|une|des)\s+",
                            "",
                            subject,
                            flags=re.IGNORECASE,
                        )
                        obj = re.sub(
                            r"^(?:le|la|les|un|une|des)\s+",
                            "",
                            obj,
                            flags=re.IGNORECASE,
                        )

                        if len(subject) > 2 and len(obj) > 2:
                            fact = Fact(
                                subject=subject[:100],  # Limiter la taille
                                predicate=rel_type.value,
                                object=obj[:100],
                                confidence=0.7,
                                source_text=sentence[:200],
                            )
                            facts.append(fact)
                except Exception:
                    continue

        return facts

    def _build_knowledge_graph(self):
        """Construit le graphe de connaissances √† partir des entit√©s et faits"""

        # Ajouter les entit√©s comme n≈ìuds
        for entity in self.entities:
            node_id = self._get_node_id(entity.text)

            if node_id not in self.knowledge_graph:
                self.knowledge_graph[node_id] = {
                    "text": entity.text,
                    "type": entity.entity_type.value,
                    "normalized": entity.normalized_form,
                    "contexts": [],
                    "relations": [],
                    "mentions": 0,
                }

            self.knowledge_graph[node_id]["contexts"].append(entity.context)
            self.knowledge_graph[node_id]["mentions"] += 1

        # Ajouter les faits comme ar√™tes
        for fact in self.facts:
            subj_id = self._get_node_id(fact.subject)
            obj_id = self._get_node_id(fact.object)

            # Cr√©er les n≈ìuds s'ils n'existent pas
            for node_id, text in [(subj_id, fact.subject), (obj_id, fact.object)]:
                if node_id not in self.knowledge_graph:
                    self.knowledge_graph[node_id] = {
                        "text": text,
                        "type": "concept",
                        "normalized": text.lower(),
                        "contexts": [fact.source_text],
                        "relations": [],
                        "mentions": 1,
                    }

            # Ajouter la relation
            self.knowledge_graph[subj_id]["relations"].append(
                {
                    "predicate": fact.predicate,
                    "target": obj_id,
                    "confidence": fact.confidence,
                }
            )

    def _get_node_id(self, text: str) -> str:
        """G√©n√®re un ID unique pour un n≈ìud du graphe"""
        return hashlib.md5(text.lower().strip().encode()).hexdigest()[:12]

    def _build_indexes(self):
        """Construit les index invers√©s pour recherche rapide"""

        # Index des entit√©s par texte
        for entity in self.entities:
            key = entity.text.lower()
            self.entity_index[key].append(entity)

            # Aussi indexer par type
            type_key = f"type:{entity.entity_type.value}"
            self.entity_index[type_key].append(entity)

        # Index des faits par sujet et objet
        for fact in self.facts:
            self.fact_index[fact.subject.lower()].append(fact)
            self.fact_index[fact.object.lower()].append(fact)

        # Index des mots-cl√©s
        for section in self.sections:
            for keyword in section.keywords:
                self.keyword_index[keyword].append((section.title, 1.0))

    def _compute_statistics(self, content: str):
        """Calcule les statistiques du document"""
        self.document_stats["total_words"] = len(content.split())
        self.document_stats["total_sentences"] = len(re.split(r"[.!?]+", content))
        self.document_stats["total_sections"] = len(self.sections)

        # Comptage des entit√©s par type
        for entity in self.entities:
            self.document_stats["entity_counts"][entity.entity_type.value] += 1

        # Fr√©quence des mots-cl√©s globaux
        all_keywords = []
        for section in self.sections:
            all_keywords.extend(section.keywords)
        self.document_stats["keyword_frequency"] = Counter(all_keywords)

    def answer_question(self, question: str) -> Dict[str, Any]:
        """
        üéØ R√©pond √† une question sur le document analys√©

        1. Analyse de la question
        2. Recherche dans le graphe de connaissances
        3. Extraction des informations pertinentes
        4. G√©n√©ration de la r√©ponse en langage naturel
        """
        print(f"‚ùì [QUESTION] '{question}'")

        # √âtape 1: Analyser la question
        question_analysis = self._analyze_question(question)
        print(
            f"üîç [QUESTION] Type: {question_analysis['type']}, Focus: {question_analysis['focus']}"
        )

        # √âtape 2: Rechercher les informations pertinentes
        relevant_info = self._search_knowledge(question_analysis)
        print(
            f"üìö [QUESTION] {len(relevant_info['entities'])} entit√©s, {len(relevant_info['facts'])} faits trouv√©s"
        )

        # √âtape 3: G√©n√©rer la r√©ponse
        response = self._generate_answer(question_analysis, relevant_info)

        return {
            "answer": response["text"],
            "confidence": response["confidence"],
            "sources": response["sources"],
            "entities_used": relevant_info["entities"],
            "facts_used": relevant_info["facts"],
        }

    def _analyze_question(self, question: str) -> Dict[str, Any]:
        """Analyse une question pour comprendre ce qui est demand√©"""
        question_lower = question.lower()

        # D√©terminer le type de question
        question_type = "what"  # Par d√©faut
        for qtype, keywords in self.question_types.items():
            if any(kw in question_lower for kw in keywords):
                question_type = qtype
                break

        # Extraire les entit√©s de la question
        question_entities = self._extract_entities(question)

        # Identifier le focus de la question (ce sur quoi porte la question)
        focus_terms = []

        # Chercher les termes s√©mantiques
        for concept, synonyms in self.semantic_equivalences.items():
            if any(syn in question_lower for syn in synonyms):
                focus_terms.append(concept)

        # Extraire les mots-cl√©s de la question
        keywords = self._extract_keywords(question, top_n=5)

        # D√©tecter les contraintes (comparaisons, valeurs attendues)
        constraints = []
        if re.search(r"[<>‚â§‚â•]", question):
            constraints.append("comparison")
        if re.search(r"\d+", question):
            constraints.append("numeric")

        return {
            "type": question_type,
            "focus": focus_terms,
            "keywords": keywords,
            "entities": question_entities,
            "constraints": constraints,
            "original": question,
        }

    def _search_knowledge(self, question_analysis: Dict) -> Dict[str, Any]:
        """Recherche les informations pertinentes dans le graphe de connaissances"""
        relevant_entities = []
        relevant_facts = []
        relevant_sections = []

        # Recherche par mots-cl√©s
        keywords = question_analysis["keywords"]
        focus_terms = question_analysis["focus"]

        # Recherche dans les entit√©s
        for keyword in keywords + focus_terms:
            if keyword in self.entity_index:
                relevant_entities.extend(self.entity_index[keyword])

            # Recherche partielle
            for key, entities in self.entity_index.items():
                if keyword in key or key in keyword:
                    relevant_entities.extend(entities)

        # Recherche dans les faits
        for keyword in keywords + focus_terms:
            if keyword in self.fact_index:
                relevant_facts.extend(self.fact_index[keyword])

            # Recherche partielle
            for key, facts in self.fact_index.items():
                if keyword in key or key in keyword:
                    relevant_facts.extend(facts)

        # Recherche dans les sections
        for section in self.sections:
            score = 0
            section_lower = section.content.lower()

            for keyword in keywords:
                if keyword in section_lower:
                    score += section_lower.count(keyword)

            for term in focus_terms:
                for syn in self.semantic_equivalences.get(term, [term]):
                    if syn in section_lower:
                        score += 2

            if score > 0:
                relevant_sections.append((section, score))

        # Trier par pertinence
        relevant_sections.sort(key=lambda x: x[1], reverse=True)

        # D√©dupliquer
        seen_entities = set()
        unique_entities = []
        for e in relevant_entities:
            if e.text.lower() not in seen_entities:
                seen_entities.add(e.text.lower())
                unique_entities.append(e)

        seen_facts = set()
        unique_facts = []
        for f in relevant_facts:
            fact_key = f"{f.subject}:{f.predicate}:{f.object}"
            if fact_key not in seen_facts:
                seen_facts.add(fact_key)
                unique_facts.append(f)

        return {
            "entities": unique_entities[:10],  # Top 10
            "facts": unique_facts[:10],
            "sections": [s for s, _ in relevant_sections[:5]],  # Top 5 sections
        }

    def _generate_answer(
        self, analysis: Dict, info: Dict
    ) -> Dict[str, Any]:
        """G√©n√®re une r√©ponse en langage naturel"""

        question_type = analysis["type"]
        focus = analysis["focus"]
        entities = info["entities"]
        facts = info["facts"]
        sections = info["sections"]

        # Aucune information trouv√©e
        if not entities and not facts and not sections:
            return {
                "text": "Je n'ai pas trouv√© d'information pertinente dans les documents analys√©s pour r√©pondre √† cette question.",
                "confidence": 0.0,
                "sources": [],
            }

        # Construire la r√©ponse selon le type de question
        answer_parts = []
        confidence = 0.0
        sources = []

        # Questions sur une valeur sp√©cifique (version, nombre, date)
        if question_type == "what" or question_type == "how_much":
            # Chercher les entit√©s pertinentes selon le focus
            for entity in entities:
                if self._entity_matches_focus(entity, focus):
                    answer_parts.append(self._format_entity_answer(entity, focus))
                    confidence = max(confidence, entity.confidence)
                    sources.append(entity.context[:100])

        # Questions temporelles
        elif question_type == "when":
            date_entities = [e for e in entities if e.entity_type == EntityType.DATE]
            if date_entities:
                best_date = max(date_entities, key=lambda e: e.confidence)
                answer_parts.append(f"Cela s'est pass√© en {best_date.text}.")
                confidence = best_date.confidence
                sources.append(best_date.context)

        # Questions sur une personne
        elif question_type == "who":
            person_entities = [
                e for e in entities if e.entity_type == EntityType.PERSON
            ]
            if person_entities:
                persons = [e.text for e in person_entities]
                answer_parts.append(f"Il s'agit de {', '.join(persons)}.")
                confidence = max(e.confidence for e in person_entities)
                sources.extend([e.context for e in person_entities])

        # Si pas assez d'entit√©s, utiliser les faits
        if not answer_parts and facts:
            relevant_fact = facts[0]  # Le plus pertinent
            answer_parts.append(
                f"{relevant_fact.subject.capitalize()} {self._predicate_to_french(relevant_fact.predicate)} {relevant_fact.object}."
            )
            confidence = relevant_fact.confidence
            sources.append(relevant_fact.source_text)

        # Si toujours rien, utiliser les sections
        if not answer_parts and sections:
            best_section = sections[0]
            # Extraire un passage pertinent
            passage = self._extract_relevant_passage(
                best_section.content, analysis["keywords"]
            )
            if passage:
                answer_parts.append(passage)
                confidence = 0.6
                sources.append(f"Section: {best_section.title}")

        # Construire la r√©ponse finale
        if answer_parts:
            final_answer = " ".join(answer_parts)
        else:
            final_answer = "Je n'ai pas pu trouver une r√©ponse pr√©cise √† cette question dans les documents."
            confidence = 0.2

        return {
            "text": final_answer,
            "confidence": confidence,
            "sources": sources[:3],  # Max 3 sources
        }

    def _entity_matches_focus(self, entity: Entity, focus: List[str]) -> bool:
        """V√©rifie si une entit√© correspond au focus de la question"""
        if not focus:
            return True

        entity_type_mapping = {
            "version": [EntityType.VERSION],
            "performance": [EntityType.NUMBER],
            "temps": [EntityType.NUMBER, EntityType.DATE],
            "capacit√©": [EntityType.NUMBER],
            "langage": [EntityType.TECHNOLOGY],
            "date": [EntityType.DATE],
            "cr√©ateur": [EntityType.PERSON],
        }

        for f in focus:
            if f in entity_type_mapping:
                if entity.entity_type in entity_type_mapping[f]:
                    return True

        return False

    def _format_entity_answer(self, entity: Entity, focus: List[str]) -> str:
        """Formate une entit√© en r√©ponse naturelle"""
        if entity.entity_type == EntityType.VERSION:
            return f"La version est {entity.text}."
        elif entity.entity_type == EntityType.NUMBER:
            if "temps" in focus or "performance" in focus:
                return f"Le temps est de {entity.text}."
            elif "capacit√©" in focus:
                return f"La capacit√© est de {entity.text}."
            else:
                return f"La valeur est {entity.text}."
        elif entity.entity_type == EntityType.DATE:
            return f"La date est {entity.text}."
        elif entity.entity_type == EntityType.PERSON:
            return f"Il s'agit de {entity.text}."
        elif entity.entity_type == EntityType.TECHNOLOGY:
            return f"La technologie utilis√©e est {entity.text}."
        else:
            return f"{entity.text}."

    def _predicate_to_french(self, predicate: str) -> str:
        """Convertit un pr√©dicat en verbe fran√ßais"""
        mapping = {
            "is_a": "est",
            "has": "a",
            "belongs_to": "appartient √†",
            "created_by": "a √©t√© cr√©√© par",
            "located_in": "se trouve √†",
            "occurred_at": "s'est pass√© le",
            "value_of": "a pour valeur",
            "uses": "utilise",
            "produces": "produit",
            "requires": "n√©cessite",
            "equals": "est √©gal √†",
            "greater_than": "est sup√©rieur √†",
            "less_than": "est inf√©rieur √†",
        }
        return mapping.get(predicate, predicate)

    def _extract_relevant_passage(self, content: str, keywords: List[str]) -> str:
        """Extrait le passage le plus pertinent d'un contenu"""
        sentences = re.split(r"[.!?]+", content)

        best_sentence = ""
        best_score = 0

        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) < 20:
                continue

            score = sum(1 for kw in keywords if kw.lower() in sentence.lower())

            if score > best_score:
                best_score = score
                best_sentence = sentence

        if best_sentence:
            return best_sentence + "."

        # Fallback: premiers 200 caract√®res
        return content[:200].strip() + "..."

    def get_document_summary(self) -> str:
        """G√©n√®re un r√©sum√© du document analys√©"""
        if not self.sections:
            return "Aucun document n'a √©t√© analys√©."

        summary_parts = []

        # Statistiques g√©n√©rales
        summary_parts.append("üìä **Statistiques du document:**")
        summary_parts.append(f"- {self.document_stats['total_words']} mots")
        summary_parts.append(f"- {self.document_stats['total_sections']} sections")
        summary_parts.append(f"- {len(self.entities)} entit√©s identifi√©es")
        summary_parts.append(f"- {len(self.facts)} faits extraits")

        # Principaux sujets
        top_keywords = self.document_stats["keyword_frequency"].most_common(5)
        if top_keywords:
            kw_list = ", ".join([kw for kw, _ in top_keywords])
            summary_parts.append(f"\nüìå **Sujets principaux:** {kw_list}")

        # Entit√©s cl√©s
        if self.entities:
            entity_types = Counter(e.entity_type.value for e in self.entities)
            summary_parts.append("\nüè∑Ô∏è **Types d'entit√©s:**")
            for etype, count in entity_types.most_common(5):
                summary_parts.append(f"- {etype}: {count}")

        # Titres des sections principales
        main_sections = [s for s in self.sections if s.level == 1][:5]
        if main_sections:
            summary_parts.append("\nüìë **Sections principales:**")
            for section in main_sections:
                summary_parts.append(f"- {section.title}")

        return "\n".join(summary_parts)


# Instance globale pour utilisation
document_analyzer = IntelligentDocumentAnalyzer()
