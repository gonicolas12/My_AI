"""
Syst√®me d'Agents IA Sp√©cialis√©s bas√©s sur Ollama
Chaque agent a une expertise et un comportement sp√©cifique
"""

from typing import Dict, List, Optional, Any
from datetime import datetime
import re

from models.local_llm import LocalLLM
from models.internet_search import EnhancedInternetSearchEngine


class AIAgent:
    """
    Agent IA de base avec expertise sp√©cialis√©e
    """

    def __init__(
        self,
        name: str,
        expertise: str,
        system_prompt: str,
        model: str = "llama3.2",
        temperature: float = 0.7,
    ):
        """
        Initialise un agent IA

        Args:
            name: Nom de l'agent
            expertise: Domaine d'expertise
            system_prompt: Prompt syst√®me qui d√©finit le comportement
            model: Mod√®le Ollama √† utiliser
            temperature: Cr√©ativit√© (0.0-1.0)
        """
        self.name = name
        self.expertise = expertise
        self.system_prompt = system_prompt
        self.model = model
        self.temperature = temperature

        # Cr√©er une instance LLM d√©di√©e pour cet agent
        self.llm = LocalLLM(model=model)

        # Historique des t√¢ches de cet agent
        self.task_history: List[Dict[str, Any]] = []

        # Statistiques
        self.stats = {
            "tasks_completed": 0,
            "total_tokens": 0,
            "success_rate": 1.0,
            "created_at": datetime.now().isoformat(),
        }

        print(f"ü§ñ Agent '{self.name}' initialis√© (expertise: {self.expertise})")

    def execute_task(self, task: str, context: Optional[Dict] = None) -> Dict[str, Any]:
        """
        Ex√©cute une t√¢che avec le contexte de l'agent

        Args:
            task: Description de la t√¢che
            context: Contexte additionnel

        Returns:
            Dict avec le r√©sultat et les m√©tadonn√©es
        """
        if not self.llm.is_ollama_available:
            return {
                "success": False,
                "result": "‚ùå Ollama non disponible",
                "agent": self.name,
                "error": "ollama_unavailable",
            }

        # Construire le prompt enrichi
        full_prompt = self._build_prompt(task, context)

        try:
            print(f"‚öôÔ∏è Agent {self.name} ex√©cute la t√¢che...")

            # G√©n√©rer la r√©ponse
            response = self.llm.generate(
                prompt=full_prompt, system_prompt=self.system_prompt
            )

            if response:
                # Enregistrer dans l'historique
                task_record = {
                    "task": task,
                    "result": response,
                    "timestamp": datetime.now().isoformat(),
                    "context": context,
                }
                self.task_history.append(task_record)

                # Mettre √† jour les stats
                self.stats["tasks_completed"] += 1

                print(f"‚úÖ Agent {self.name} a termin√© la t√¢che")

                return {
                    "success": True,
                    "result": response,
                    "agent": self.name,
                    "expertise": self.expertise,
                    "timestamp": task_record["timestamp"],
                }
            else:
                return {
                    "success": False,
                    "result": "‚ùå Pas de r√©ponse g√©n√©r√©e",
                    "agent": self.name,
                    "error": "no_response",
                }

        except Exception as e:
            print(f"‚ùå Erreur agent {self.name}: {e}")
            return {
                "success": False,
                "result": f"‚ùå Erreur: {str(e)}",
                "agent": self.name,
                "error": str(e),
            }

    def execute_task_stream(
        self, task: str, context: Optional[Dict] = None, on_token=None
    ) -> Dict[str, Any]:
        """
        Ex√©cute une t√¢che avec streaming (affichage progressif)

        Args:
            task: Description de la t√¢che
            context: Contexte additionnel
            on_token: Callback appel√© pour chaque token re√ßu

        Returns:
            Dict avec le r√©sultat et les m√©tadonn√©es
        """
        if not self.llm.is_ollama_available:
            return {
                "success": False,
                "result": "‚ùå Ollama non disponible",
                "agent": self.name,
                "error": "ollama_unavailable",
            }

        # Construire le prompt enrichi
        full_prompt = self._build_prompt(task, context)

        try:
            print(f"‚öôÔ∏è Agent {self.name} ex√©cute la t√¢che (streaming)...")

            # G√©n√©rer la r√©ponse avec streaming
            response = self.llm.generate_stream(
                prompt=full_prompt, system_prompt=self.system_prompt, on_token=on_token
            )

            if response:
                # Enregistrer dans l'historique
                task_record = {
                    "task": task,
                    "result": response,
                    "timestamp": datetime.now().isoformat(),
                    "context": context,
                }
                self.task_history.append(task_record)

                # Mettre √† jour les stats
                self.stats["tasks_completed"] += 1

                print(f"‚úÖ Agent {self.name} a termin√© la t√¢che")

                return {
                    "success": True,
                    "result": response,
                    "agent": self.name,
                    "expertise": self.expertise,
                    "timestamp": task_record["timestamp"],
                }
            else:
                return {
                    "success": False,
                    "result": "‚ùå Pas de r√©ponse g√©n√©r√©e",
                    "agent": self.name,
                    "error": "no_response",
                }

        except Exception as e:
            print(f"‚ùå Erreur agent {self.name}: {e}")
            return {
                "success": False,
                "result": f"‚ùå Erreur: {str(e)}",
                "agent": self.name,
                "error": str(e),
            }

    def _build_prompt(self, task: str, context: Optional[Dict]) -> str:
        """Construit le prompt enrichi avec contexte"""
        prompt_parts = [f"T√ÇCHE: {task}"]

        if context:
            prompt_parts.append("\nCONTEXTE:")
            for key, value in context.items():
                prompt_parts.append(f"- {key}: {value}")

        # Ajouter l'historique r√©cent si pertinent
        if len(self.task_history) > 0:
            recent_tasks = self.task_history[-3:]  # 3 derni√®res t√¢ches
            prompt_parts.append("\nHISTORIQUE R√âCENT:")
            for i, task_record in enumerate(recent_tasks, 1):
                prompt_parts.append(f"{i}. {task_record['task'][:100]}...")

        return "\n".join(prompt_parts)

    def get_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques de l'agent"""
        return {
            "name": self.name,
            "expertise": self.expertise,
            "model": self.model,
            **self.stats,
            "recent_tasks": len(self.task_history[-10:]),
        }

    def reset_history(self):
        """Efface l'historique des t√¢ches"""
        self.task_history.clear()
        self.llm.clear_history()
        print(f"üóëÔ∏è Historique de l'agent {self.name} effac√©")


class WebSearchAgent(AIAgent):
    """
    Agent sp√©cialis√© en recherche internet R√âELLE
    Utilise EnhancedInternetSearchEngine pour faire de vraies recherches
    puis synth√©tise les r√©sultats avec Ollama
    """

    def __init__(
        self,
        name: str = "WebSearchAgent",
        expertise: str = "Recherche Internet & Fact-Checking",
        model: str = "llama3.2",
        temperature: float = 0.5,
        focus_year: int = None,
    ):
        """
        Initialise l'agent de recherche web

        Args:
            name: Nom de l'agent
            expertise: Domaine d'expertise
            model: Mod√®le Ollama
            temperature: Cr√©ativit√©
            focus_year: Ann√©e sp√©cifique √† privil√©gier (optionnel, None = pas de filtre)
        """
        # Construire le comportement selon focus_year
        year_instruction = f"- Privil√©gier les sources datant de {focus_year} quand pertinent\n" if focus_year else "- Privil√©gier les sources les plus r√©centes et pertinentes\n"

        system_prompt = f"""Tu es {name}, un agent de recherche internet expert.

EXPERTISE: Recherche web approfondie, fact-checking, croisement de sources, analyse de sources r√©centes

COMPORTEMENT:
- Tu re√ßois des R√âSULTATS DE RECHERCHE R√âELS provenant d'internet
- Tu DOIS te baser UNIQUEMENT sur ces r√©sultats pour r√©pondre
- Ne JAMAIS inventer de donn√©es ou d'informations
- Croiser plusieurs sources quand disponibles
{year_instruction}- Citer les sources utilis√©es
- Si les r√©sultats sont insuffisants, le dire clairement

FORMAT DE R√âPONSE:
- Introduction concise
- Informations factuelles bas√©es sur les sources
- Sources utilis√©es
- Note sur la fiabilit√© si pertinent
- Suggestions pour approfondir si n√©cessaire

IMPORTANT: Si tu ne trouves pas d'information dans les r√©sultats fournis, DIS-LE clairement au lieu d'inventer."""

        super().__init__(
            name=name,
            expertise=expertise,
            system_prompt=system_prompt,
            model=model,
            temperature=temperature,
        )

        # Module de recherche internet R√âEL avec acc√®s au LLM pour analyse intelligente
        self.search_engine = EnhancedInternetSearchEngine(llm=self.llm)
        self.focus_year = focus_year

        if focus_year:
            print(f"üåê Agent de recherche web initialis√© avec Ollama (focus: {focus_year})")
        else:
            print("üåê Agent de recherche web initialis√© avec Ollama (mode: recherche universelle)")

    def _optimize_search_query(self, query: str) -> str:
        """
        Optimise la requ√™te de recherche avec LLM (1er appel Ollama).
        Transforme une question naturelle en requ√™te de recherche concise et efficace.
        Fallback regex si LLM indisponible.

        Exemple: "cherche les r√©sultats des √©lecions l√©gilatives de 2024 en France"
                 -> "√©lections l√©gislatives France 2024 r√©sultats"
        """
        # Nettoyage de base commun
        query_clean = query.strip()
        query_clean = re.sub(r"\s+", " ", query_clean)

        # Appel LLM si disponible
        if self.llm and self.llm.is_ollama_available:
            try:
                llm_prompt = (
                    f"Transforme cette demande en une requ√™te de recherche Wikipedia courte et efficace "
                    f"(5 √† 8 mots maximum, mots-cl√©s essentiels uniquement, sans verbes ni politesse). "
                    f"R√©ponds UNIQUEMENT avec la requ√™te, rien d'autre.\n\nDemande: {query_clean}"
                )
                optimized = self.llm.generate(
                    prompt=llm_prompt,
                    system_prompt="Tu es un expert en recherche d'information. R√©ponds uniquement avec la requ√™te optimis√©e, sans explication ni ponctuation.",
                )
                if optimized:
                    optimized = optimized.strip().strip("\"'.,!?:;\n\r")
                    # Sanity check: r√©sultat raisonnable (entre 3 et 120 chars)
                    if 3 <= len(optimized) <= 120:
                        print(f"üîß Requ√™te optimis√©e (LLM): '{query}' ‚Üí '{optimized}'")
                        return optimized
            except Exception as e:
                print(f"‚ö†Ô∏è LLM query optimization failed, fallback regex: {e}")

        # Fallback regex
        query_clean = re.sub(
            r"^(?:peux[-\s]?tu\s+|pourrais[-\s]?tu\s+|merci\s+de\s+)?",
            "",
            query_clean,
            flags=re.IGNORECASE,
        )
        query_clean = re.sub(
            r"^(?:cherche(?:r)?|trouve(?:r)?|recherche(?:r)?)\s+(?:sur\s+)?(?:internet|web|google)?\s*",
            "",
            query_clean,
            flags=re.IGNORECASE,
        )
        query_clean = query_clean.strip(" .,!?:;\"'\n\r")

        if len(query_clean) >= 3 and len(query_clean) < len(query):
            print(f"üîß Requ√™te optimis√©e (regex): '{query}' ‚Üí '{query_clean}'")
            return query_clean
        return query

    def execute_task(self, task: str, context: Optional[Dict] = None) -> Dict[str, Any]:
        """
        Ex√©cute une recherche internet R√âELLE puis synth√©tise avec Ollama

        Args:
            task: Question/requ√™te de recherche
            context: Contexte additionnel

        Returns:
            Dict avec r√©sultats r√©els de la recherche
        """
        if not self.llm.is_ollama_available:
            return {
                "success": False,
                "result": "‚ùå Ollama non disponible",
                "agent": self.name,
                "error": "ollama_unavailable",
            }

        try:
            # √âTAPE 0: Optimiser la requ√™te avec Ollama
            optimized_query = self._optimize_search_query(task)
            print(f"üåê Lancement de la recherche pour: '{optimized_query[:100]}'")

            # √âTAPE 1: Recherche source unique (sans synth√®se interm√©diaire)
            search_results = self.search_engine.search_best_source_context(
                optimized_query
            )

            if not search_results or len(str(search_results)) < 50:
                return {
                    "success": False,
                    "result": f"‚ùå Aucun r√©sultat de recherche trouv√© pour: {optimized_query}",
                    "agent": self.name,
                    "error": "no_search_results",
                }

            # √âTAPE 2: Filtrer/prioriser sources de l'ann√©e focus (seulement si focus_year d√©fini)
            if self.focus_year:
                filtered_results = self._filter_by_year(search_results, self.focus_year)
            else:
                filtered_results = search_results

            # √âTAPE 3: Construire le prompt avec LES VRAIS R√âSULTATS
            synthesis_prompt = f"""QUESTION ORIGINALE: {task}

R√âSULTATS DE RECHERCHE INTERNET (SOURCES R√âELLES):
{filtered_results if filtered_results else search_results}

------------------------------------------

INSTRUCTIONS:
1. Analyse ces r√©sultats de recherche R√âELS
2. R√©ponds √† la question en te basant UNIQUEMENT sur ces informations
3. Cite les sources utilis√©es
4. Si les informations sont contradictoires, mentionne-le
5. Si les r√©sultats sont insuffisants pour r√©pondre compl√®tement, dis-le clairement
6. NE JAMAIS inventer ou supposer des informations qui ne sont pas dans les r√©sultats

R√©ponds maintenant:"""

            # √âTAPE 4: Demander √† Ollama de SYNTH√âTISER les vrais r√©sultats
            print(f"üß† Synth√®se des r√©sultats avec {self.model}...")
            synthesis = self.llm.generate(
                prompt=synthesis_prompt, system_prompt=self.system_prompt
            )

            if synthesis:
                # Enregistrer dans l'historique
                task_record = {
                    "task": task,
                    "search_results": search_results,
                    "synthesis": synthesis,
                    "timestamp": datetime.now().isoformat(),
                    "context": context,
                }
                self.task_history.append(task_record)
                self.stats["tasks_completed"] += 1

                print("‚úÖ Recherche et synth√®se termin√©es")

                return {
                    "success": True,
                    "result": synthesis,
                    "agent": self.name,
                    "expertise": self.expertise,
                    "search_results_raw": search_results,
                    "timestamp": task_record["timestamp"],
                }
            else:
                return {
                    "success": False,
                    "result": f"‚ùå √âchec de la synth√®se\n\nR√©sultats bruts:\n{search_results}",
                    "agent": self.name,
                    "error": "synthesis_failed",
                }

        except Exception as e:
            print(f"‚ùå Erreur recherche web: {e}")
            return {
                "success": False,
                "result": f"‚ùå Erreur lors de la recherche: {str(e)}",
                "agent": self.name,
                "error": str(e),
            }

    def execute_task_stream(
        self, task: str, context: Optional[Dict] = None, on_token=None
    ) -> Dict[str, Any]:
        """
        Ex√©cute une recherche avec streaming de la synth√®se

        Args:
            task: Question/requ√™te
            context: Contexte
            on_token: Callback streaming

        Returns:
            R√©sultats avec streaming
        """
        if not self.llm.is_ollama_available:
            return {
                "success": False,
                "result": "‚ùå Ollama non disponible",
                "agent": self.name,
                "error": "ollama_unavailable",
            }

        try:
            # √âTAPE 0: Optimiser la requ√™te avec Ollama
            optimized_query = self._optimize_search_query(task)
            print(f"üåê Lancement de la recherche pour: '{optimized_query[:100]}'")

            # √âTAPE 1: Recherche source unique (pas de synth√®se interm√©diaire)
            search_results = self.search_engine.search_best_source_context(
                optimized_query
            )

            if not search_results or len(str(search_results)) < 50:
                error_msg = (
                    f"‚ùå Aucun r√©sultat de recherche trouv√© pour: {optimized_query}"
                )
                if on_token:
                    on_token(error_msg)
                return {
                    "success": False,
                    "result": error_msg,
                    "agent": self.name,
                    "error": "no_search_results",
                }

            # Notifier l'utilisateur que la recherche est termin√©e
            if on_token:
                on_token("\nüîç **Recherche termin√©e** - Synth√®se en cours...\n\n")

            # √âTAPE 2: Filtrer par ann√©e (seulement si focus_year d√©fini)
            if self.focus_year:
                filtered_results = self._filter_by_year(search_results, self.focus_year)
            else:
                filtered_results = search_results

            # √âTAPE 3: Construire le prompt
            synthesis_prompt = f"""QUESTION ORIGINALE: {task}

R√âSULTATS DE RECHERCHE INTERNET (SOURCES R√âELLES):
{filtered_results if filtered_results else search_results}

------------------------------------------

INSTRUCTIONS:
1. Analyse ces r√©sultats de recherche R√âELS
2. R√©ponds √† la question en te basant UNIQUEMENT sur ces informations
3. Cite les sources utilis√©es
4. Si les informations sont contradictoires, mentionne-le
5. Si les r√©sultats sont insuffisants, dis-le clairement
6. NE JAMAIS inventer des informations

R√©ponds maintenant:"""

            # √âTAPE 4: Streaming de la synth√®se
            print(f"üß† Synth√®se streaming avec {self.model}...")
            synthesis = self.llm.generate_stream(
                prompt=synthesis_prompt,
                system_prompt=self.system_prompt,
                on_token=on_token,
            )

            if synthesis:
                task_record = {
                    "task": task,
                    "search_results": search_results,
                    "synthesis": synthesis,
                    "timestamp": datetime.now().isoformat(),
                    "context": context,
                }
                self.task_history.append(task_record)
                self.stats["tasks_completed"] += 1

                print("‚úÖ Recherche et synth√®se streaming termin√©es")

                return {
                    "success": True,
                    "result": synthesis,
                    "agent": self.name,
                    "expertise": self.expertise,
                    "search_results_raw": search_results,
                    "timestamp": task_record["timestamp"],
                }
            else:
                fallback = (
                    f"‚ùå √âchec de la synth√®se\n\nR√©sultats bruts:\n{search_results}"
                )
                if on_token:
                    on_token(fallback)
                return {
                    "success": False,
                    "result": fallback,
                    "agent": self.name,
                    "error": "synthesis_failed",
                }

        except Exception as e:
            error_msg = f"‚ùå Erreur lors de la recherche: {str(e)}"
            print(error_msg)
            if on_token:
                on_token(error_msg)
            return {
                "success": False,
                "result": error_msg,
                "agent": self.name,
                "error": str(e),
            }

    def _filter_by_year(self, search_results: str, year: int) -> str:
        """
        Filtre/priorise les r√©sultats contenant l'ann√©e sp√©cifi√©e

        Args:
            search_results: R√©sultats bruts
            year: Ann√©e √† prioriser

        Returns:
            R√©sultats filtr√©s ou texte indiquant la priorit√©
        """
        # Chercher des mentions de l'ann√©e dans les r√©sultats
        year_pattern = rf"\b{year}\b"
        lines = search_results.split("\n")

        # Lignes contenant l'ann√©e
        year_lines = [line for line in lines if re.search(year_pattern, line)]

        if year_lines:
            filtered = "\n".join(year_lines)
            return f"""‚ö†Ô∏è R√âSULTATS FILTR√âS - Focus sur {year}:

{filtered}

--- Tous les r√©sultats ---

{search_results}"""
        else:
            return f"""‚ö†Ô∏è Aucune source sp√©cifique √† {year} trouv√©e. Voici les r√©sultats disponibles:

{search_results}"""


class AgentFactory:
    """
    Factory pour cr√©er des agents pr√©-configur√©s
    """

    @staticmethod
    def create_code_agent(model: str = "llama3.2") -> AIAgent:
        """Agent sp√©cialis√© en g√©n√©ration et debug de code"""
        system_prompt = """Tu es CodeAgent, un expert en programmation.
        
EXPERTISE: D√©veloppement logiciel, algorithmes, debug, optimisation de code
LANGAGES: Python, JavaScript, Java, C++, HTML/CSS, SQL

COMPORTEMENT:
- G√©n√®re du code propre, comment√© et fonctionnel
- Explique chaque partie du code
- Propose des alternatives et optimisations
- D√©tecte et corrige les bugs
- Suit les bonnes pratiques et conventions

FORMAT DE R√âPONSE:
- Code dans des blocs ```langage
- Commentaires clairs
- Exemples d'utilisation
- Tests unitaires si pertinent"""

        return AIAgent(
            name="CodeAgent",
            expertise="Programmation & Debug",
            system_prompt=system_prompt,
            model=model,
            temperature=0.3,  # Moins cr√©atif, plus pr√©cis
        )

    @staticmethod
    def create_analyst_agent(model: str = "llama3.2") -> AIAgent:
        """Agent sp√©cialis√© en analyse de donn√©es et documents"""
        system_prompt = """Tu es AnalystAgent, un analyste de donn√©es expert.

EXPERTISE: Analyse de donn√©es, statistiques, visualisation, extraction d'insights, documents

COMPORTEMENT:
- Analyse en profondeur les donn√©es fournies
- Identifie les patterns et tendances
- Calcule des statistiques pertinentes
- Propose des visualisations
- Tire des conclusions bas√©es sur les donn√©es

FORMAT DE R√âPONSE:
- R√©sum√© ex√©cutif
- Analyses d√©taill√©es avec chiffres
- Insights cl√©s
- Recommandations
- Graphiques sugg√©r√©s (en texte)"""

        return AIAgent(
            name="AnalystAgent",
            expertise="Analyse de Donn√©es",
            system_prompt=system_prompt,
            model=model,
            temperature=0.4,
        )

    @staticmethod
    def create_creative_agent(model: str = "llama3.2") -> AIAgent:
        """Agent sp√©cialis√© en contenu cr√©atif"""
        system_prompt = """Tu es CreativeAgent, un cr√©ateur de contenu expert.

EXPERTISE: R√©daction cr√©ative, storytelling, marketing, communication

COMPORTEMENT:
- G√©n√®re du contenu original et engageant
- Adapte le ton et le style √† l'audience
- Utilise des techniques narratives efficaces
- Propose plusieurs variantes
- Optimise pour la lisibilit√© et l'impact

FORMAT DE R√âPONSE:
- Contenu principal
- Variantes propos√©es
- Explications des choix cr√©atifs
- Suggestions d'am√©lioration"""

        return AIAgent(
            name="CreativeAgent",
            expertise="Contenu Cr√©atif",
            system_prompt=system_prompt,
            model=model,
            temperature=0.8,  # Plus cr√©atif
        )

    @staticmethod
    def create_debug_agent(model: str = "llama3.2") -> AIAgent:
        """Agent sp√©cialis√© en d√©tection et correction d'erreurs"""
        system_prompt = """Tu es DebugAgent, un expert en debug et correction d'erreurs.

EXPERTISE: D√©bogage, analyse d'erreurs, tests, qualit√© du code

COMPORTEMENT:
- Identifie les bugs et erreurs logiques
- Analyse les stack traces
- Propose des corrections pr√©cises
- Sugg√®re des tests pour pr√©venir les r√©gressions
- Explique la cause racine du probl√®me

FORMAT DE R√âPONSE:
- Probl√®me identifi√©
- Cause racine
- Solution propos√©e avec code corrig√©
- Tests recommand√©s
- Conseils de pr√©vention"""

        return AIAgent(
            name="DebugAgent",
            expertise="Debug & Correction",
            system_prompt=system_prompt,
            model=model,
            temperature=0.2,  # Tr√®s pr√©cis
        )

    @staticmethod
    def create_planner_agent(model: str = "llama3.2") -> AIAgent:
        """Agent sp√©cialis√© en planification de t√¢ches complexes"""
        system_prompt = """Tu es PlannerAgent, un expert en planification et organisation.

EXPERTISE: D√©composition de t√¢ches, planification de projets, gestion de workflow

COMPORTEMENT:
- D√©compose les t√¢ches complexes en √©tapes simples
- Identifie les d√©pendances entre t√¢ches
- Estime les efforts et priorit√©s
- Propose un ordre d'ex√©cution optimal
- Identifie les risques potentiels

FORMAT DE R√âPONSE:
- Vue d'ensemble du projet
- Liste des t√¢ches num√©rot√©es
- D√©pendances entre t√¢ches
- Estimation de complexit√©
- Ordre d'ex√©cution recommand√©
- Risques et mitigations"""

        return AIAgent(
            name="PlannerAgent",
            expertise="Planification & Organisation",
            system_prompt=system_prompt,
            model=model,
            temperature=0.5,
        )

    @staticmethod
    def create_security_agent(model: str = "llama3.2") -> AIAgent:
        """Agent sp√©cialis√© en s√©curit√© et audit de code"""
        system_prompt = """Tu es SecurityAgent, un expert en cybers√©curit√© et audit de code.

EXPERTISE: S√©curit√© applicative, d√©tection de vuln√©rabilit√©s, audit de code, OWASP, bonnes pratiques

COMPORTEMENT:
- Analyse le code pour d√©tecter les failles de s√©curit√©
- Identifie les vuln√©rabilit√©s (injection SQL, XSS, CSRF, etc.)
- Propose des corrections s√©curis√©es
- V√©rifie la gestion des donn√©es sensibles
- Applique les recommandations OWASP et CWE
- Audite les d√©pendances et configurations

FORMAT DE R√âPONSE:
- Niveau de risque (Critique/√âlev√©/Moyen/Faible)
- Vuln√©rabilit√©s d√©tect√©es avec description
- Code corrig√© et s√©curis√©
- Recommandations de pr√©vention
- Checklist de s√©curit√©"""

        return AIAgent(
            name="SecurityAgent",
            expertise="S√©curit√© & Audit",
            system_prompt=system_prompt,
            model=model,
            temperature=0.2,
        )

    @staticmethod
    def create_optimizer_agent(model: str = "llama3.2") -> AIAgent:
        """Agent sp√©cialis√© en optimisation et performance"""
        system_prompt = """Tu es OptimizerAgent, un expert en optimisation et performance logicielle.

EXPERTISE: Optimisation de code, refactoring, performance, profilage, design patterns

COMPORTEMENT:
- Analyse les performances du code
- Identifie les goulots d'√©tranglement
- Propose des optimisations algorithmiques
- Refactore le code pour une meilleure maintenabilit√©
- Applique les design patterns appropri√©s
- Optimise la consommation m√©moire et CPU

FORMAT DE R√âPONSE:
- Analyse de complexit√© (Big O)
- Points d'optimisation identifi√©s
- Code optimis√© avec comparaison avant/apr√®s
- M√©triques de performance attendues
- Suggestions d'architecture"""

        return AIAgent(
            name="OptimizerAgent",
            expertise="Optimisation & Performance",
            system_prompt=system_prompt,
            model=model,
            temperature=0.3,
        )

    @staticmethod
    def create_datascience_agent(model: str = "llama3.2") -> AIAgent:
        """Agent sp√©cialis√© en data science et machine learning"""
        system_prompt = """Tu es DataScienceAgent, un expert en data science et machine learning.

EXPERTISE: Data science, machine learning, statistiques, mod√©lisation pr√©dictive, visualisation de donn√©es

COMPORTEMENT:
- Analyse et pr√©pare les donn√©es (nettoyage, feature engineering)
- S√©lectionne les mod√®les ML appropri√©s
- Propose des pipelines de traitement de donn√©es
- √âvalue les performances des mod√®les
- Cr√©e des visualisations pertinentes
- Interpr√®te les r√©sultats statistiques

FORMAT DE R√âPONSE:
- Analyse exploratoire des donn√©es
- Choix de mod√®le justifi√©
- Code Python avec pandas, scikit-learn, etc.
- M√©triques d'√©valuation
- Visualisations sugg√©r√©es
- Interpr√©tation des r√©sultats"""

        return AIAgent(
            name="DataScienceAgent",
            expertise="Data Science & ML",
            system_prompt=system_prompt,
            model=model,
            temperature=0.4,
        )

    @staticmethod
    def create_web_agent(
        model: str = "llama3.2", focus_year: int = None
    ) -> WebSearchAgent:
        """Agent sp√©cialis√© en recherche internet R√âELLE avec fact-checking"""
        return WebSearchAgent(
            name="WebAgent",
            expertise="Recherche Internet & Fact-Checking",
            model=model,
            temperature=0.5,
            focus_year=focus_year,
        )


# Agents pr√©-configur√©s disponibles
AVAILABLE_AGENTS = {
    "code": AgentFactory.create_code_agent,
    "web": AgentFactory.create_web_agent,  # üåê Agent de recherche internet R√âELLE (remplace research)
    "analyst": AgentFactory.create_analyst_agent,
    "creative": AgentFactory.create_creative_agent,
    "debug": AgentFactory.create_debug_agent,
    "planner": AgentFactory.create_planner_agent,
    "security": AgentFactory.create_security_agent,
    "optimizer": AgentFactory.create_optimizer_agent,
    "datascience": AgentFactory.create_datascience_agent,
}


def create_agent(agent_type: str, model: str = "llama3.2") -> Optional[AIAgent]:
    """
    Cr√©e un agent du type sp√©cifi√©

    Args:
        agent_type: Type d'agent (code, web, analyst, creative, debug, planner, security, optimizer, datascience)
        model: Mod√®le Ollama √† utiliser

    Returns:
        Instance de AIAgent ou None si type invalide
    """
    factory_func = AVAILABLE_AGENTS.get(agent_type.lower())
    if factory_func:
        return factory_func(model)
    else:
        print(f"‚ùå Type d'agent invalide: {agent_type}")
        print(f"Types disponibles: {', '.join(AVAILABLE_AGENTS.keys())}")
        return None
