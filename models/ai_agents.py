"""
Syst√®me d'Agents IA Sp√©cialis√©s bas√©s sur Ollama
Chaque agent a une expertise et un comportement sp√©cifique
"""

from typing import Dict, List, Optional, Any
from datetime import datetime

from models.local_llm import LocalLLM


class AIAgent:
    """
    Agent IA de base avec expertise sp√©cialis√©e
    """

    def __init__(
        self,
        name: str,
        expertise: str,
        system_prompt: str,
        model: str = "llama3.2",
        temperature: float = 0.7,
    ):
        """
        Initialise un agent IA

        Args:
            name: Nom de l'agent
            expertise: Domaine d'expertise
            system_prompt: Prompt syst√®me qui d√©finit le comportement
            model: Mod√®le Ollama √† utiliser
            temperature: Cr√©ativit√© (0.0-1.0)
        """
        self.name = name
        self.expertise = expertise
        self.system_prompt = system_prompt
        self.model = model
        self.temperature = temperature

        # Cr√©er une instance LLM d√©di√©e pour cet agent
        self.llm = LocalLLM(model=model)

        # Historique des t√¢ches de cet agent
        self.task_history: List[Dict[str, Any]] = []

        # Statistiques
        self.stats = {
            "tasks_completed": 0,
            "total_tokens": 0,
            "success_rate": 1.0,
            "created_at": datetime.now().isoformat(),
        }

        print(f"ü§ñ Agent '{self.name}' initialis√© (expertise: {self.expertise})")

    def execute_task(self, task: str, context: Optional[Dict] = None) -> Dict[str, Any]:
        """
        Ex√©cute une t√¢che avec le contexte de l'agent

        Args:
            task: Description de la t√¢che
            context: Contexte additionnel

        Returns:
            Dict avec le r√©sultat et les m√©tadonn√©es
        """
        if not self.llm.is_ollama_available:
            return {
                "success": False,
                "result": "‚ùå Ollama non disponible",
                "agent": self.name,
                "error": "ollama_unavailable",
            }

        # Construire le prompt enrichi
        full_prompt = self._build_prompt(task, context)

        try:
            print(f"‚öôÔ∏è Agent {self.name} ex√©cute la t√¢che...")

            # G√©n√©rer la r√©ponse
            response = self.llm.generate(
                prompt=full_prompt, system_prompt=self.system_prompt
            )

            if response:
                # Enregistrer dans l'historique
                task_record = {
                    "task": task,
                    "result": response,
                    "timestamp": datetime.now().isoformat(),
                    "context": context,
                }
                self.task_history.append(task_record)

                # Mettre √† jour les stats
                self.stats["tasks_completed"] += 1

                print(f"‚úÖ Agent {self.name} a termin√© la t√¢che")

                return {
                    "success": True,
                    "result": response,
                    "agent": self.name,
                    "expertise": self.expertise,
                    "timestamp": task_record["timestamp"],
                }
            else:
                return {
                    "success": False,
                    "result": "‚ùå Pas de r√©ponse g√©n√©r√©e",
                    "agent": self.name,
                    "error": "no_response",
                }

        except Exception as e:
            print(f"‚ùå Erreur agent {self.name}: {e}")
            return {
                "success": False,
                "result": f"‚ùå Erreur: {str(e)}",
                "agent": self.name,
                "error": str(e),
            }

    def execute_task_stream(
        self, task: str, context: Optional[Dict] = None, on_token=None
    ) -> Dict[str, Any]:
        """
        Ex√©cute une t√¢che avec streaming (affichage progressif)

        Args:
            task: Description de la t√¢che
            context: Contexte additionnel
            on_token: Callback appel√© pour chaque token re√ßu

        Returns:
            Dict avec le r√©sultat et les m√©tadonn√©es
        """
        if not self.llm.is_ollama_available:
            return {
                "success": False,
                "result": "‚ùå Ollama non disponible",
                "agent": self.name,
                "error": "ollama_unavailable",
            }

        # Construire le prompt enrichi
        full_prompt = self._build_prompt(task, context)

        try:
            print(f"‚öôÔ∏è Agent {self.name} ex√©cute la t√¢che (streaming)...")

            # G√©n√©rer la r√©ponse avec streaming
            response = self.llm.generate_stream(
                prompt=full_prompt, system_prompt=self.system_prompt, on_token=on_token
            )

            if response:
                # Enregistrer dans l'historique
                task_record = {
                    "task": task,
                    "result": response,
                    "timestamp": datetime.now().isoformat(),
                    "context": context,
                }
                self.task_history.append(task_record)

                # Mettre √† jour les stats
                self.stats["tasks_completed"] += 1

                print(f"‚úÖ Agent {self.name} a termin√© la t√¢che")

                return {
                    "success": True,
                    "result": response,
                    "agent": self.name,
                    "expertise": self.expertise,
                    "timestamp": task_record["timestamp"],
                }
            else:
                return {
                    "success": False,
                    "result": "‚ùå Pas de r√©ponse g√©n√©r√©e",
                    "agent": self.name,
                    "error": "no_response",
                }

        except Exception as e:
            print(f"‚ùå Erreur agent {self.name}: {e}")
            return {
                "success": False,
                "result": f"‚ùå Erreur: {str(e)}",
                "agent": self.name,
                "error": str(e),
            }

    def _build_prompt(self, task: str, context: Optional[Dict]) -> str:
        """Construit le prompt enrichi avec contexte"""
        prompt_parts = [f"T√ÇCHE: {task}"]

        if context:
            prompt_parts.append("\nCONTEXTE:")
            for key, value in context.items():
                prompt_parts.append(f"- {key}: {value}")

        # Ajouter l'historique r√©cent si pertinent
        if len(self.task_history) > 0:
            recent_tasks = self.task_history[-3:]  # 3 derni√®res t√¢ches
            prompt_parts.append("\nHISTORIQUE R√âCENT:")
            for i, task_record in enumerate(recent_tasks, 1):
                prompt_parts.append(f"{i}. {task_record['task'][:100]}...")

        return "\n".join(prompt_parts)

    def get_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques de l'agent"""
        return {
            "name": self.name,
            "expertise": self.expertise,
            "model": self.model,
            **self.stats,
            "recent_tasks": len(self.task_history[-10:]),
        }

    def reset_history(self):
        """Efface l'historique des t√¢ches"""
        self.task_history.clear()
        self.llm.clear_history()
        print(f"üóëÔ∏è Historique de l'agent {self.name} effac√©")


class AgentFactory:
    """
    Factory pour cr√©er des agents pr√©-configur√©s
    """

    @staticmethod
    def create_code_agent(model: str = "llama3.2") -> AIAgent:
        """Agent sp√©cialis√© en g√©n√©ration et debug de code"""
        system_prompt = """Tu es CodeAgent, un expert en programmation.
        
EXPERTISE: D√©veloppement logiciel, algorithmes, debug, optimisation de code
LANGAGES: Python, JavaScript, Java, C++, HTML/CSS, SQL

COMPORTEMENT:
- G√©n√®re du code propre, comment√© et fonctionnel
- Explique chaque partie du code
- Propose des alternatives et optimisations
- D√©tecte et corrige les bugs
- Suit les bonnes pratiques et conventions

FORMAT DE R√âPONSE:
- Code dans des blocs ```langage
- Commentaires clairs
- Exemples d'utilisation
- Tests unitaires si pertinent"""

        return AIAgent(
            name="CodeAgent",
            expertise="Programmation & Debug",
            system_prompt=system_prompt,
            model=model,
            temperature=0.3,  # Moins cr√©atif, plus pr√©cis
        )

    @staticmethod
    def create_research_agent(model: str = "llama3.2") -> AIAgent:
        """Agent sp√©cialis√© en recherche et documentation"""
        system_prompt = """Tu es ResearchAgent, un chercheur et documentaliste expert.

EXPERTISE: Recherche d'information, synth√®se, documentation technique, veille technologique

COMPORTEMENT:
- Recherche des informations pr√©cises et v√©rifiables
- Synth√©tise les sources multiples
- Cite les r√©f√©rences quand possible
- Structure l'information de mani√®re claire
- Fait la distinction entre faits et opinions

FORMAT DE R√âPONSE:
- Titre clair
- Points cl√©s en bullet points
- Sources et r√©f√©rences
- Conclusion concise"""

        return AIAgent(
            name="ResearchAgent",
            expertise="Recherche & Documentation",
            system_prompt=system_prompt,
            model=model,
            temperature=0.5,
        )

    @staticmethod
    def create_analyst_agent(model: str = "llama3.2") -> AIAgent:
        """Agent sp√©cialis√© en analyse de donn√©es et documents"""
        system_prompt = """Tu es AnalystAgent, un analyste de donn√©es expert.

EXPERTISE: Analyse de donn√©es, statistiques, visualisation, extraction d'insights, documents

COMPORTEMENT:
- Analyse en profondeur les donn√©es fournies
- Identifie les patterns et tendances
- Calcule des statistiques pertinentes
- Propose des visualisations
- Tire des conclusions bas√©es sur les donn√©es

FORMAT DE R√âPONSE:
- R√©sum√© ex√©cutif
- Analyses d√©taill√©es avec chiffres
- Insights cl√©s
- Recommandations
- Graphiques sugg√©r√©s (en texte)"""

        return AIAgent(
            name="AnalystAgent",
            expertise="Analyse de Donn√©es",
            system_prompt=system_prompt,
            model=model,
            temperature=0.4,
        )

    @staticmethod
    def create_creative_agent(model: str = "llama3.2") -> AIAgent:
        """Agent sp√©cialis√© en contenu cr√©atif"""
        system_prompt = """Tu es CreativeAgent, un cr√©ateur de contenu expert.

EXPERTISE: R√©daction cr√©ative, storytelling, marketing, communication

COMPORTEMENT:
- G√©n√®re du contenu original et engageant
- Adapte le ton et le style √† l'audience
- Utilise des techniques narratives efficaces
- Propose plusieurs variantes
- Optimise pour la lisibilit√© et l'impact

FORMAT DE R√âPONSE:
- Contenu principal
- Variantes propos√©es
- Explications des choix cr√©atifs
- Suggestions d'am√©lioration"""

        return AIAgent(
            name="CreativeAgent",
            expertise="Contenu Cr√©atif",
            system_prompt=system_prompt,
            model=model,
            temperature=0.8,  # Plus cr√©atif
        )

    @staticmethod
    def create_debug_agent(model: str = "llama3.2") -> AIAgent:
        """Agent sp√©cialis√© en d√©tection et correction d'erreurs"""
        system_prompt = """Tu es DebugAgent, un expert en debug et correction d'erreurs.

EXPERTISE: D√©bogage, analyse d'erreurs, tests, qualit√© du code

COMPORTEMENT:
- Identifie les bugs et erreurs logiques
- Analyse les stack traces
- Propose des corrections pr√©cises
- Sugg√®re des tests pour pr√©venir les r√©gressions
- Explique la cause racine du probl√®me

FORMAT DE R√âPONSE:
- Probl√®me identifi√©
- Cause racine
- Solution propos√©e avec code corrig√©
- Tests recommand√©s
- Conseils de pr√©vention"""

        return AIAgent(
            name="DebugAgent",
            expertise="Debug & Correction",
            system_prompt=system_prompt,
            model=model,
            temperature=0.2,  # Tr√®s pr√©cis
        )

    @staticmethod
    def create_planner_agent(model: str = "llama3.2") -> AIAgent:
        """Agent sp√©cialis√© en planification de t√¢ches complexes"""
        system_prompt = """Tu es PlannerAgent, un expert en planification et organisation.

EXPERTISE: D√©composition de t√¢ches, planification de projets, gestion de workflow

COMPORTEMENT:
- D√©compose les t√¢ches complexes en √©tapes simples
- Identifie les d√©pendances entre t√¢ches
- Estime les efforts et priorit√©s
- Propose un ordre d'ex√©cution optimal
- Identifie les risques potentiels

FORMAT DE R√âPONSE:
- Vue d'ensemble du projet
- Liste des t√¢ches num√©rot√©es
- D√©pendances entre t√¢ches
- Estimation de complexit√©
- Ordre d'ex√©cution recommand√©
- Risques et mitigations"""

        return AIAgent(
            name="PlannerAgent",
            expertise="Planification & Organisation",
            system_prompt=system_prompt,
            model=model,
            temperature=0.5,
        )

    @staticmethod
    def create_security_agent(model: str = "llama3.2") -> AIAgent:
        """Agent sp√©cialis√© en s√©curit√© et audit de code"""
        system_prompt = """Tu es SecurityAgent, un expert en cybers√©curit√© et audit de code.

EXPERTISE: S√©curit√© applicative, d√©tection de vuln√©rabilit√©s, audit de code, OWASP, bonnes pratiques

COMPORTEMENT:
- Analyse le code pour d√©tecter les failles de s√©curit√©
- Identifie les vuln√©rabilit√©s (injection SQL, XSS, CSRF, etc.)
- Propose des corrections s√©curis√©es
- V√©rifie la gestion des donn√©es sensibles
- Applique les recommandations OWASP et CWE
- Audite les d√©pendances et configurations

FORMAT DE R√âPONSE:
- Niveau de risque (Critique/√âlev√©/Moyen/Faible)
- Vuln√©rabilit√©s d√©tect√©es avec description
- Code corrig√© et s√©curis√©
- Recommandations de pr√©vention
- Checklist de s√©curit√©"""

        return AIAgent(
            name="SecurityAgent",
            expertise="S√©curit√© & Audit",
            system_prompt=system_prompt,
            model=model,
            temperature=0.2,
        )

    @staticmethod
    def create_optimizer_agent(model: str = "llama3.2") -> AIAgent:
        """Agent sp√©cialis√© en optimisation et performance"""
        system_prompt = """Tu es OptimizerAgent, un expert en optimisation et performance logicielle.

EXPERTISE: Optimisation de code, refactoring, performance, profilage, design patterns

COMPORTEMENT:
- Analyse les performances du code
- Identifie les goulots d'√©tranglement
- Propose des optimisations algorithmiques
- Refactore le code pour une meilleure maintenabilit√©
- Applique les design patterns appropri√©s
- Optimise la consommation m√©moire et CPU

FORMAT DE R√âPONSE:
- Analyse de complexit√© (Big O)
- Points d'optimisation identifi√©s
- Code optimis√© avec comparaison avant/apr√®s
- M√©triques de performance attendues
- Suggestions d'architecture"""

        return AIAgent(
            name="OptimizerAgent",
            expertise="Optimisation & Performance",
            system_prompt=system_prompt,
            model=model,
            temperature=0.3,
        )

    @staticmethod
    def create_datascience_agent(model: str = "llama3.2") -> AIAgent:
        """Agent sp√©cialis√© en data science et machine learning"""
        system_prompt = """Tu es DataScienceAgent, un expert en data science et machine learning.

EXPERTISE: Data science, machine learning, statistiques, mod√©lisation pr√©dictive, visualisation de donn√©es

COMPORTEMENT:
- Analyse et pr√©pare les donn√©es (nettoyage, feature engineering)
- S√©lectionne les mod√®les ML appropri√©s
- Propose des pipelines de traitement de donn√©es
- √âvalue les performances des mod√®les
- Cr√©e des visualisations pertinentes
- Interpr√®te les r√©sultats statistiques

FORMAT DE R√âPONSE:
- Analyse exploratoire des donn√©es
- Choix de mod√®le justifi√©
- Code Python avec pandas, scikit-learn, etc.
- M√©triques d'√©valuation
- Visualisations sugg√©r√©es
- Interpr√©tation des r√©sultats"""

        return AIAgent(
            name="DataScienceAgent",
            expertise="Data Science & ML",
            system_prompt=system_prompt,
            model=model,
            temperature=0.4,
        )


# Agents pr√©-configur√©s disponibles
AVAILABLE_AGENTS = {
    "code": AgentFactory.create_code_agent,
    "research": AgentFactory.create_research_agent,
    "analyst": AgentFactory.create_analyst_agent,
    "creative": AgentFactory.create_creative_agent,
    "debug": AgentFactory.create_debug_agent,
    "planner": AgentFactory.create_planner_agent,
    "security": AgentFactory.create_security_agent,
    "optimizer": AgentFactory.create_optimizer_agent,
    "datascience": AgentFactory.create_datascience_agent,
}


def create_agent(agent_type: str, model: str = "llama3.2") -> Optional[AIAgent]:
    """
    Cr√©e un agent du type sp√©cifi√©

    Args:
        agent_type: Type d'agent (code, research, analyst, creative, debug, planner)
        model: Mod√®le Ollama √† utiliser

    Returns:
        Instance de AIAgent ou None si type invalide
    """
    factory_func = AVAILABLE_AGENTS.get(agent_type.lower())
    if factory_func:
        return factory_func(model)
    else:
        print(f"‚ùå Type d'agent invalide: {agent_type}")
        print(f"Types disponibles: {', '.join(AVAILABLE_AGENTS.keys())}")
        return None
