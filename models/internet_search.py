"""
Module de recherche internet avec extraction de r√©ponse directe
Version corrig√©e sans doublons ni erreurs de syntaxe
"""

import concurrent.futures
import re
import statistics
import string
import time
import traceback
from collections import Counter
from typing import Any, Dict, List, Optional
from urllib.parse import quote

import requests
from rapidfuzz import fuzz
from bs4 import BeautifulSoup

# Import cloudscraper pour contourner les protections anti-bot
try:
    import cloudscraper

    CLOUDSCRAPER_AVAILABLE = True
except ImportError:
    CLOUDSCRAPER_AVAILABLE = False
    print("‚ö†Ô∏è cloudscraper non disponible, utilisation de requests standard")


class EnhancedInternetSearchEngine:
    """Moteur de recherche internet avec extraction de r√©ponse directe"""

    def __init__(self, llm=None):
        """
        Initialise le moteur de recherche

        Args:
            llm: Instance de LocalLLM pour l'analyse intelligente du contenu (optionnel)
        """
        self.search_apis = {
            "duckduckgo": "https://api.duckduckgo.com/",
            "bing": "https://api.bing.microsoft.com/v7.0/search",
            "google_custom": "https://www.googleapis.com/customsearch/v1",
        }

        # Configuration
        self.max_results = 8
        self.max_content_length = 3000
        self.timeout = 15  # Augment√© √† 15 secondes

        # LLM pour analyse intelligente (optionnel)
        self.llm = llm

        # User-agents multiples pour √©viter la d√©tection
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
        ]

        self.current_user_agent_index = 0
        self.user_agent = self.user_agents[0]

        # Cache des r√©sultats r√©cents
        self.search_cache = {}
        self.cache_duration = 3600

        # Initialiser cloudscraper si disponible
        if CLOUDSCRAPER_AVAILABLE:
            self.scraper = cloudscraper.create_scraper(
                browser={"browser": "chrome", "platform": "windows", "mobile": False}
            )
            print("‚úÖ Cloudscraper initialis√© (contournement anti-bot activ√©)")
        else:
            self.scraper = None

        # Patterns pour l'extraction de r√©ponses directes
        self.answer_patterns = self._init_answer_patterns()

    def _get_next_user_agent(self) -> str:
        """Obtient le prochain user-agent pour √©viter la d√©tection"""
        self.current_user_agent_index = (self.current_user_agent_index + 1) % len(
            self.user_agents
        )
        self.user_agent = self.user_agents[self.current_user_agent_index]
        return self.user_agent

    def _correct_common_typos(self, query: str) -> str:
        """Corrige les fautes d'orthographe via dictionnaire statique + fuzzy matching g√©n√©rique."""
        # Dictionnaire statique (cas certains)
        corrections = {
            # Monuments et lieux c√©l√®bres
            "effeil": "eiffel",
            "eifeil": "eiffel",
            "effell": "eiffel",
            "eifel": "eiffel",
            # Villes
            "pariss": "paris",
            "paaris": "paris",
            # Mesures courantes
            "tail": "taille",
            "taile": "taille",
            "mesur": "mesure",
            # Mots courants mal orthographi√©s
            "populatoin": "population",
            "populaton": "population",
            "hauteure": "hauteur",
        }

        words = query.split()
        corrected_words = []
        has_correction = False

        for word in words:
            clean_word = word.lower().strip(".,;:!?")
            if clean_word in corrections:
                corrected_word = corrections[clean_word]
                suffix = word[len(clean_word) :]
                corrected_words.append(corrected_word + suffix)
                has_correction = True
                print(f"‚úèÔ∏è Correction: '{word}' ‚Üí '{corrected_word}'")
            else:
                corrected_words.append(word)

        corrected_query = " ".join(corrected_words)
        if has_correction:
            print(f"‚úèÔ∏è Requ√™te corrig√©e: '{query}' ‚Üí '{corrected_query}'")
        return corrected_query

    def _spellcheck_with_wikipedia(self, query: str) -> str:
        """
        Utilise l'API Wikipedia pour corriger l'orthographe d'une requ√™te.
        Envoie la requ√™te √† l'API opensearch de Wikipedia pour obtenir des suggestions.
        Retourne la requ√™te corrig√©e si Wikipedia a trouv√© mieux, sinon retourne la requ√™te originale.
        """
        try:
            api_url = "https://fr.wikipedia.org/w/api.php"
            params = {
                "action": "opensearch",
                "search": query,
                "limit": 1,
                "format": "json",
            }
            headers = {"User-Agent": self.user_agent}
            resp = requests.get(api_url, params=params, headers=headers, timeout=6)
            data = resp.json()
            # Format: [query, [titles], [descriptions], [urls]]
            if len(data) > 1 and data[1]:
                suggestion = data[1][0]
                if suggestion and suggestion.lower() != query.lower():
                    print(f"üåê Wikipedia spellcheck: '{query}' ‚Üí '{suggestion}'")
                    return suggestion
        except Exception:
            pass

        # Fallback: essayer aussi l'API spellcheck de Wikipedia
        try:
            api_url = "https://fr.wikipedia.org/w/api.php"
            params = {
                "action": "query",
                "list": "search",
                "srsearch": query,
                "srinfo": "suggestion",
                "srprop": "",
                "format": "json",
            }
            headers = {"User-Agent": self.user_agent}
            resp = requests.get(api_url, params=params, headers=headers, timeout=6)
            data = resp.json()
            suggestion = (
                data.get("query", {}).get("searchinfo", {}).get("suggestion", "")
            )
            if suggestion and suggestion.lower() != query.lower():
                print(f"üåê Wikipedia spellcheck suggestion: '{query}' ‚Üí '{suggestion}'")
                return suggestion
        except Exception:
            pass

        return query

    def _init_answer_patterns(self) -> Dict[str, List[str]]:
        """Initialise les patterns pour extraire des r√©ponses directes"""
        return {
            "taille": [
                r"(?:mesure|fait|taille|hauteur)(?:\s+de)?\s+([\d,]+\.?\d*)\s*(m√®tres?|m|km|centim√®tres?|cm)",
                r"([\d,]+\.?\d*)\s*(m√®tres?|m|km|centim√®tres?|cm)(?:\s+de|d')?\s+(?:haut|hauteur|taille)",
                r"(?:est|fait)(?:\s+de)?\s+([\d,]+\.?\d*)\s*(m√®tres?|m|km)",
            ],
            "poids": [
                r"(?:p√®se|poids)(?:\s+de)?\s+([\d,]+\.?\d*)\s*(kilogrammes?|kg|tonnes?|grammes?|g)",
                r"([\d,]+\.?\d*)\s*(kilogrammes?|kg|tonnes?|grammes?|g)(?:\s+de|d')?\s+(?:poids|lourd)",
            ],
            "population": [
                r"(?:population|habitants?)(?:\s+de)?\s+([\d\s,]+\.?\d*)",
                r"([\d\s,]+\.?\d*)(?:\s+d')?\s*habitants?",
                r"compte\s+([\d\s,]+\.?\d*)(?:\s+d')?\s*habitants?",
            ],
            "date": [
                r"(?:en|depuis|dans|cr√©√©|fond√©|construit|n√©)\s+(\d{4})",
                r"(\d{1,2})\s+(?:janvier|f√©vrier|mars|avril|mai|juin|juillet|ao√ªt|septembre|octobre|novembre|d√©cembre)\s+(\d{4})",
                r"(\d{4})(?:\s*-\s*\d{4})?",
            ],
            "prix": [
                r"(?:co√ªte|prix|vaut)\s+([\d,]+\.?\d*)\s*(euros?|dollars?|\$|‚Ç¨)",
                r"([\d,]+\.?\d*)\s*(euros?|dollars?|\$|‚Ç¨)(?:\s+pour|de)?",
            ],
            "definition": [
                r"(?:est|sont)\s+([^.!?]{20,100})",
                r"([^.!?]{20,100})(?:\s+est|\s+sont)",
                r"(?:c'est|ce sont)\s+([^.!?]{20,100})",
            ],
        }

    def _is_weather_query(self, query: str) -> bool:
        """D√©tecte si la question concerne la m√©t√©o"""
        weather_keywords = [
            "m√©t√©o",
            "meteo",
            "weather",
            "temp√©rature",
            "temperature",
            "temps",
            "pluie",
            "soleil",
            "neige",
            "vent",
            "climat",
            "chaud",
            "froid",
            "degr√©s",
            "celsius",
            "forecast",
            "pr√©visions",
            "previsions",
        ]
        query_lower = query.lower()
        return any(keyword in query_lower for keyword in weather_keywords)

    def _handle_weather_query(self, query: str) -> str:
        """
        G√®re les questions m√©t√©o en utilisant wttr.in (service gratuit)
        Fournit la m√©t√©o temps r√©el sans cl√© API
        """
        # Extraire la ville si possible
        city = self._extract_city_from_query(query)

        if not city:
            return """üå§Ô∏è **Recherche m√©t√©o mondiale**

‚ùå Je n'ai pas pu identifier la ville dans votre question.

üí° **Exemples de questions valides :**
   - "Quelle est la m√©t√©o √† Tokyo ?"
   - "Quel temps fait-il √† New York aujourd'hui ?"
   - "Temp√©rature √† Londres ?"
   - "M√©t√©o S√£o Paulo"
   - "Weather in Sydney ?"

üåç **Villes support√©es :** Toutes les villes du monde !
   - Europe : Paris, Londres, Berlin, Madrid, Rome...
   - Am√©rique : New York, Los Angeles, Toronto, S√£o Paulo...
   - Asie : Tokyo, P√©kin, Bangkok, Mumbai, S√©oul...
   - Oc√©anie : Sydney, Melbourne, Auckland...
   - Afrique : Le Caire, Casablanca, Johannesburg...

üåê **Sites m√©t√©o recommand√©s :**
   - [wttr.in](https://wttr.in/) - M√©t√©o mondiale gratuite
   - [M√©t√©o-France](https://meteofrance.com/) - Service officiel fran√ßais"""

        # Obtenir la m√©t√©o via wttr.in
        try:
            print(f"üå§Ô∏è R√©cup√©ration m√©t√©o pour: {city.title()}")
            weather_data = self._get_wttr_weather(city)
            return weather_data
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur m√©t√©o wttr.in: {e}")
            return f"""üå§Ô∏è **M√©t√©o pour {city.title()}**

‚ö†Ô∏è **Impossible de r√©cup√©rer la m√©t√©o en temps r√©el** (erreur: {str(e)[:100]})

üí° **Solutions alternatives :**

1. **Consultez directement :**
   - üåê [M√©t√©o {city.title()} sur wttr.in](https://wttr.in/{city})
   - üåê [Weather.com](https://weather.com/)

2. **V√©rifiez l'orthographe** de la ville (en anglais si possible)

3. **R√©essayez dans quelques instants** (probl√®me de connexion temporaire)"""

    def _get_wttr_weather(self, city: str) -> str:
        """
        R√©cup√®re la m√©t√©o via wttr.in (service gratuit, pas de cl√© API)
        Format fran√ßais, donn√©es temps r√©el
        """
        # wttr.in API endpoints
        # Format: wttr.in/VILLE?format=...
        # ?0 = conditions actuelles
        # ?1 = temp√©rature
        # ?2 = vent
        # ?3 = humidit√©
        # ?4 = pr√©cipitations
        # ?format=j1 = JSON complet

        base_url = f"https://wttr.in/{quote(city)}"

        headers = {
            "User-Agent": self.user_agent,
            "Accept-Language": "fr-FR,fr;q=0.9,en;q=0.8",
        }

        try:
            # R√©cup√©rer la m√©t√©o en format texte (plus simple √† parser)
            # Format: wttr.in/VILLE?format="%C+%t+%h+%w+%p"
            # %C = Condition (Ensoleill√©, Nuageux, etc.)
            # %t = Temp√©rature
            # %h = Humidit√©
            # %w = Vent
            # %p = Pr√©cipitations

            params = {
                "format": "%C|%t|%h|%w|%p|%m",  # Pipe-separated pour parsing
                "lang": "fr",  # Texte en fran√ßais
            }

            response = requests.get(
                base_url, params=params, headers=headers, timeout=self.timeout
            )
            response.raise_for_status()

            # Parser la r√©ponse (format: "Condition|Temp|Humidity|Wind|Precip|Moon")
            weather_parts = response.text.strip().split("|")

            if len(weather_parts) >= 5:
                condition = weather_parts[0].strip()
                temperature = weather_parts[1].strip()
                humidity = weather_parts[2].strip()
                wind = weather_parts[3].strip()
                precipitation = weather_parts[4].strip()

                # R√©cup√©rer aussi les pr√©visions sur 3 jours
                try:
                    forecast_params = {"format": "3", "lang": "fr"}
                    forecast_response = requests.get(
                        base_url, params=forecast_params, headers=headers, timeout=10
                    )
                    forecast_text = forecast_response.text.strip()
                except Exception:
                    forecast_text = "Pr√©visions non disponibles"

                # Formater la r√©ponse
                weather_summary = f"""üå§Ô∏è**M√©t√©o √† {city.title()}** (Temps r√©el)

**Conditions actuelles :**
‚òÅÔ∏è       **{condition}**
üå°Ô∏è Temp√©rature : **{temperature}**
üíß       Humidit√© : **{humidity}**
üí®       Vent : **{wind}**
üåßÔ∏è Pr√©cipitations : **{precipitation}**

**Pr√©visions sur 3 jours :**
{forecast_text}

üìç **Source** : [wttr.in/{city}](https://wttr.in/{city})
‚è∞ Donn√©es temps r√©el mises √† jour automatiquement

üí° **Astuce** : Tapez `https://wttr.in/{city}` dans votre navigateur pour voir une m√©t√©o d√©taill√©e en ASCII art !"""

                return weather_summary
            else:
                raise ValueError("Format de r√©ponse inattendu de wttr.in")

        except requests.exceptions.Timeout as e:
            raise Exception(
                f"Timeout lors de la connexion √† wttr.in (> {self.timeout}s)"
            ) from e
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 404:
                raise Exception(
                    f"Ville '{city}' non trouv√©e. V√©rifiez l'orthographe."
                ) from e
            else:
                raise Exception(f"Erreur HTTP {e.response.status_code}") from e
        except Exception as e:
            raise Exception(f"Erreur inattendue: {str(e)}") from e

    def _extract_city_from_query(self, query: str) -> Optional[str]:
        """
        Extrait le nom de ville d'une requ√™te m√©t√©o.
        Supporte les villes du monde entier gr√¢ce √† wttr.in.
        """
        _query_lower = query.lower()

        # Mots √† exclure (mots communs qui ne sont pas des villes)
        stop_words = {
            "le",
            "la",
            "les",
            "un",
            "une",
            "des",
            "cette",
            "ce",
            "cet",
            "m√©t√©o",
            "meteo",
            "weather",
            "temp√©rature",
            "temperature",
            "temps",
            "pluie",
            "soleil",
            "neige",
            "vent",
            "climat",
            "chaud",
            "froid",
            "degr√©s",
            "celsius",
            "forecast",
            "pr√©visions",
            "previsions",
            "quel",
            "quelle",
            "quels",
            "quelles",
            "comment",
            "est",
            "fait",
            "aujourd'hui",
            "demain",
            "semaine",
            "maintenant",
            "actuelle",
            "il",
            "elle",
            "on",
            "nous",
            "vous",
            "ils",
            "elles",
            "dans",
            "sur",
            "pour",
            "avec",
            "sans",
            "chez",
        }

        # Pr√©positions √† supprimer du d√©but du nom de ville
        prepositions_to_remove = {
            "au",
            "aux",
            "en",
            "√†",
            "a",
            "de",
            "du",
            "des",
            "le",
            "la",
            "les",
            "l",
        }

        # Pattern pour extraire les noms de villes avec pr√©positions
        # Supporte les villes multi-mots comme "New York", "Los Angeles", "S√£o Paulo"
        patterns = [
            # "au N√©pal", "aux √âtats-Unis", "en France"
            r"\b(?:au|aux|en)\s+([A-Za-z√Ä-√ø][A-Za-z√Ä-√ø\-\s']+?)(?:\s*\?|$|\s+(?:aujourd|demain|cette|il|fait|quel))",
            # "√† Paris", "√† New York", "√† S√£o Paulo"
            r"\b√†\s+([A-Za-z√Ä-√ø][A-Za-z√Ä-√ø\-\s']+?)(?:\s*\?|$|\s+(?:aujourd|demain|cette|il|fait|quel))",
            # "de Paris", "de Tokyo"
            r"\bde\s+([A-Za-z√Ä-√ø][A-Za-z√Ä-√ø\-\s']+?)(?:\s*\?|$|\s+(?:aujourd|demain|cette|il|fait|quel))",
            # "pour Paris", "pour Londres"
            r"\bpour\s+([A-Za-z√Ä-√ø][A-Za-z√Ä-√ø\-\s']+?)(?:\s*\?|$|\s+(?:aujourd|demain|cette|il|fait|quel))",
            # "sur Paris"
            r"\bsur\s+([A-Za-z√Ä-√ø][A-Za-z√Ä-√ø\-\s']+?)(?:\s*\?|$|\s+(?:aujourd|demain|cette|il|fait|quel))",
            # "m√©t√©o Tokyo", "weather London", "m√©t√©o au Japon"
            r"(?:m√©t√©o|meteo|weather)\s+(?:au|aux|en|√†|du|de la|de)?\s*([A-Za-z√Ä-√ø][A-Za-z√Ä-√ø\-\s']+?)(?:\s*\?|$)",
            # "Tokyo m√©t√©o", "Paris weather"
            r"([A-Za-z√Ä-√ø][A-Za-z√Ä-√ø\-\s']+?)\s+(?:m√©t√©o|meteo|weather)(?:\s*\?|$)",
            # Pattern simple pour villes en fin de phrase: "... √† Tokyo?", "... au Japon?"
            r"\b(?:√†|au|aux|en)\s+([A-Za-z√Ä-√ø][A-Za-z√Ä-√ø\-\s']+?)\s*\?",
        ]

        def clean_city_name(city: str) -> str:
            """Nettoie le nom de la ville en supprimant les pr√©positions au d√©but"""
            city = city.strip()
            # Supprimer les pr√©positions au d√©but
            words = city.split()
            while words and words[0].lower() in prepositions_to_remove:
                words.pop(0)
            return " ".join(words).strip()

        for pattern in patterns:
            match = re.search(pattern, query, re.IGNORECASE)
            if match:
                potential_city = match.group(1).strip()
                # Nettoyer les espaces en trop
                potential_city = " ".join(potential_city.split())
                # Nettoyer les pr√©positions au d√©but
                potential_city = clean_city_name(potential_city)
                # V√©rifier que ce n'est pas un mot commun
                if (
                    potential_city
                    and potential_city.lower() not in stop_words
                    and len(potential_city) >= 2
                ):
                    print(f"üåç Ville d√©tect√©e: {potential_city}")
                    return potential_city

        # Derni√®re tentative: chercher un mot capitalis√© qui pourrait √™tre une ville
        # Pattern pour les noms propres (commence par majuscule)
        capital_pattern = r"\b([A-Z][a-z√Ä-√ø]+(?:\s+[A-Z][a-z√Ä-√ø]+)*)\b"
        matches = re.findall(capital_pattern, query)
        for potential_city in matches:
            cleaned_city = clean_city_name(potential_city)
            if (
                cleaned_city
                and cleaned_city.lower() not in stop_words
                and len(cleaned_city) >= 2
            ):
                print(f"üåç Ville d√©tect√©e (nom propre): {cleaned_city}")
                return cleaned_city

        return None

    def search_and_summarize(self, query: str) -> str:
        """Recherche sur internet et extrait la r√©ponse directe"""
        try:
            print(f"üîç Recherche internet pour: '{query}'")

            # D√©tection des questions m√©t√©o (n√©cessitent des donn√©es temps r√©el)
            if self._is_weather_query(query):
                return self._handle_weather_query(query)

            # NOUVEAU : Corriger les fautes d'orthographe courantes
            corrected_query = self._correct_common_typos(query)

            # V√©rifier le cache (avec la requ√™te corrig√©e)
            cache_key = self._generate_cache_key(corrected_query)
            if self._is_cache_valid(cache_key):
                print("üìã R√©sultats trouv√©s en cache")
                return self.search_cache[cache_key]["summary"]

            # Effectuer la recherche avec la requ√™te corrig√©e
            search_results = self._perform_search(corrected_query)

            if not search_results:
                return f"‚ùå D√©sol√©, je n'ai pas pu trouver d'informations sur '{query}'. V√©rifiez votre connexion internet."

            # Extraire le contenu des pages
            page_contents = self._extract_page_contents(search_results)

            # Extraire la r√©ponse directe (avec la requ√™te originale pour l'affichage)
            direct_answer = self._extract_direct_answer(corrected_query, page_contents)

            # G√©n√©rer le r√©sum√©
            summary = self._generate_answer_focused_summary(
                query, direct_answer, page_contents
            )

            # Mettre en cache
            self._cache_results(cache_key, summary, search_results)

            return summary

        except Exception as e:
            print(f"‚ùå Erreur lors de la recherche: {str(e)}")
            return f"D√©sol√©, une erreur s'est produite lors de la recherche internet : {str(e)}"

    def search_best_source_context(self, query: str) -> str:
        """
        Recherche internet optimis√©e pour un flux single-pass LLM:
        - S√©lectionne la source la plus pertinente
        - Retourne un contexte structur√© + sources
        - N'appelle PAS le LLM ici
        """
        try:
            print(f"üîç Recherche internet (mode source unique) pour: '{query}'")

            if self._is_weather_query(query):
                return self._handle_weather_query(query)

            # 1) Correction dictionnaire statique
            corrected_query = self._correct_common_typos(query)
            # 2) Correction via Wikipedia si la requ√™te semble contenir des fautes
            #    (heuristique: mot inconnu d√©tect√© = mot qui n'existe pas en minuscules sans accent)
            corrected_query = self._spellcheck_with_wikipedia(corrected_query)
            search_results = self._perform_search(corrected_query)

            if not search_results:
                return f"‚ùå D√©sol√©, je n'ai pas pu trouver d'informations sur '{query}'. V√©rifiez votre connexion internet."

            page_contents = self._extract_page_contents(search_results)
            if not page_contents:
                return f"‚ùå D√©sol√©, aucune source exploitable trouv√©e sur '{query}'."

            best_source = self._select_best_source_for_query(
                corrected_query, page_contents
            )
            if not best_source:
                return f"‚ùå D√©sol√©, aucune source pertinente trouv√©e sur '{query}'."

            title = best_source.get("title", "Source principale")
            url = best_source.get("url", "")
            content = best_source.get("full_content") or best_source.get("snippet", "")

            if len(content) > 12000:
                content = content[:12000] + "\n\n[...contenu tronqu√©...]"

            print(f"üéØ Source principale retenue: {title}")

            return (
                f"**Source principale s√©lectionn√©e**\n"
                f"Titre: {title}\n"
                f"URL: {url}\n\n"
                f"**Contenu source**\n{content}\n\n"
                f"üîó **Sources**\n"
                f"‚Ä¢ [{title}]({url})"
            )

        except Exception as e:
            print(f"‚ùå Erreur mode source unique: {str(e)}")
            return f"D√©sol√©, une erreur s'est produite lors de la recherche internet : {str(e)}"

    def _select_best_source_for_query(
        self, query: str, page_contents: List[Dict[str, Any]]
    ) -> Optional[Dict[str, Any]]:
        """S√©lectionne la source la plus pertinente pour la requ√™te (sans LLM, scoring g√©n√©rique)."""
        if not page_contents:
            return None

        query_lower = query.lower().strip()

        stopwords = {
            "le",
            "la",
            "les",
            "de",
            "des",
            "du",
            "un",
            "une",
            "et",
            "ou",
            "en",
            "sur",
            "dans",
            "pour",
            "avec",
            "sans",
            "par",
            "que",
            "qui",
            "quoi",
            "comment",
            "cherche",
            "recherche",
            "trouve",
            "internet",
            "web",
            "google",
        }

        query_words = [
            word
            for word in re.findall(r"\w+", query_lower, flags=re.UNICODE)
            if len(word) > 2 and word not in stopwords
        ]
        query_terms = set(query_words)

        if not query_terms:
            return page_contents[0]

        wants_recent = any(
            word in query_lower
            for word in [
                "dernier",
                "derni√®re",
                "derniers",
                "derni√®res",
                "r√©cent",
                "r√©cente",
                "r√©cents",
                "r√©centes",
            ]
        )

        query_years = {
            int(year) for year in re.findall(r"\b(19\d{2}|20\d{2})\b", query_lower)
        }

        all_texts = []
        for page in page_contents:
            title = (page.get("title") or "").lower()
            snippet = (page.get("snippet") or "").lower()
            all_texts.extend(
                set(re.findall(r"\w+", f"{title} {snippet}", flags=re.UNICODE))
            )
        token_frequency = Counter(token for token in all_texts if len(token) > 2)

        scored = []
        for page in page_contents:
            title = (page.get("title") or "").lower()
            snippet = (page.get("snippet") or "").lower()
            content = (page.get("full_content") or snippet).lower()
            combined_short = f"{title} {snippet}".strip()

            title_terms = {
                word
                for word in re.findall(r"\w+", title, flags=re.UNICODE)
                if len(word) > 2 and word not in stopwords
            }
            short_terms = {
                word
                for word in re.findall(r"\w+", combined_short, flags=re.UNICODE)
                if len(word) > 2 and word not in stopwords
            }

            title_overlap = query_terms.intersection(title_terms)
            short_overlap = query_terms.intersection(short_terms)

            score = 0.0

            # Couverture des termes de la requ√™te
            coverage_ratio = len(short_overlap) / max(len(query_terms), 1)
            title_coverage_ratio = len(title_overlap) / max(len(query_terms), 1)
            score += coverage_ratio * 420
            score += title_coverage_ratio * 360

            # Favoriser les termes rares parmi les r√©sultats (discriminants)
            rarity_bonus = 0.0
            for token in title_overlap:
                rarity_bonus += 1.0 / max(token_frequency.get(token, 1), 1)
            score += rarity_bonus * 220

            # Similarit√© globale query <-> titre
            score += fuzz.partial_ratio(query_lower, title) * 1.4

            # Gestion temporelle g√©n√©rique
            page_years = {
                int(y) for y in re.findall(r"\b(19\d{2}|20\d{2})\b", combined_short)
            }
            if query_years:
                if page_years.intersection(query_years):
                    score += 220
                elif page_years:
                    score -= 140
            elif wants_recent and page_years:
                newest_year = max(page_years)
                score += max(0, (newest_year - 2018) * 18)

            # P√©nalit√© forte pour contenus trop courts (pages vides/navigation)
            content_len = len(content)
            if content_len < 400:
                score -= 350
            elif content_len < 800:
                score -= 80

            # Bonus fort proportionnel √† la richesse du contenu
            score += min(content_len / 200, 180)

            scored.append((score, page))

        scored.sort(key=lambda item: item[0], reverse=True)
        best_score, best_page = scored[0]
        print(f"üèÜ Meilleure source score={best_score}: {best_page.get('title', '')}")
        return best_page

    def _extract_direct_answer(
        self, query: str, page_contents: List[Dict[str, Any]]
    ) -> Optional[str]:
        """Extrait la r√©ponse directe √† partir des contenus des pages"""
        print(f"üéØ Extraction de r√©ponse directe pour: '{query}'")

        # Si LLM disponible, utiliser l'analyse intelligente
        if self.llm and self.llm.is_ollama_available:
            print("üß† Utilisation d'Ollama pour analyser le contenu web")
            return self._extract_with_llm_analysis(query, page_contents)

        # Sinon, utiliser l'extraction traditionnelle
        print("üìä Utilisation de l'extraction traditionnelle (pas de LLM)")

        # Analyser le type de question
        question_type = self._analyze_question_type(query)
        print(f"üîç Type de question d√©tect√©: {question_type}")

        # Extraire toutes les phrases candidates
        candidate_sentences = self._extract_candidate_sentences(page_contents, query)
        print(f"üìù {len(candidate_sentences)} phrases candidates trouv√©es")

        # Utiliser diff√©rentes strat√©gies selon le type de question
        if question_type == "factual":
            return self._extract_factual_answer(query, candidate_sentences)
        elif question_type == "measurement":
            return self._extract_measurement_answer(candidate_sentences, query)
        elif question_type == "definition":
            return self._extract_definition_answer(candidate_sentences)
        elif question_type == "date":
            return self._extract_date_answer(candidate_sentences)
        else:
            return self._extract_general_answer(query, candidate_sentences)

    def _extract_with_llm_analysis(
        self, query: str, page_contents: List[Dict[str, Any]]
    ) -> Optional[str]:
        """
        Utilise Ollama pour analyser intelligemment le contenu web et extraire les informations pertinentes

        Cette approche est g√©n√©rique et s'adapte automatiquement au type de question
        """
        if not page_contents:
            return None

        # Combiner le contenu de toutes les pages
        all_content = []
        sources = []

        for page in page_contents:
            title = page.get("title", "")
            snippet = page.get("snippet", "")
            full_content = page.get("full_content", "")
            url = page.get("url", "")

            content_parts = []
            if snippet:
                content_parts.append(f"Extrait de recherche (tr√®s r√©cent): {snippet}")
            if full_content and len(full_content) > 50:
                content_parts.append(f"Contenu de la page: {full_content[:3000]}")

            content = "\n".join(content_parts)

            if content:
                all_content.append(f"Source: {title}\n{content}")
                sources.append({"title": title, "url": url})

        if not all_content:
            return None

        # Limiter le nombre total de caract√®res pour ne pas surcharger Ollama
        combined_content = "\n\n---\n\n".join(all_content[:5])  # Max 5 sources

        # Limiter √† 15000 caract√®res total (environ 4000 tokens)
        if len(combined_content) > 15000:
            combined_content = combined_content[:15000] + "\n\n[...contenu tronqu√©...]"

        print(
            f"üìä Envoi de {len(combined_content)} caract√®res de contenu √† Ollama pour analyse"
        )

        # Demander √† Ollama d'extraire les informations pertinentes
        prompt = f"""Tu es un expert en extraction d'informations depuis du contenu web.

QUESTION DE L'UTILISATEUR: "{query}"

CONTENU WEB (provenant de plusieurs sources fiables):
{combined_content}

T√ÇCHE:
Analyse CE contenu et extrais UNIQUEMENT les informations qui r√©pondent directement √† la question.

R√àGLES STRICTES:
‚úì Extrais TOUTES les informations pertinentes (noms, chiffres, dates, d√©tails, contexte)
‚úì Pour les questions sur des prix, cours de bourse ou donn√©es en temps r√©el, donne la priorit√© absolue aux "Extraits de recherche" qui sont souvent plus r√©cents que le "Contenu de la page".
‚úì Si ce sont des r√©sultats chiffr√©s (√©lections, comp√©titions, scores): donne TOUS les chiffres importants
‚úì Garde les noms propres exacts (partis, √©quipes, personnes, lieux)
‚úì Organise clairement les informations (liste √† puces si pertinent)
‚úì Cite les chiffres pr√©cis trouv√©s dans le contenu
‚úì NE r√©ponds QUE si l'information est pr√©sente dans le contenu fourni
‚úì Si l'information n'est pas dans le contenu, dis "Information non trouv√©e dans les sources"
‚úì S√©pare les diff√©rentes informations par des sauts de ligne doubles (\\n\\n)

R√©ponds de mani√®re factuelle et structur√©e:"""

        try:
            # Sauvegarder l'historique pour ne pas le polluer
            saved_history = self.llm.conversation_history.copy()
            self.llm.conversation_history = []

            on_token_cb = getattr(self, "on_llm_token", None)
            if callable(on_token_cb):
                # L'appelant (AIEngine) a branch√© le callback de la GUI :
                # utiliser generate_stream pour envoyer chaque token
                # directement dans la bulle de r√©ponse en temps r√©el.
                collected: list = []

                def _collect_and_fwd(token):
                    collected.append(token)
                    on_token_cb(token)
                    return True

                self.llm.generate_stream(
                    prompt=prompt,
                    system_prompt="Tu extrais des informations factuelles depuis du contenu web. Tu es pr√©cis, complet et factuel.",
                    on_token=_collect_and_fwd,
                )
                extracted_info = "".join(collected).strip()
            else:
                # Mode non-streaming : appel bloquant classique
                extracted_info = self.llm.generate(
                    prompt=prompt,
                    system_prompt="Tu extrais des informations factuelles depuis du contenu web. Tu es pr√©cis, complet et factuel.",
                )

            # Restaurer l'historique
            self.llm.conversation_history = saved_history

            if extracted_info and len(extracted_info.strip()) > 20:
                print(
                    f"‚úÖ Ollama a extrait {len(extracted_info)} caract√®res d'informations pertinentes"
                )
                return extracted_info.strip()
            else:
                print("‚ö†Ô∏è Ollama n'a pas pu extraire d'informations pertinentes")
                return None

        except Exception as e:
            print(f"‚ùå Erreur lors de l'analyse LLM: {str(e)}")
            return None

    def _analyze_question_type(self, query: str) -> str:
        """Analyse le type de question pour adapter l'extraction"""
        query_lower = query.lower()

        if any(
            word in query_lower
            for word in [
                "taille",
                "hauteur",
                "mesure",
                "fait",
                "m√®tres",
                "long",
                "large",
            ]
        ):
            return "measurement"

        if any(
            word in query_lower
            for word in [
                "qu'est-ce",
                "c'est quoi",
                "qui est",
                "que signifie",
                "d√©finition",
            ]
        ):
            return "definition"

        if any(
            word in query_lower
            for word in ["quand", "date", "ann√©e", "cr√©√©", "fond√©", "n√©", "construit"]
        ):
            return "date"

        if any(
            word in query_lower
            for word in [
                "combien",
                "quel",
                "quelle",
                "prix",
                "co√ªt",
                "population",
                "nombre",
            ]
        ):
            return "factual"

        return "general"

    def _extract_candidate_sentences(
        self, page_contents: List[Dict[str, Any]], query: str
    ) -> List[Dict[str, Any]]:
        """Extrait les phrases candidates contenant potentiellement la r√©ponse"""
        candidates = []
        query_words = set(
            word.lower().strip(string.punctuation)
            for word in query.split()
            if len(word) > 2
        )

        # Extraire les noms propres potentiels (mots de >=4 lettres non-question)
        entity_words = set()
        for word in query.split():
            clean_word = word.strip("?.,!;:").lower()
            if len(clean_word) >= 4 and clean_word not in [
                "quel",
                "quelle",
                "comment",
                "taille",
                "hauteur",
                "fait",
                "mesure",
                "what",
                "which",
                "height",
                "size",
            ]:
                entity_words.add(clean_word)

        print(f"üéØ [FILTER] Entit√©(s) recherch√©e(s): {entity_words}")

        for page in page_contents:
            # Utiliser full_content en priorit√©, sinon snippet
            # (√©vite de traiter deux fois le m√™me contenu)
            text = page.get("full_content") or page.get("snippet", "")

            if text:
                # D√©couper en phrases
                sentences = re.split(r"(?<=[.!?])\s+", text)

                for sentence in sentences:
                    sentence = sentence.strip()
                    if len(sentence) < 10 or len(sentence) > 500:
                        continue

                    # Calculer la pertinence
                    sentence_words = set(
                        word.lower().strip(string.punctuation)
                        for word in sentence.split()
                    )
                    relevance_score = len(query_words.intersection(sentence_words))

                    # BONUS MAJEUR si la phrase contient les mots de l'entit√© recherch√©e
                    entity_matches = len(entity_words.intersection(sentence_words))
                    if entity_matches > 0:
                        relevance_score += entity_matches * 5  # Tr√®s fort bonus !
                        print(
                            f"  ‚úÖ Phrase avec entit√© '{entity_words}': {sentence[:80]}..."
                        )

                    # Bonus pour les phrases avec des nombres ou des faits pr√©cis
                    if re.search(r"\d+", sentence):
                        relevance_score += 2

                    # Bonus pour les d√©buts de phrase indicatifs
                    if any(
                        sentence.lower().startswith(start)
                        for start in [
                            "la",
                            "le",
                            "il",
                            "elle",
                            "c'est",
                            "ce sont",
                            "on trouve",
                            "situ√©",
                        ]
                    ):
                        relevance_score += 1

                    if relevance_score > 0:
                        candidates.append(
                            {
                                "sentence": sentence,
                                "relevance": relevance_score,
                                "source": page.get("title", "Source inconnue"),
                                "url": page.get("url", ""),
                            }
                        )

        # Trier par pertinence et retourner les 100 meilleures (augment√© de 20)
        # Cela permet d'avoir beaucoup plus de contexte pour les questions complexes
        candidates.sort(key=lambda x: x["relevance"], reverse=True)
        print(
            f"üìä Total de {len(candidates)} candidates, retour des {min(len(candidates), 100)} meilleures"
        )
        return candidates[:100]  # Augment√© de 20 √† 100 pour capturer plus de d√©tails

    def _extract_factual_answer(
        self, query: str, candidates: List[Dict[str, Any]]
    ) -> Optional[str]:
        """Extrait une r√©ponse factuelle (nombre, mesure, etc.)"""
        query_lower = query.lower()

        # Patterns sp√©cifiques selon le type de fait recherch√©
        if "taille" in query_lower or "hauteur" in query_lower:
            pattern = r"([\d,]+\.?\d*)\s*(m√®tres?|m|km|centim√®tres?|cm)"
        elif "population" in query_lower or "habitant" in query_lower:
            pattern = r"([\d\s,]+\.?\d*)\s*(?:habitants?|personnes?)"
        elif "prix" in query_lower or "co√ªt" in query_lower:
            pattern = r"([\d,]+\.?\d*)\s*(euros?|dollars?|\$|‚Ç¨)"
        else:
            pattern = r"([\d,]+\.?\d*)"

        # Rechercher dans les candidates
        answer_counts = Counter()

        for candidate in candidates:
            sentence = candidate["sentence"]
            matches = re.findall(pattern, sentence, re.IGNORECASE)

            for match in matches:
                if isinstance(match, tuple):
                    answer_key = (
                        f"{match[0]} {match[1]}" if len(match) > 1 else match[0]
                    )
                else:
                    answer_key = match

                answer_counts[answer_key] += candidate["relevance"]

        if answer_counts:
            # Prendre la r√©ponse la plus consensuelle
            best_answer = answer_counts.most_common(1)[0][0]

            # Trouver TOUTES les phrases contenant cette r√©ponse pour plus de contexte
            relevant_sentences = []
            for candidate in candidates:
                if best_answer.split()[0] in candidate["sentence"]:
                    cleaned_sentence = self._universal_word_spacing_fix(
                        candidate["sentence"]
                    )
                    relevant_sentences.append(cleaned_sentence.strip())

            # Retourner les 20 premi√®res phrases pertinentes (ou moins s'il y en a moins)
            if relevant_sentences:
                return "\n\n".join(relevant_sentences[:20])

        return None

    def _extract_measurement_answer(
        self, candidates: List[Dict[str, Any]], query: str = ""
    ) -> Optional[str]:
        """Extrait une r√©ponse de mesure sp√©cifique avec validation multi-sources"""
        measurement_patterns = [
            # Patterns de hauteur explicites
            r"(?:mesure|fait|taille|hauteur)(?:\s+de)?\s+([\d,\s]+\.?\d*)\s*(m√®tres?|m\b|km|centim√®tres?|cm)",
            r"([\d,\s]+\.?\d*)\s*(m√®tres?|m\b|km|centim√®tres?|cm)(?:\s+de|d')?\s+(?:haut|hauteur|taille)",
            r"(?:s'√©l√®ve √†|atteint|culmine)\s+([\d,\s]+\.?\d*)\s*(m√®tres?|m\b|km)",
            # Patterns avec contexte de hauteur
            r"([\d,\s]+\.?\d*)\s*(?:m\b|m√®tres?)\s+(?:de haut|d'altitude|au-dessus)",
            r"(?:environ|plus de|pr√®s de|quelque)\s+([\d,\s]+\.?\d*)\s*(m√®tres?|m\b)",
            # Pattern important: "de XXX m de hauteur" ou "de XXX m√®tres"
            r"de\s+([\d,\s]+\.?\d*)\s*(m√®tres?|m)\s*(?:\[|de hauteur|d'altitude)",
            r"\b(\d{2,4})\s*(?:m\b|m√®tres?)\b",  # Capture simple comme "828 m"
        ]

        # Extraire les mots-cl√©s de l'entit√© recherch√©e depuis la query
        entity_keywords = set()
        for word in query.split():
            clean_word = word.strip("?.,!;:").lower()
            if len(clean_word) >= 4 and clean_word not in [
                "quel",
                "quelle",
                "comment",
                "taille",
                "hauteur",
                "fait",
                "mesure",
                "what",
                "which",
                "height",
                "size",
            ]:
                entity_keywords.add(clean_word)

        print(f"üîë [MEASUREMENT] Mots-cl√©s de l'entit√©: {entity_keywords}")

        # Collecter toutes les mesures avec leurs sources
        measurements_with_sources = []

        for candidate in candidates:
            sentence = candidate["sentence"]
            source = candidate.get("source", "Source inconnue")

            for pattern in measurement_patterns:
                # Utiliser finditer pour avoir la position de chaque match
                for match_obj in re.finditer(pattern, sentence, re.IGNORECASE):
                    match = match_obj.groups()
                    match_position = match_obj.start()

                    # G√©rer √† la fois les tuples et les strings simples
                    if isinstance(match, tuple):
                        if len(match) == 2 and match[1]:  # (nombre, unit√©)
                            value_str = match[0]
                            unit_str = match[1]
                        elif len(match) == 1:  # (nombre,) sans unit√©
                            value_str = match[0]
                            unit_str = "m"  # D√©faut
                        else:
                            continue
                    else:
                        # Match simple (string)
                        value_str = match
                        unit_str = "m"

                    # Normaliser la valeur en nombre
                    value_str_clean = (
                        value_str.replace(",", ".").replace(" ", "").strip()
                    )

                    try:
                        value_num = float(value_str_clean)
                        # Filtrer les valeurs aberrantes (trop petites ou trop grandes)
                        if value_num < 10 or value_num > 10000:
                            continue

                        unit = unit_str.lower()

                        # NOUVEAU : V√©rifier l'entit√© dans le CONTEXTE LOCAL de la mesure (pas toute la phrase)
                        entity_match_score = 0

                        if entity_keywords:
                            # Extraire un contexte de ~15 mots AVANT et 5 mots APR√àS la mesure
                            # Plus de mots avant car l'entit√© est g√©n√©ralement avant la mesure
                            words = sentence.split()

                            # Trouver la position du mot dans la liste de mots
                            char_count = 0
                            word_position = 0
                            for i, word in enumerate(words):
                                if char_count >= match_position:
                                    word_position = i
                                    break
                                char_count += len(word) + 1  # +1 pour l'espace

                            # Extraire SEULEMENT le contexte AVANT la mesure (plus important)
                            # et un petit contexte APR√àS (pour capturer l'unit√© et contexte imm√©diat)
                            context_before_start = max(0, word_position - 15)
                            context_before_words = words[
                                context_before_start:word_position
                            ]

                            context_after_words = words[
                                word_position : min(len(words), word_position + 5)
                            ]

                            # V√©rifier l'entit√© PRIORITAIREMENT dans le contexte AVANT
                            context_before_set = set(
                                word.lower().strip(string.punctuation)
                                for word in context_before_words
                            )
                            entity_in_before = len(
                                entity_keywords.intersection(context_before_set)
                            )

                            # R√àGLE STRICTE : L'entit√© DOIT √™tre COMPL√àTEMENT AVANT la mesure
                            # Si l'entit√© est apr√®s ou partiellement apr√®s, c'est une autre mesure dans une liste
                            if entity_in_before >= len(entity_keywords):
                                # ‚úÖ Toute l'entit√© est AVANT la mesure ‚Üí ACCEPT√â
                                entity_match_score = entity_in_before
                            else:
                                # ‚ùå L'entit√© n'est pas compl√®te avant ‚Üí REJET√â
                                # M√™me si elle est apr√®s, c'est probablement une autre mesure dans une liste
                                entity_match_score = 0

                            context = " ".join(
                                context_before_words + context_after_words
                            ).lower()

                            if entity_match_score > 0:
                                print(
                                    f"  ‚úÖ [LOCAL] Mesure {value_num} {unit} avec entit√© dans contexte: '{context[:80]}...'"
                                )
                            else:
                                print(
                                    f"  ‚ùå [LOCAL] Mesure {value_num} {unit} SANS entit√© dans contexte: '{context[:80]}...'"
                                )

                        # Score de pertinence total = pertinence candidate + bonus entit√©
                        total_relevance = candidate["relevance"] + (
                            entity_match_score * 10
                        )

                        measurements_with_sources.append(
                            {
                                "value": value_num,
                                "unit": unit,
                                "value_str": f"{value_str} {unit_str}",
                                "source": source,
                                "sentence": sentence,
                                "relevance": candidate["relevance"],
                                "entity_relevance": entity_match_score,
                                "total_relevance": total_relevance,
                            }
                        )
                    except (ValueError, IndexError):
                        continue

        if not measurements_with_sources:
            return None

        print(f"üìä [MULTI-SOURCE] {len(measurements_with_sources)} mesures trouv√©es")

        # Afficher toutes les valeurs trouv√©es pour debug avec leur pertinence entit√©
        for m in measurements_with_sources:
            entity_indicator = (
                f" üéØx{m['entity_relevance']}" if m["entity_relevance"] > 0 else ""
            )
            print(
                f"  üìç {m['value']} {m['unit']}{entity_indicator} (source: {m['source']})"
            )

        # FILTRER d'abord par pertinence √† l'entit√© si on a des mots-cl√©s
        if entity_keywords:
            # S√©parer les mesures avec et sans match d'entit√©
            entity_matches = [
                m for m in measurements_with_sources if m["entity_relevance"] > 0
            ]
            non_entity_matches = [
                m for m in measurements_with_sources if m["entity_relevance"] == 0
            ]

            if entity_matches:
                print(
                    f"üéØ [FILTER] {len(entity_matches)} mesures correspondent √† l'entit√© '{entity_keywords}'"
                )
                print(
                    f"  ‚ö†Ô∏è {len(non_entity_matches)} mesures d'autres entit√©s ignor√©es"
                )
                # Utiliser SEULEMENT les mesures qui mentionnent l'entit√© recherch√©e
                measurements_with_sources = entity_matches
            else:
                print(
                    "‚ö†Ô∏è [FILTER] Aucune mesure avec l'entit√© recherch√©e, utilisation de toutes les mesures"
                )

        # Validation multi-sources et d√©tection d'outliers
        validated_measurement = self._validate_measurements_consensus(
            measurements_with_sources, query
        )

        if validated_measurement:
            return validated_measurement

        # Fallback sur l'ancienne m√©thode si pas assez de donn√©es
        if measurements_with_sources:
            # Trier par pertinence totale
            measurements_with_sources.sort(
                key=lambda x: x["total_relevance"], reverse=True
            )
            best_match = measurements_with_sources[0]

            # Si on a une mesure valide, on essaie de la formater
            # Surtout si on a filtr√© par entit√©, c'est probablement la bonne
            print(
                f"  ‚úÖ Utilisation de la meilleure mesure (score {best_match['total_relevance']}) sans consensus"
            )

            # Convertir en m√®tres pour l'affichage uniforme
            value = best_match["value"]
            unit = best_match["unit"]
            if "km" in unit or "kilo" in unit:
                value = value * 1000
            elif "cm" in unit or "centi" in unit:
                value = value / 100

            return self._generate_formatted_measurement_answer(
                value,
                query,
                best_match.get("source", ""),
                best_match["sentence"],
                1,
            )

        return None

    def _generate_formatted_measurement_answer(
        self,
        value: float,
        query: str,
        source_name: str,
        sentence: str,
        num_confirming: int = 1,
    ) -> str:
        """G√©n√®re une r√©ponse format√©e pour une mesure"""
        # STRAT√âGIE PRIORITAIRE : Extraire l'entit√© depuis la REQU√äTE utilisateur
        entity_name = None

        if query:
            # Extraire les mots significatifs (>= 4 lettres, pas de mots-questions)
            query_words = query.split()
            entity_words = []
            stop_words = {
                "quel",
                "quelle",
                "comment",
                "taille",
                "hauteur",
                "fait",
                "mesure",
                "what",
                "which",
                "height",
                "size",
                "est",
                "la",
                "le",
                "du",
                "de",
                "des",
            }

            for word in query_words:
                clean_word = word.strip("?.,!;:").lower()
                if len(clean_word) >= 4 and clean_word not in stop_words:
                    entity_words.append(word.strip("?.,!;:"))

            # Si on a au moins 2 mots, les combiner
            if len(entity_words) >= 2:
                entity_name = " ".join(entity_words[:2])  # Prendre les 2 premiers
                # Capitaliser correctement (premi√®re lettre de chaque mot en majuscule)
                entity_name = " ".join(w.capitalize() for w in entity_name.split())
                print(f"  üéØ [ENTITY] Nom extrait de la REQU√äTE: '{entity_name}'")

        # Strat√©gie 2 : Chercher dans la source
        if not entity_name and source_name:
            # Nettoyer le nom de la source
            clean_source = (
                source_name.replace("(Article direct)", "")
                .replace("Wikipedia FR", "")
                .replace("Wikipedia EN", "")
                .strip()
            )

            # V√©rifier si c'est une page sp√©cifique (pas une liste g√©n√©rique)
            if clean_source and clean_source not in [
                "Structure",
                "Source inconnue",
                "Liste des plus hautes structures du monde",
                "Listes des plus hautes constructions du monde",
                "Ordres de grandeur de longueur",
                "Chronologie des plus hautes structures du monde",
            ]:
                entity_name = clean_source
                print(f"  üéØ [ENTITY] Nom trouv√© depuis source: '{entity_name}'")

        # Strat√©gie 3 : Extraire depuis la phrase
        if not entity_name and sentence:
            # Chercher un pattern comme "Burj Khalifa" ou "le/la Nom"
            name_patterns = [
                r"([A-Z][a-zA-Z\s]+?)\s+\(",  # Nom avant une parenth√®se
                r"\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+)+)\b",  # Nom propre (mots en majuscule)
            ]

            for pattern in name_patterns:
                match = re.search(pattern, sentence)
                if match:
                    potential_name = match.group(1).strip()
                    # V√©rifier que ce n'est pas un mot commun
                    if potential_name not in [
                        "La",
                        "Le",
                        "Un",
                        "Une",
                        "De",
                        "Du",
                        "Des",
                        "Il",
                        "Elle",
                    ]:
                        entity_name = potential_name
                        print(
                            f"  üéØ [ENTITY] Nom extrait de la phrase: '{entity_name}'"
                        )
                        break

        # Strat√©gie 4 : Fallback g√©n√©rique
        if not entity_name:
            entity_name = "structure"
            print("  ‚ö†Ô∏è [ENTITY] Nom g√©n√©rique utilis√©")

            # Construire la r√©ponse simple et directe
        # Adapter l'article selon le genre (si commence par voyelle, utiliser "l'")
        if entity_name[0].lower() in "aeiouh√©√®√™":
            article = "L'"
            simple_answer = (
                f"{article}{entity_name} mesure {int(value)} m√®tres de hauteur."
            )
        else:
            # D√©tecter si c'est masculin ou f√©minin (par d√©faut masculin)
            if entity_name.lower().startswith(
                ("tour", "fl√®che", "antenne", "structure")
            ):
                article = "La"
            else:
                article = "Le"
            simple_answer = (
                f"{article} {entity_name} mesure {int(value)} m√®tres de hauteur."
            )

        print(f"  üìù [SIMPLE] R√©ponse g√©n√©r√©e: {simple_answer}")

        # Ajouter l'information de validation si pertinent
        if num_confirming >= 3:
            validation_note = (
                f" (‚úÖ Confirm√© par {num_confirming} sources ind√©pendantes)"
            )
            simple_answer += validation_note

        return simple_answer.strip()

    def _validate_measurements_consensus(
        self, measurements: List[Dict[str, Any]], query: str = ""
    ) -> Optional[str]:
        """
        Valide les mesures en croisant plusieurs sources et d√©tecte les outliers

        Args:
            measurements: Liste de dictionnaires avec 'value', 'unit', 'source', 'sentence'
            query: La requ√™te originale de l'utilisateur pour extraire l'entit√©

        Returns:
            str: Phrase avec la mesure consensuelle et les sources, ou None
        """
        if len(measurements) < 2:
            # Pas assez de sources pour validation
            return None

        print("üîç [CONSENSUS] Validation multi-sources en cours...")

        # Normaliser toutes les valeurs dans la m√™me unit√© (m√®tres)
        normalized_measurements = []
        for m in measurements:
            value = m["value"]
            unit = m["unit"]

            # Convertir en m√®tres
            if "km" in unit or "kilo" in unit:
                value = value * 1000
            elif "cm" in unit or "centi" in unit:
                value = value / 100

            normalized_measurements.append({**m, "normalized_value": value})

        # Extraire les valeurs normalis√©es
        values = [m["normalized_value"] for m in normalized_measurements]

        # Calculer les statistiques
        mean_value = statistics.mean(values)

        if len(values) >= 3:
            median_value = statistics.median(values)
            # Calculer l'√©cart-type pour d√©tecter les outliers
            std_dev = statistics.stdev(values) if len(values) > 1 else 0
        else:
            median_value = mean_value
            std_dev = 0

        print(
            f"  üìä Moyenne: {mean_value:.1f}m, M√©diane: {median_value:.1f}m, √âcart-type: {std_dev:.1f}m"
        )

        # D√©tection des outliers (valeurs √† plus de 2 √©cart-types)
        inliers = []
        outliers = []

        for m in normalized_measurements:
            if std_dev == 0 or abs(m["normalized_value"] - mean_value) <= 2 * std_dev:
                inliers.append(m)
            else:
                outliers.append(m)
            print(
                f"  ‚ö†Ô∏è Outlier d√©tect√©: {m['value']} {m['unit']} de {m['source']} (trop √©loign√© du consensus)"
            )

        # G√©n√©rer une r√©ponse si on a au moins 1 mesure fiable
        if len(inliers) >= 1:
            # Prendre la m√©diane des valeurs fiables (ou la seule valeur si 1 seule mesure)
            if len(inliers) >= 2:
                consensus_value = statistics.median(
                    [m["normalized_value"] for m in inliers]
                )
            else:
                # Une seule mesure fiable
                consensus_value = inliers[0]["normalized_value"]

            # Trouver la mesure la plus proche du consensus
            closest_measurement = min(
                inliers, key=lambda m: abs(m["normalized_value"] - consensus_value)
            )

            # Compter les sources qui confirment (valeurs similaires √† ¬±5%)
            tolerance = consensus_value * 0.05
            confirming_sources = [
                m
                for m in inliers
                if abs(m["normalized_value"] - consensus_value) <= tolerance
            ]

            num_confirming = len(set(m["source"] for m in confirming_sources))

            if len(inliers) >= 2:
                print(
                    f"  ‚úÖ Consensus trouv√©: {consensus_value:.0f}m ({num_confirming} sources concordantes)"
                )
            else:
                print(
                    f"  ‚ÑπÔ∏è Mesure unique trouv√©e: {consensus_value:.0f}m (source: {inliers[0].get('source', 'inconnue')})"
                )  # NOUVELLE APPROCHE : G√©n√©rer une r√©ponse SIMPLE et DIRECTE
            # Au lieu d'extraire de phrases complexes, construire la r√©ponse nous-m√™mes

            # STRAT√âGIE PRIORITAIRE : Extraire l'entit√© depuis la REQU√äTE utilisateur
            entity_name = None

            if query:
                # Extraire les mots significatifs (>= 3 lettres, pas de mots-questions/articles)
                query_words = query.split()
                entity_words = []
                stop_words = {
                    "quel",
                    "quelle",
                    "quels",
                    "quelles",
                    "comment",
                    "combien",
                    "pourquoi",
                    "taille",
                    "hauteur",
                    "fait",
                    "mesure",
                    "poids",
                    "what",
                    "which",
                    "height",
                    "size",
                    "how",
                    "tall",
                    "est",
                    "sont",
                    "√©tait",
                    "√©taient",
                    "la",
                    "le",
                    "les",
                    "l",
                    "un",
                    "une",
                    "des",
                    "du",
                    "de",
                    "d",
                    "au",
                    "aux",
                }

                for word in query_words:
                    # Nettoyer le mot des ponctuations et apostrophes
                    clean_word = word.strip("?.,!;:'\"").lower()
                    # G√©rer les mots avec apostrophe comme "l'empire" -> "empire"
                    if "'" in clean_word:
                        parts = clean_word.split("'")
                        clean_word = parts[-1] if len(parts[-1]) > 2 else clean_word

                    if len(clean_word) >= 3 and clean_word not in stop_words:
                        # Garder le mot original (sans ponctuation finale) pour le capitaliser correctement
                        original_word = word.strip("?.,!;:")
                        # Si le mot contient une apostrophe, ne garder que la partie apr√®s
                        if "'" in original_word:
                            original_word = original_word.split("'")[-1]
                        entity_words.append(original_word)

                # Si on a au moins 1 mot significatif, l'utiliser
                if len(entity_words) >= 1:
                    # Combiner les mots (max 3 pour √©viter les noms trop longs)
                    entity_name = " ".join(entity_words[:3])
                    # Capitaliser correctement (Title Case)
                    entity_name = entity_name.title()
                    print(f"  üéØ [ENTITY] Nom extrait de la REQU√äTE: '{entity_name}'")

            # Strat√©gie 2 : Chercher dans les sources Wikipedia
            if not entity_name:
                for m in inliers:
                    source = m.get("source", "")
                    # Nettoyer le nom de la source
                    clean_source = (
                        source.replace("(Article direct)", "")
                        .replace("Wikipedia FR", "")
                        .replace("Wikipedia EN", "")
                        .strip()
                    )

                    # V√©rifier si c'est une page sp√©cifique (pas une liste g√©n√©rique)
                    if clean_source and clean_source not in [
                        "Structure",
                        "Source inconnue",
                        "Liste des plus hautes structures du monde",
                        "Listes des plus hautes constructions du monde",
                        "Ordres de grandeur de longueur",
                        "Chronologie des plus hautes structures du monde",
                    ]:
                        entity_name = clean_source
                        print(
                            f"  üéØ [ENTITY] Nom trouv√© depuis source: '{entity_name}'"
                        )
                        break

            # Strat√©gie 3 : Extraire depuis la phrase
            if not entity_name:
                sentence = closest_measurement["sentence"]

                # Chercher un pattern comme "Burj Khalifa" ou "le/la Nom"
                name_patterns = [
                    r"([A-Z][a-zA-Z\s]+?)\s+\(",  # Nom avant une parenth√®se
                    r"\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+)+)\b",  # Nom propre (mots en majuscule)
                ]

                for pattern in name_patterns:
                    match = re.search(pattern, sentence)
                    if match:
                        potential_name = match.group(1).strip()
                        # V√©rifier que ce n'est pas un mot commun
                        if potential_name not in [
                            "La",
                            "Le",
                            "Un",
                            "Une",
                            "De",
                            "Du",
                            "Des",
                            "Il",
                            "Elle",
                        ]:
                            entity_name = potential_name
                            print(
                                f"  üéØ [ENTITY] Nom extrait de la phrase: '{entity_name}'"
                            )
                            break

            # Strat√©gie 4 : Fallback g√©n√©rique
            if not entity_name:
                entity_name = "structure"
                print("  ‚ö†Ô∏è [ENTITY] Nom g√©n√©rique utilis√©")

            # Construire la r√©ponse simple et directe
            # Nettoyer le nom de l'entit√© (enlever articles r√©siduels au d√©but)
            entity_name_clean = entity_name.strip()
            # Enlever les articles au d√©but s'ils sont pr√©sents
            for prefix in ["L'", "l'", "Le ", "le ", "La ", "la ", "Les ", "les "]:
                if entity_name_clean.startswith(prefix):
                    entity_name_clean = entity_name_clean[len(prefix) :].strip()
                    break

            # Recapitaliser proprement
            entity_name_clean = entity_name_clean.title()

            # Adapter l'article selon le genre et la premi√®re lettre
            first_letter = entity_name_clean[0].lower() if entity_name_clean else ""

            # Mots f√©minins connus
            feminine_words = (
                "tour",
                "fl√®che",
                "antenne",
                "structure",
                "statue",
                "pyramide",
                "cath√©drale",
                "√©glise",
                "mosqu√©e",
            )
            is_feminine = entity_name_clean.lower().startswith(feminine_words)

            # Construire la phrase naturelle
            if first_letter in "aeiouh√©√®√™y":
                # Voyelle -> utiliser "L'"
                simple_answer = f"L'{entity_name_clean} mesure **{int(consensus_value)} m√®tres** de hauteur."
            elif is_feminine:
                simple_answer = f"La {entity_name_clean} mesure **{int(consensus_value)} m√®tres** de hauteur."
            else:
                simple_answer = f"Le {entity_name_clean} mesure **{int(consensus_value)} m√®tres** de hauteur."

            print(f"  üìù [SIMPLE] R√©ponse g√©n√©r√©e: {simple_answer}")

            # Ajouter l'information de validation si pertinent
            if num_confirming >= 3:
                validation_note = (
                    f" (‚úÖ Confirm√© par {num_confirming} sources ind√©pendantes)"
                )
                simple_answer += validation_note

            return simple_answer.strip()

        # Pas de consensus clair
        print(
            f"  ‚ö†Ô∏è Pas de consensus clair ({len(inliers)} sources fiables sur {len(measurements)})"
        )
        return None

    def _extract_definition_answer(
        self, candidates: List[Dict[str, Any]]
    ) -> Optional[str]:
        """Extrait une d√©finition"""
        definition_patterns = [
            r"(?:est|sont)\s+([^.!?]{20,150})",
            r"(?:c'est|ce sont)\s+([^.!?]{20,150})",
            r"([^.!?]{20,150})(?:\s+est|\s+sont)",
        ]

        definitions = Counter()
        source_sentences = {}

        for candidate in candidates:
            sentence = candidate["sentence"]
            for pattern in definition_patterns:
                matches = re.findall(pattern, sentence, re.IGNORECASE)
                for match in matches:
                    clean_match = match.strip()
                    if len(clean_match) > 15:
                        definitions[clean_match] += candidate["relevance"]
                        if clean_match not in source_sentences:
                            source_sentences[clean_match] = sentence

        if definitions:
            # Trier par pertinence et prendre les 20 meilleures d√©finitions
            top_definitions = definitions.most_common(20)

            # Construire un r√©sum√© avec toutes les d√©finitions pertinentes
            result_parts = []
            for definition, _score in top_definitions:
                if definition in source_sentences:
                    cleaned_sentence = self._universal_word_spacing_fix(
                        source_sentences[definition]
                    )
                    result_parts.append(cleaned_sentence.strip())

            if result_parts:
                return "\n\n".join(result_parts)

        return None

    def _extract_date_answer(self, candidates: List[Dict[str, Any]]) -> Optional[str]:
        """Extrait une r√©ponse de date"""
        date_patterns = [
            r"(?:en|depuis|dans|cr√©√©|fond√©|construit|n√©|inaugur√©)\s+(\d{4})",
            r"(\d{1,2})\s+(?:janvier|f√©vrier|mars|avril|mai|juin|juillet|ao√ªt|septembre|octobre|novembre|d√©cembre)\s+(\d{4})",
            r"(\d{4})(?:\s*-\s*\d{4})?",
        ]

        dates = Counter()
        source_sentences = {}

        for candidate in candidates:
            sentence = candidate["sentence"]
            for pattern in date_patterns:
                matches = re.findall(pattern, sentence, re.IGNORECASE)
                for match in matches:
                    if isinstance(match, tuple):
                        date_key = " ".join(str(m) for m in match if m)
                    else:
                        date_key = str(match)

                    dates[date_key] += candidate["relevance"]
                    if date_key not in source_sentences:
                        source_sentences[date_key] = sentence

        if dates:
            # Trier par pertinence et prendre les 20 meilleures dates
            top_dates = dates.most_common(20)

            # Construire un r√©sum√© avec toutes les phrases contenant des dates pertinentes
            result_parts = []
            for date, _score in top_dates:
                if date in source_sentences:
                    cleaned_sentence = self._universal_word_spacing_fix(
                        source_sentences[date]
                    )
                    result_parts.append(cleaned_sentence.strip())

            if result_parts:
                return "\n\n".join(result_parts)

        return None

    def _extract_general_answer(
        self, query: str, candidates: List[Dict[str, Any]]
    ) -> Optional[str]:
        """Extrait une r√©ponse g√©n√©rale bas√©e sur le consensus"""
        if not candidates:
            return None

        # Analyser les mots-cl√©s de la question
        query_words = set(
            word.lower().strip(string.punctuation)
            for word in query.split()
            if len(word) > 2
        )

        # Calculer un score de consensus pour chaque phrase
        sentence_scores = {}

        for candidate in candidates:
            sentence = candidate["sentence"]

            # Score bas√© sur la pertinence originale
            score = candidate["relevance"]

            # Bonus pour les phrases contenant plusieurs mots-cl√©s de la question
            sentence_words = set(
                word.lower().strip(string.punctuation) for word in sentence.split()
            )
            keyword_overlap = len(query_words.intersection(sentence_words))
            score += keyword_overlap * 2

            # Bonus pour les phrases avec des informations pr√©cises (nombres, dates, etc.)
            if re.search(r"\d+", sentence):
                score += 3

            # Bonus pour les phrases qui semblent √™tre des r√©ponses directes
            if any(
                sentence.lower().startswith(start)
                for start in [
                    "la",
                    "le",
                    "il",
                    "elle",
                    "c'est",
                    "ce sont",
                    "on trouve",
                    "situ√©e",
                    "situ√©",
                ]
            ):
                score += 2

            sentence_scores[sentence] = score

        # Prendre les 50 meilleures phrases (augment√© de 25) pour donner BEAUCOUP plus de contexte
        if sentence_scores:
            # Trier par score d√©croissant et prendre les 50 meilleures
            sorted_sentences = sorted(
                sentence_scores.items(), key=lambda x: x[1], reverse=True
            )

            # Prendre les 50 meilleures phrases (ou moins s'il y en a moins)
            top_sentences = sorted_sentences[:50]

            # Construire un r√©sum√© structur√© avec toutes les phrases pertinentes
            result_parts = []
            for sentence, score in top_sentences:
                cleaned_sentence = self._universal_word_spacing_fix(sentence)
                result_parts.append(cleaned_sentence.strip())

            # Joindre avec des sauts de ligne pour lisibilit√©
            return "\n\n".join(result_parts)

        return None

    def _is_natural_response(self, text: str) -> bool:
        """
        V√©rifie si une r√©ponse est formul√©e naturellement ou si c'est un extrait brut.

        Une r√©ponse naturelle :
        - Commence par un sujet clair (La/Le/L'/Un/Une + nom)
        - Contient un verbe principal (mesure, est, fait, etc.)
        - Ne contient pas de fragments de navigation (Adresse, Acc√®s, Coordonn√©es, etc.)

        Returns:
            bool: True si la r√©ponse est naturelle, False si c'est un extrait brut
        """
        if not text:
            return False

        text_lower = text.lower().strip()

        # Indicateurs d'extrait brut de Wikipedia/site web
        raw_indicators = [
            "adresse",
            "acc√®s et transport",
            "coordonn√©es",
            "modifier le code",
            "modifier wikidata",
            "autobus",
            "ratp",
            "gare",
            "m√©tro",
            "[modifier",
            "| modifier",
            "navigation",
            "menu",
            "sommaire",
            "r√©f√©rences",
            "voir aussi",
            "liens externes",
            "notes et r√©f√©rences",
            "¬∞",
            "‚Ä≤",
            "‚Ä≥",  # Coordonn√©es GPS
        ]

        # Si l'un de ces indicateurs est pr√©sent, c'est un extrait brut
        for indicator in raw_indicators:
            if indicator in text_lower:
                return False

        # V√©rifier si √ßa commence par une structure de phrase naturelle
        natural_starts = [
            r"^l[ae']?\s+\w+\s+(mesure|fait|est|a|poss√®de|compte|s'√©l√®ve)",
            r"^(la|le|les|un|une)\s+\w+\s+(mesure|fait|est|a|poss√®de|compte|s'√©l√®ve)",
            r"^\w+\s+(mesure|fait|est|a|poss√®de|compte|s'√©l√®ve)",
        ]

        for pattern in natural_starts:
            if re.match(pattern, text_lower):
                return True

        # Si le texte est court et contient une mesure claire, c'est probablement naturel
        if len(text) < 200 and re.search(r"\d+\s*(m√®tres?|m\b|km)", text_lower):
            # Mais v√©rifier qu'il n'y a pas trop de bruit
            word_count = len(text.split())
            if word_count < 30:
                return True

        return False

    def _reformulate_raw_extract(self, raw_text: str, query: str) -> str:
        """
        Reformule un extrait brut en r√©ponse naturelle.

        Extrait les informations cl√©s (mesures, dates, faits) et les reformule
        dans une phrase naturelle.

        Args:
            raw_text: L'extrait brut √† reformuler
            query: La question originale de l'utilisateur

        Returns:
            str: Une r√©ponse reformul√©e naturellement
        """
        # Extraire l'entit√© de la question
        entity_name = self._extract_entity_from_query(query)

        # Chercher les mesures dans le texte brut
        measurement_match = re.search(
            r"(\d+(?:[,.\s]\d+)?)\s*(m√®tres?|m\b|km|cm)\s*(?:\[|\(|de hauteur|d'altitude)?",
            raw_text,
            re.IGNORECASE,
        )

        if measurement_match:
            value = measurement_match.group(1).replace(",", ".").replace(" ", "")
            unit = measurement_match.group(2).lower()

            # Normaliser l'unit√©
            if unit == "m":
                unit = "m√®tres"

            try:
                value_float = float(value)
                value_int = int(value_float)

                # Construire une r√©ponse naturelle
                return self._build_natural_measurement_response(
                    entity_name, value_int, unit
                )
            except ValueError:
                pass

        # Si pas de mesure trouv√©e, chercher d'autres informations
        # Fallback: nettoyer l'extrait et prendre la premi√®re phrase pertinente
        sentences = re.split(r"[.!?]+", raw_text)
        for sentence in sentences:
            sentence = sentence.strip()
            # Ignorer les phrases de navigation
            if len(sentence) > 20 and len(sentence) < 300:
                lower_sent = sentence.lower()
                if not any(
                    x in lower_sent
                    for x in ["adresse", "coordonn√©es", "modifier", "acc√®s", "autobus"]
                ):
                    # Nettoyer et retourner
                    cleaned = self._universal_word_spacing_fix(sentence)
                    return self._intelligent_bold_formatting(cleaned)

        # Dernier recours: retourner un message g√©n√©rique
        return f"üìç Informations trouv√©es sur **{entity_name}** - consultez les sources ci-dessous pour plus de d√©tails."

    def _extract_entity_from_query(self, query: str) -> str:
        """Extrait le nom de l'entit√© depuis la question."""
        stop_words = {
            "quel",
            "quelle",
            "quels",
            "quelles",
            "comment",
            "combien",
            "pourquoi",
            "o√π",
            "taille",
            "hauteur",
            "fait",
            "mesure",
            "poids",
            "what",
            "which",
            "height",
            "size",
            "how",
            "tall",
            "est",
            "sont",
            "√©tait",
            "√©taient",
            "la",
            "le",
            "les",
            "l",
            "un",
            "une",
            "des",
            "du",
            "de",
            "d",
            "au",
            "aux",
        }

        words = query.split()
        entity_words = []

        for word in words:
            clean_word = word.strip("?.,!;:'\"").lower()
            # G√©rer les apostrophes
            if "'" in clean_word:
                parts = clean_word.split("'")
                clean_word = parts[-1] if len(parts[-1]) > 2 else clean_word

            if len(clean_word) >= 3 and clean_word not in stop_words:
                original_word = word.strip("?.,!;:")
                if "'" in original_word:
                    original_word = original_word.split("'")[-1]
                entity_words.append(original_word)

        if entity_words:
            return " ".join(entity_words[:3]).title()
        return "cette entit√©"

    def _build_natural_measurement_response(
        self, entity_name: str, value: int, unit: str
    ) -> str:
        """Construit une r√©ponse naturelle pour une mesure."""
        # Nettoyer le nom de l'entit√©
        entity_clean = entity_name.strip()
        for prefix in ["L'", "l'", "Le ", "le ", "La ", "la ", "Les ", "les "]:
            if entity_clean.startswith(prefix):
                entity_clean = entity_clean[len(prefix) :].strip()
                break
        entity_clean = entity_clean.title()

        # D√©terminer l'article appropri√©
        first_letter = entity_clean[0].lower() if entity_clean else ""
        feminine_words = (
            "tour",
            "fl√®che",
            "antenne",
            "structure",
            "statue",
            "pyramide",
            "cath√©drale",
            "√©glise",
            "mosqu√©e",
        )
        is_feminine = entity_clean.lower().startswith(feminine_words)

        # Construire la phrase
        if first_letter in "aeiouh√©√®√™y":
            response = f"L'{entity_clean} mesure **{value} {unit}** de hauteur."
        elif is_feminine:
            response = f"La {entity_clean} mesure **{value} {unit}** de hauteur."
        else:
            response = f"Le {entity_clean} mesure **{value} {unit}** de hauteur."

        return response

    def _generate_answer_focused_summary(
        self,
        query: str,
        direct_answer: Optional[str],
        page_contents: List[Dict[str, Any]],
    ) -> str:
        """G√©n√®re un r√©sum√© centr√© sur la r√©ponse directe - VERSION CORRIG√âE"""
        summary = ""

        if direct_answer:
            # üî• NOUVEAU: Si direct_answer contient d√©j√† plusieurs phrases (s√©par√©es par \n\n),
            # NE PAS le reformuler - c'est d√©j√† un r√©sum√© enrichi de _extract_general_answer()
            if "\n\n" in direct_answer:
                # Multiple phrases d√©j√† format√©es - juste nettoyer et formater
                cleaned_answer = self._universal_word_spacing_fix(direct_answer)
                enhanced_answer = self._intelligent_bold_formatting(cleaned_answer)
                summary += f"{enhanced_answer}\n\n"
            else:
                # Une seule phrase - v√©rifier si naturelle ou si besoin de reformulation
                is_natural_response = self._is_natural_response(direct_answer)

                if is_natural_response:
                    # R√©ponse d√©j√† naturelle, juste nettoyer
                    cleaned_answer = self._universal_word_spacing_fix(direct_answer)
                    enhanced_answer = self._intelligent_bold_formatting(cleaned_answer)
                    summary += f"{enhanced_answer}\n\n"
                else:
                    # Extrait brut -> essayer de reformuler naturellement
                    reformulated = self._reformulate_raw_extract(direct_answer, query)
                    summary += f"{reformulated}\n\n"
        else:
            key_info = self._extract_concentrated_summary(query, page_contents)
            cleaned_info = self._universal_word_spacing_fix(key_info)
            enhanced_info = self._intelligent_bold_formatting(cleaned_info)
            summary += f"üìç **Information trouv√©e :**\n{enhanced_info}\n\n"

        # Format horizontal pour les sources
        summary += "üîó **Sources** : "

        source_links = []
        for result in page_contents[:5]:
            if result.get("title") and result.get("url"):
                title = self._clean_title(result["title"])
                clean_title = self._universal_word_spacing_fix(title)
                url = result.get("url")

                if url and url.startswith("http"):
                    source_links.append(f"[{clean_title}]({url})")
                else:
                    source_links.append(f"[{clean_title}]")

        if source_links:
            summary += ", ".join(source_links) + "\n"
        else:
            summary += "Aucune source disponible\n"

        print(f"[DEBUG] R√©sum√© g√©n√©r√© avec {len(source_links)} liens:")
        print(f"[DEBUG] Nombre total de caract√®res dans le r√©sum√©: {len(summary)}")
        print(f"[DEBUG] R√©sum√© complet (premiers 500 chars):\n{summary[:500]}")
        print(f"[DEBUG] R√©sum√© complet (500-1000 chars):\n{summary[500:1000]}")
        print(f"[DEBUG] R√©sum√© complet (1000-1500 chars):\n{summary[1000:1500]}")

        return summary

    def _enhance_answer_formatting(self, text: str) -> str:
        """Am√©liore le formatage de la r√©ponse"""
        if not text:
            return text

        # 1. CORRECTION UNIVERSELLE DES ESPACEMENTS
        cleaned_text = self._universal_word_spacing_fix(text)

        # 2. FORMATAGE INTELLIGENT DES MOTS IMPORTANTS
        formatted_text = self._intelligent_bold_formatting(cleaned_text)

        return formatted_text

    def _intelligent_bold_formatting(self, text: str) -> str:
        """Formatage intelligent en gras"""

        if not text:
            return text

        # 1. CHIFFRES + UNIT√âS
        text = re.sub(
            r"\b(\d+(?:[,.\s]\d+)?)\s*(m√®tres?|kilom√®tres?|centim√®tres?|m|km|cm)(?!\w)",
            r"**\1 \2**",
            text,
            flags=re.IGNORECASE,
        )

        # Poids
        text = re.sub(
            r"\b(\d+(?:[,.\s]\d+)?)\s*(kilogrammes?|tonnes?|grammes?|kg|g)(?!\w)",
            r"**\1 \2**",
            text,
            flags=re.IGNORECASE,
        )

        # Monnaie
        text = re.sub(
            r"\b(\d+(?:[,.\s]\d+)?)\s*(euros?|dollars?|\$|‚Ç¨)(?!\w)",
            r"**\1 \2**",
            text,
            flags=re.IGNORECASE,
        )

        # 2. DATES
        text = re.sub(r"\b(\d{4})\b", r"**\1**", text)

        # 3. Noms propres importants
        important_names = [r"\bTour\s+Eiffel\b", r"\bNotre[-\s]Dame\b", r"\bLouvre\b"]

        for pattern in important_names:
            text = re.sub(
                pattern, lambda m: f"**{m.group(0)}**", text, flags=re.IGNORECASE
            )

        # 4. Nettoyer le formatage
        text = re.sub(r"\*{3,}", "**", text)
        text = re.sub(r"\*\*\s*\*\*", "", text)

        return text

    def _universal_word_spacing_fix(self, text: str) -> str:
        """Correction AM√âLIOR√âE qui ne casse pas les mots valides"""

        if not text:
            return text

        # √âtape 1: S√©parer SEULEMENT les mots vraiment coll√©s (minuscule + majuscule)
        text = re.sub(r"([a-z])([A-Z])", r"\1 \2", text)

        # √âtape 2: S√©parer les chiffres des lettres
        text = re.sub(r"([a-zA-Z])(\d)", r"\1 \2", text)
        text = re.sub(r"(\d)([a-zA-Z])", r"\1 \2", text)

        # √âtape 3: CORRECTION - Seulement les cas √©vidents de mots coll√©s
        # Ne pas toucher aux mots valides comme "mesure", "actuellement", etc.

        # Liste RESTREINTE aux vrais cas de mots coll√©s courants
        obvious_splits = [
            # Cas tr√®s √©vidents uniquement
            (r"\b(la|le|les)(tour|ville|monde|france|paris)\b", r"\1 \2"),
            (r"\b(tour|ville)(eiffel|paris|france)\b", r"\1 \2"),
            (r"\b(de|du|des)(la|le|les)\b", r"\1 \2"),
            # Pr√©positions coll√©es √©videntes
            (r"\b(dans|sur|pour|avec|sans)(le|la|les|un|une)\b", r"\1 \2"),
        ]

        # Appliquer SEULEMENT les cas √©vidents
        for pattern, replacement in obvious_splits:
            old_text = text
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
            if text != old_text:
                print(f"[DEBUG] Appliqu√©: {pattern} -> chang√© en: {repr(text)}")

        # √âtape 4: Nettoyer les espaces SANS toucher aux doubles sauts de ligne
        # üî• CRITIQUE: Pr√©server les \n\n pour garder les phrases s√©par√©es dans les r√©sum√©s enrichis
        # Remplacer d'abord les doubles+ sauts de ligne par un marqueur temporaire
        text = text.replace("\n\n", "¬ß¬ßDOUBLE_NEWLINE¬ß¬ß")
        # Nettoyer les espaces multiples et les simples newlines
        text = re.sub(r"[ \t]+", " ", text)  # Espaces/tabs multiples -> un espace
        text = text.replace("\n", " ")  # Simples newlines -> espace
        # Restaurer les doubles sauts de ligne
        text = text.replace("¬ß¬ßDOUBLE_NEWLINE¬ß¬ß", "\n\n")

        # Nettoyer la ponctuation
        text = re.sub(r"\s+([.!?:;,])", r"\1", text)
        text = re.sub(r"([.!?:;,])([a-zA-Z])", r"\1 \2", text)

        result = text.strip()
        return result

    def _clean_title(self, title: str) -> str:
        """Nettoie le titre et s'assure qu'il n'est pas None"""
        if not title:
            return "Source"

        cleaned = str(title)  # Conversion s√©curis√©e en string

        # CORRECTION : Remplacer les caract√®res probl√©matiques pour les liens
        cleaned = cleaned.replace(":", " -")

        # Remplacer d'autres caract√®res potentiellement probl√©matiques
        cleaned = cleaned.replace("|", "-")
        cleaned = cleaned.replace("[", "(").replace("]", ")")

        # Supprimer les parties ind√©sirables
        cleaned = re.sub(r"\s*[\[\(].*?[\]\)]\s*$", "", cleaned)
        cleaned = re.sub(r"\s*[-|‚Äî]\s*[^-]+$", "", cleaned)

        # Limiter la longueur
        if len(cleaned) > 60:
            cleaned = cleaned[:57] + "..."

        # Appliquer la correction d'espacement
        cleaned = self._universal_word_spacing_fix(cleaned)

        return cleaned.strip() if cleaned.strip() else "Source"

    def _extract_concentrated_summary(
        self, query: str, page_contents: List[Dict[str, Any]]
    ) -> str:
        """Extrait un r√©sum√© concentr√© quand pas de r√©ponse directe"""
        # Combiner tous les snippets
        all_snippets = []
        for content in page_contents:
            snippet = content.get("snippet", "")
            if snippet:
                # Nettoyer le snippet avec la correction universelle
                cleaned_snippet = self._universal_word_spacing_fix(snippet)
                all_snippets.append(cleaned_snippet)

        combined_text = " ".join(all_snippets)

        # Extraire les phrases les plus pertinentes
        sentences = re.split(r"[.!?]+", combined_text)
        query_words = set(word.lower() for word in query.split() if len(word) > 2)

        scored_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 20:
                sentence_words = set(word.lower() for word in sentence.split())
                score = len(query_words.intersection(sentence_words))
                if score > 0:
                    # Nettoyer la phrase avec la correction universelle
                    cleaned_sentence = self._universal_word_spacing_fix(sentence)
                    scored_sentences.append((cleaned_sentence, score))

        # Prendre la meilleure phrase
        scored_sentences.sort(key=lambda x: x[1], reverse=True)

        if scored_sentences:
            return scored_sentences[0][0]

        return "Information trouv√©e mais n√©cessite une recherche plus sp√©cifique."

    def _perform_search(self, query: str) -> List[Dict[str, Any]]:
        """Effectue la recherche sur internet"""
        search_methods = [
            self._search_duckduckgo_instant,  # PRIORIT√â 1: API officielle stable et rapide
            self._search_with_cloudscraper,  # PRIORIT√â 2: Contourne anti-bot si API √©choue
            self._search_searxng,  # PRIORIT√â 3: M√©tamoteur alternatif
            self._search_with_requests,  # PRIORIT√â 4: Scraping HTML classique
            self._search_google_html,  # PRIORIT√â 5: Google en dernier recours
            self._search_brave,  # PRIORIT√â 6: Brave alternatif
            self._search_fallback,  # PRIORIT√â 7: Wikipedia fallback
        ]

        for method in search_methods:
            try:
                results = method(query)
                if results:
                    print(f"‚úÖ {len(results)} r√©sultats trouv√©s avec {method.__name__}")
                    return results
            except Exception as e:
                print(f"‚ö†Ô∏è {method.__name__} a √©chou√©: {str(e)}")
                continue

        return []

    def _search_with_cloudscraper(self, query: str) -> List[Dict[str, Any]]:
        """
        Recherche avec cloudscraper (contourne CAPTCHA et anti-bot)
        Essaie DuckDuckGo Lite en priorit√©, puis Google si √©chec
        """
        print(
            f"üöÄ [CLOUDSCRAPER] M√©thode appel√©e - AVAILABLE={CLOUDSCRAPER_AVAILABLE}, scraper={self.scraper is not None if hasattr(self, 'scraper') else 'NO_ATTR'}"
        )

        if not CLOUDSCRAPER_AVAILABLE or self.scraper is None:
            print("‚ö†Ô∏è Cloudscraper non disponible ou non initialis√©")
            return []

        # Essayer DuckDuckGo Lite d'abord
        print("üîç Cloudscraper: Recherche sur DuckDuckGo Lite...")
        try:
            search_url = "https://lite.duckduckgo.com/lite/"
            print(f"üìç URL: {search_url}")
            print("üìç Appel scraper.post()...")

            data = {"q": query, "kl": "fr-fr"}
            response = self.scraper.post(search_url, data=data, timeout=15)
            print(
                f"üìç R√©ponse re√ßue: {response.status_code}, Taille: {len(response.text)} chars"
            )
            response.raise_for_status()
            response.encoding = "utf-8"

            soup = BeautifulSoup(response.text, "html.parser")
            results = []

            # DuckDuckGo Lite utilise une structure de table simple
            result_tables = soup.find_all("table", class_="result-table")
            print(f"üìä Nombre de tables trouv√©es: {len(result_tables)}")

            for table in result_tables[: self.max_results]:
                try:
                    title_elem = table.find("a", class_="result-link")
                    snippet_elem = table.find("td", class_="result-snippet")

                    if title_elem:
                        title = title_elem.get_text(strip=True)
                        url = title_elem.get("href", "")
                        snippet = (
                            snippet_elem.get_text(strip=True) if snippet_elem else ""
                        )

                        if title and len(title) > 3:
                            results.append(
                                {
                                    "title": title,
                                    "snippet": snippet if snippet else title,
                                    "url": url,
                                    "source": "DuckDuckGo Lite",
                                }
                            )
                except Exception as e:
                    print(f"‚ö†Ô∏è Erreur lors du parsing DuckDuckGo Lite: {str(e)}")
                    continue

            if results:
                print(f"‚úÖ DuckDuckGo Lite: {len(results)} r√©sultats trouv√©s")
                return results

        except Exception as e:
            print(f"‚ö†Ô∏è Cloudscraper DDG √©chou√©: {str(e)}")
            traceback.print_exc()

        # Si DDG √©choue, essayer Google
        print("üîç Cloudscraper: Tentative Google...")
        try:
            search_url = f"https://www.google.com/search?q={quote(query)}&hl=fr"
            print(f"üìç URL Google: {search_url[:80]}...")

            response = self.scraper.get(search_url, timeout=15)
            print(
                f"üìç R√©ponse Google: {response.status_code}, Taille: {len(response.text)} chars"
            )
            response.raise_for_status()
            response.encoding = "utf-8"

            soup = BeautifulSoup(response.text, "html.parser")
            results = []

            # Google: chercher les divs de r√©sultats
            result_divs = soup.find_all("div", class_="g")
            print(f"üìä Divs class='g' trouv√©s: {len(result_divs)}")

            if not result_divs:
                # Fallback: chercher les h3 (titres de r√©sultats)
                result_divs = soup.find_all("div", class_=lambda x: x and "Gx5Zad" in x)
                print(f"üìä Fallback divs trouv√©s: {len(result_divs)}")

            # Si toujours rien, chercher directement les h3
            if not result_divs:
                h3_tags = soup.find_all("h3")
                print(f"üìä H3 tags trouv√©s: {len(h3_tags)}")

                # Essayer d'utiliser les h3 directement
                for h3 in h3_tags[: self.max_results]:
                    parent = h3.parent
                    while parent and parent.name != "a":
                        parent = parent.parent

                    if parent and parent.name == "a":
                        url = parent.get("href", "")
                        if url.startswith("/url?q="):
                            url = url.split("/url?q=")[1].split("&")[0]

                        if url.startswith("http"):
                            results.append(
                                {
                                    "title": h3.get_text(strip=True),
                                    "snippet": h3.get_text(strip=True),
                                    "url": url,
                                    "source": "Google (Cloudscraper - h3 fallback)",
                                }
                            )

                if results:
                    print(
                        f"‚úÖ Cloudscraper Google (h3): {len(results)} r√©sultats trouv√©s"
                    )
                    return results[: self.max_results]

            for div in result_divs[: self.max_results]:
                try:
                    # Titre
                    h3 = div.find("h3")
                    if not h3:
                        continue

                    title = h3.get_text(strip=True)

                    # URL
                    link = div.find("a", href=True)
                    if not link:
                        continue

                    url = link.get("href", "")

                    # Nettoyer l'URL Google
                    if url.startswith("/url?q="):
                        url = url.split("/url?q=")[1].split("&")[0]

                    # Snippet
                    snippet_div = div.find(
                        "div", class_=lambda x: x and ("VwiC3b" in x or "IsZvec" in x)
                    )
                    snippet = snippet_div.get_text(strip=True) if snippet_div else ""

                    if title and url.startswith("http"):
                        results.append(
                            {
                                "title": title,
                                "snippet": snippet if snippet else title,
                                "url": url,
                                "source": "Google (Cloudscraper)",
                            }
                        )
                except Exception:
                    continue

            if results:
                print(f"‚úÖ Cloudscraper Google: {len(results)} r√©sultats trouv√©s")
                return results[: self.max_results]

        except Exception as e:
            print(f"‚ö†Ô∏è Cloudscraper Google √©chou√©: {str(e)}")

        return []

    def _search_duckduckgo_instant(self, query: str) -> List[Dict[str, Any]]:
        """Recherche via l'API instant de DuckDuckGo - AM√âLIOR√âE"""
        url = "https://api.duckduckgo.com/"
        params = {
            "q": query,
            "format": "json",
            "no_redirect": "1",
            "no_html": "1",
            "skip_disambig": "1",
        }

        headers = {"User-Agent": self.user_agent}

        response = requests.get(
            url, params=params, headers=headers, timeout=self.timeout
        )
        response.raise_for_status()

        data = response.json()
        results = []

        # R√©sultat instant (Abstract)
        if data.get("Abstract") and len(data.get("Abstract", "")) > 20:
            results.append(
                {
                    "title": data.get("Heading", "Information"),
                    "snippet": data.get("Abstract", ""),
                    "url": data.get("AbstractURL", ""),
                    "source": "DuckDuckGo Instant",
                }
            )

        # R√©sultats de topics (augment√© √† max_results au lieu de 3)
        for topic in data.get("RelatedTopics", []):
            if isinstance(topic, dict) and topic.get("Text"):
                # Extraire un titre propre depuis l'URL ou le texte
                first_url = topic.get("FirstURL", "")
                if first_url:
                    # Extraire le dernier segment de l'URL comme titre
                    title = (
                        first_url.split("/")[-1].replace("_", " ").replace("%20", " ")
                    )
                else:
                    # Utiliser les 50 premiers caract√®res du texte comme titre
                    title = topic.get("Text", "")[:50] + "..."

                results.append(
                    {
                        "title": title,
                        "snippet": topic.get("Text", ""),
                        "url": first_url,
                        "source": "DuckDuckGo",
                    }
                )
            elif isinstance(topic, dict) and topic.get("Topics"):
                # RelatedTopics peut contenir des sous-topics (pour les cat√©gories)
                for subtopic in topic.get("Topics", []):
                    if subtopic.get("Text"):
                        first_url = subtopic.get("FirstURL", "")
                        title = (
                            first_url.split("/")[-1]
                            .replace("_", " ")
                            .replace("%20", " ")
                            if first_url
                            else subtopic.get("Text", "")[:50]
                        )

                        results.append(
                            {
                                "title": title,
                                "snippet": subtopic.get("Text", ""),
                                "url": first_url,
                                "source": "DuckDuckGo",
                            }
                        )

        # Limiter au nombre max de r√©sultats
        final_results = results[: self.max_results]

        if final_results:
            print(
                f"‚úÖ DuckDuckGo Instant API: {len(final_results)} r√©sultats (Abstract: {bool(data.get('Abstract'))}, Topics: {len(data.get('RelatedTopics', []))})"
            )

        return final_results

    def _search_searxng(self, query: str) -> List[Dict[str, Any]]:
        """
        Recherche via SearXNG (meta-moteur open-source avec API JSON)
        Utilise des instances publiques - pas de scraping, vraie API
        """
        # Liste d'instances SearXNG publiques fiables
        searxng_instances = [
            "https://search.bus-hit.me",
            "https://searx.be",
            "https://search.ononoki.org",
            "https://searx.work",
        ]

        for instance_url in searxng_instances:
            try:
                # SearXNG accepte les requ√™tes JSON
                api_url = f"{instance_url}/search"

                params = {
                    "q": query,
                    "format": "json",
                    "language": "fr",
                    "safesearch": 0,
                }

                headers = {
                    "User-Agent": self.user_agent,
                    "Accept": "application/json",
                }

                response = requests.get(
                    api_url, params=params, headers=headers, timeout=10
                )

                # Si cette instance ne r√©pond pas, essayer la suivante
                if response.status_code != 200:
                    continue

                data = response.json()
                results = []

                # SearXNG retourne les r√©sultats dans data['results']
                for item in data.get("results", [])[: self.max_results]:
                    title = item.get("title", "")
                    url = item.get("url", "")
                    content = item.get("content", "") or item.get("snippet", "")

                    if title and url:
                        results.append(
                            {
                                "title": title,
                                "snippet": content,
                                "url": url,
                                "source": f"SearXNG ({instance_url})",
                            }
                        )

                if results:
                    print(
                        f"‚úÖ SearXNG ({instance_url}): {len(results)} r√©sultats trouv√©s"
                    )
                    return results

            except Exception as e:
                # Si cette instance √©choue, essayer la suivante
                print(f"‚ö†Ô∏è SearXNG instance {instance_url} √©chou√©e: {str(e)}")
                continue

        # Aucune instance n'a fonctionn√©
        return []

    def _search_with_requests(self, query: str) -> List[Dict[str, Any]]:
        """Recherche en scrapant les r√©sultats DuckDuckGo HTML - VERSION AM√âLIOR√âE"""
        # Essayer d'abord DuckDuckGo Lite (plus simple √† scraper)
        try:
            results = self._search_duckduckgo_lite(query)
            if results:
                return results
        except Exception as e:
            print(f"‚ö†Ô∏è DuckDuckGo Lite a √©chou√©: {str(e)}")

        # Fallback vers DuckDuckGo HTML classique
        search_url = f"https://html.duckduckgo.com/html/?q={quote(query)}"

        # Utiliser un user-agent rotatif
        current_ua = self._get_next_user_agent()

        headers = {
            "User-Agent": current_ua,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
            "Accept-Language": "fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7",
            "Accept-Encoding": "gzip, deflate, br",
            "DNT": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Cache-Control": "max-age=0",
        }

        # Ajouter un petit d√©lai pour √©viter le rate limiting
        time.sleep(0.5)

        response = requests.get(
            search_url, headers=headers, timeout=self.timeout, allow_redirects=True
        )
        response.raise_for_status()

        # CORRECTION: Forcer l'encodage UTF-8 pour √©viter les probl√®mes de d√©codage
        response.encoding = "utf-8"

        soup = BeautifulSoup(response.text, "html.parser")
        results = []

        # Essayer plusieurs s√©lecteurs CSS pour plus de robustesse
        result_selectors = [
            ("div", {"class": "result"}),
            ("div", {"class": "results_links"}),
            ("div", {"class": "web-result"}),
            ("div", {"id": lambda x: x and x.startswith("r1-")}),
        ]

        for selector_tag, selector_attrs in result_selectors:
            result_divs = soup.find_all(
                selector_tag, selector_attrs, limit=self.max_results
            )
            if result_divs:
                break

        for result_div in result_divs:
            try:
                # Essayer diff√©rents s√©lecteurs pour le titre
                title_elem = (
                    result_div.find("a", class_="result__a")
                    or result_div.find("a", class_="result-link")
                    or result_div.find("h2").find("a")
                    if result_div.find("h2")
                    else None
                )

                # Essayer diff√©rents s√©lecteurs pour le snippet
                snippet_elem = (
                    result_div.find("a", class_="result__snippet")
                    or result_div.find("div", class_="result__snippet")
                    or result_div.find("span", class_="result-snippet")
                    or result_div.find("div", class_="snippet")
                )

                if title_elem:
                    title = title_elem.get_text(strip=True)
                    url = title_elem.get("href", "")

                    # Le snippet peut √™tre dans plusieurs endroits
                    if snippet_elem:
                        snippet = snippet_elem.get_text(strip=True)
                    else:
                        # Fallback: prendre tout le texte du div
                        snippet = result_div.get_text(strip=True)[:200]

                    if title and snippet and len(snippet) > 20:
                        results.append(
                            {
                                "title": title,
                                "snippet": snippet,
                                "url": url,
                                "source": "DuckDuckGo",
                            }
                        )
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur lors du parsing d'un r√©sultat: {str(e)}")
                continue

        return results[: self.max_results]

    def _search_duckduckgo_lite(self, query: str) -> List[Dict[str, Any]]:
        """Recherche via DuckDuckGo Lite (version simplifi√©e et plus stable)"""
        search_url = "https://lite.duckduckgo.com/lite/"

        current_ua = self._get_next_user_agent()

        headers = {
            "User-Agent": current_ua,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "fr-FR,fr;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "DNT": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Content-Type": "application/x-www-form-urlencoded",
        }

        data = {"q": query, "kl": "fr-fr"}

        response = requests.post(
            search_url,
            headers=headers,
            data=data,
            timeout=self.timeout,
            allow_redirects=True,
        )
        response.raise_for_status()

        # CORRECTION: Forcer l'encodage UTF-8 pour √©viter les probl√®mes de d√©codage
        response.encoding = "utf-8"

        # Utiliser response.text au lieu de response.content pour avoir du texte d√©j√† d√©cod√©
        soup = BeautifulSoup(response.text, "html.parser")
        results = []

        # DuckDuckGo Lite utilise une structure de table simple
        result_tables = soup.find_all("table", class_="result-table")

        for table in result_tables[: self.max_results]:
            try:
                # Le titre est dans un lien avec class="result-link"
                title_elem = table.find("a", class_="result-link")

                # Le snippet est dans un td avec class="result-snippet"
                snippet_elem = table.find("td", class_="result-snippet")

                if title_elem:
                    title = title_elem.get_text(strip=True)
                    url = title_elem.get("href", "")

                    if snippet_elem:
                        snippet = snippet_elem.get_text(strip=True)
                    else:
                        snippet = ""

                    if title and len(title) > 3:
                        results.append(
                            {
                                "title": title,
                                "snippet": snippet if snippet else title,
                                "url": url,
                                "source": "DuckDuckGo Lite",
                            }
                        )
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur lors du parsing DuckDuckGo Lite: {str(e)}")
                continue

        print(f"‚úÖ DuckDuckGo Lite: {len(results)} r√©sultats trouv√©s")
        return results[: self.max_results]

    def _search_brave(self, query: str) -> List[Dict[str, Any]]:
        """Recherche via Brave Search (moteur alternatif sans API key)"""
        search_url = f"https://search.brave.com/search?q={quote(query)}"

        current_ua = self._get_next_user_agent()

        headers = {
            "User-Agent": current_ua,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "fr-FR,fr;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate",
            "DNT": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }

        time.sleep(0.5)  # √âviter le rate limiting

        response = requests.get(
            search_url, headers=headers, timeout=self.timeout, allow_redirects=True
        )
        response.raise_for_status()

        # Forcer l'encodage UTF-8
        response.encoding = "utf-8"

        soup = BeautifulSoup(response.text, "html.parser")
        results = []

        # Brave Search utilise des divs avec class="snippet"
        result_divs = soup.find_all("div", class_="snippet", limit=self.max_results * 2)

        for div in result_divs:
            try:
                # Ignorer les publicit√©s
                if div.has_attr("data-advertiser-id"):
                    continue

                # Chercher le titre
                title_elem = div.find("div", class_="title")

                # Chercher le lien
                link_elem = div.find("a")

                # Chercher le snippet
                snippet_elem = div.find("div", class_="content")

                if title_elem and link_elem:
                    title = title_elem.get_text(strip=True)
                    url = link_elem.get("href", "")

                    snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""

                    if title and len(title) > 3 and url.startswith("http"):
                        results.append(
                            {
                                "title": title,
                                "snippet": snippet if snippet else title,
                                "url": url,
                                "source": "Brave Search",
                            }
                        )

                if len(results) >= self.max_results:
                    break
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur lors du parsing Brave Search: {str(e)}")
                continue

        print(f"‚úÖ Brave Search: {len(results)} r√©sultats trouv√©s")
        return results[: self.max_results]

    def _search_google_html(self, query: str) -> List[Dict[str, Any]]:
        """Recherche Google via scraping HTML simple"""
        search_url = f"https://www.google.com/search?q={quote(query)}&hl=fr"

        current_ua = self._get_next_user_agent()

        headers = {
            "User-Agent": current_ua,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "fr-FR,fr;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "DNT": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Referer": "https://www.google.com/",
        }

        time.sleep(1)  # D√©lai plus long pour Google

        response = requests.get(
            search_url, headers=headers, timeout=self.timeout, allow_redirects=True
        )
        response.raise_for_status()

        # Forcer l'encodage UTF-8
        response.encoding = "utf-8"

        soup = BeautifulSoup(response.text, "html.parser")
        results = []

        # Google utilise des divs avec class contenant "g" pour les r√©sultats
        # Essayer plusieurs s√©lecteurs pour robustesse
        result_divs = soup.find_all("div", class_="g", limit=self.max_results)

        # Fallback si premier s√©lecteur ne fonctionne pas
        if not result_divs:
            result_divs = soup.find_all(
                "div", attrs={"data-sokoban-container": True}, limit=self.max_results
            )

        # Autre fallback
        if not result_divs:
            result_divs = soup.select("div#search div.tF2Cxc", limit=self.max_results)

        for div in result_divs:
            try:
                # Titre - chercher dans h3 ou lien
                title_elem = div.find("h3") or div.find("a")

                # URL - chercher le premier lien
                link_elem = div.find("a", href=True)

                # Snippet - chercher les spans ou divs de description
                snippet_elem = (
                    div.find(
                        "span",
                        class_=lambda x: x and ("st" in x.lower() or "aCOpRe" in x),
                    )
                    or div.find(
                        "div",
                        class_=lambda x: x
                        and ("VwiC3b" in x or "snippet" in x.lower()),
                    )
                    or div.find("div", attrs={"data-sncf": "1"})
                )

                if title_elem and link_elem:
                    title = title_elem.get_text(strip=True)
                    url = link_elem.get("href", "")

                    # Nettoyer l'URL Google (enlever les redirections /url?q=)
                    if url.startswith("/url?q="):
                        url = url.split("/url?q=")[1].split("&")[0]

                    snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""

                    if title and len(title) > 3 and url.startswith("http"):
                        results.append(
                            {
                                "title": title,
                                "snippet": snippet if snippet else title,
                                "url": url,
                                "source": "Google Search",
                            }
                        )
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur lors du parsing Google: {str(e)}")
                continue

        print(f"‚úÖ Google Search: {len(results)} r√©sultats trouv√©s")
        return results[: self.max_results]

    def _search_fallback(self, query: str) -> List[Dict[str, Any]]:
        """M√©thode de recherche de secours - UTILISE WIKIPEDIA"""
        print("üîÑ Tentative de recherche de secours avec Wikipedia...")

        try:
            # Essayer d'abord Wikipedia fran√ßais
            results = self._search_wikipedia_fr(query)
            if results:
                print(f"‚úÖ {len(results)} r√©sultats trouv√©s via Wikipedia FR")
                return results

        except Exception as e:
            print(f"‚ö†Ô∏è Recherche Wikipedia FR a √©chou√©: {str(e)}")

        try:
            # Fallback vers Wikipedia anglais
            results = self._search_wikipedia_en(query)
            if results:
                print(f"‚úÖ {len(results)} r√©sultats trouv√©s via Wikipedia EN")
                return results

        except Exception as e:
            print(f"‚ö†Ô∏è Recherche Wikipedia EN a √©chou√©: {str(e)}")

        # Si tout √©choue, retourner un message informatif
        return [
            {
                "title": f"Recherche sur '{query}'",
                "snippet": f"Je n'ai pas pu acc√©der aux moteurs de recherche pour '{query}'. V√©rifiez votre connexion internet ou reformulez votre question.",
                "url": "",
                "source": "Syst√®me local",
            }
        ]

    def _search_wikipedia_fr(self, query: str) -> List[Dict[str, Any]]:
        """Recherche sur Wikipedia fran√ßais avec CONTENU COMPLET et correction orthographique"""
        try:
            # Utiliser l'API Wikipedia
            api_url = "https://fr.wikipedia.org/w/api.php"

            # √âtape 0: Demander une suggestion orthographique √† Wikipedia
            suggestion_params = {
                "action": "opensearch",
                "search": query,
                "limit": 1,
                "format": "json",
            }

            headers = {"User-Agent": self.user_agent}

            print(f"üîç Recherche de suggestion pour: '{query}'")

            suggestion_response = requests.get(
                api_url, params=suggestion_params, headers=headers, timeout=10
            )
            suggestion_response.raise_for_status()
            suggestion_data = suggestion_response.json()

            # La r√©ponse est [query, [suggestions], [descriptions], [urls]]
            suggested_query = query
            if len(suggestion_data) > 1 and suggestion_data[1]:
                suggested_query = suggestion_data[1][0]  # Premi√®re suggestion
                if suggested_query.lower() != query.lower():
                    print(f"‚úèÔ∏è Wikipedia sugg√®re: '{query}' ‚Üí '{suggested_query}'")

            # √âtape 1: Rechercher les pages pertinentes avec la suggestion
            # D√©tecter les noms propres (mots significatifs en majuscules OU mots de >=4 lettres dans question)
            potential_names = []
            for word in query.split():
                clean_word = word.strip("?.,!;:")
                # Majuscule OU mot long dans question de type "burj khalifa"
                if len(clean_word) >= 4 and clean_word.lower() not in [
                    "quel",
                    "quelle",
                    "comment",
                    "taille",
                    "hauteur",
                    "fait",
                    "mesure",
                ]:
                    potential_names.append(clean_word)

            has_proper_noun = (
                len(potential_names) >= 2
            )  # Au moins 2 mots significatifs (ex: "burj khalifa")
            search_limit = 5 if has_proper_noun else 3

            if has_proper_noun:
                direct_search_term = " ".join(potential_names)
                print(
                    f"üéØ Entit√© d√©tect√©e: '{direct_search_term}', recherche √©largie √† {search_limit} pages + recherche directe"
                )

            search_params = {
                "action": "query",
                "list": "search",
                "srsearch": suggested_query,
                "format": "json",
                "srlimit": search_limit,  # Ajust√© dynamiquement
            }

            print(f"üåê Requ√™te Wikipedia FR avec: '{suggested_query}'")

            # Initialiser la liste des r√©sultats
            results = []

            # NOUVELLE √âTAPE 1.5: Si entit√© d√©tect√©e, chercher DIRECTEMENT l'article sp√©cifique
            if has_proper_noun:
                direct_article_title = (
                    direct_search_term.title()
                )  # "burj khalifa" ‚Üí "Burj Khalifa"
                print(
                    f"üéØ Tentative de r√©cup√©ration directe de l'article: '{direct_article_title}'"
                )

                # D'abord, chercher une suggestion de Wikipedia si l'orthographe est incorrecte
                article_title_to_fetch = direct_article_title

                try:
                    # Utiliser l'API opensearch pour obtenir des suggestions
                    opensearch_params = {
                        "action": "opensearch",
                        "search": direct_search_term,
                        "limit": 5,
                        "namespace": 0,
                        "format": "json",
                    }

                    opensearch_response = requests.get(
                        api_url, params=opensearch_params, headers=headers, timeout=10
                    )
                    opensearch_response.raise_for_status()
                    opensearch_data = opensearch_response.json()

                    # Format de la r√©ponse: [query, [titles], [descriptions], [urls]]
                    if len(opensearch_data) > 1 and opensearch_data[1]:
                        suggestions = opensearch_data[1]
                        if suggestions:
                            # Calculer la similarit√© avec fuzzy matching
                            best_match = None
                            best_score = 0

                            for suggestion in suggestions:
                                score = fuzz.ratio(
                                    direct_search_term.lower(), suggestion.lower()
                                )
                                if score > best_score:
                                    best_score = score
                                    best_match = suggestion

                            # Si on a un bon match (score > 70), l'utiliser
                            if best_match and best_score > 70:
                                if best_match.lower() != direct_article_title.lower():
                                    print(
                                        f"üìù Correction orthographique: '{direct_article_title}' ‚Üí '{best_match}' (score: {best_score})"
                                    )
                                    article_title_to_fetch = best_match
                                else:
                                    print(
                                        f"‚úì Orthographe correcte confirm√©e (score: {best_score})"
                                    )
                except Exception as e:
                    print(f"‚ö†Ô∏è Suggestion orthographique √©chou√©e: {str(e)}")

                try:
                    direct_content_params = {
                        "action": "query",
                        "titles": article_title_to_fetch,
                        "prop": "extracts",
                        "exintro": False,
                        "explaintext": True,
                        "format": "json",
                    }

                    direct_response = requests.get(
                        api_url,
                        params=direct_content_params,
                        headers=headers,
                        timeout=10,
                    )
                    direct_response.raise_for_status()
                    direct_data = direct_response.json()

                    direct_pages = direct_data.get("query", {}).get("pages", {})
                    for page_id, page_data in direct_pages.items():
                        if page_id != "-1":  # -1 signifie page not found
                            extract = page_data.get("extract", "")
                            if extract and len(extract) > 100:
                                print(
                                    f"‚úÖ Article direct trouv√©: {len(extract)} caract√®res"
                                )
                                results.append(
                                    {
                                        "title": page_data.get(
                                            "title", article_title_to_fetch
                                        ),
                                        "snippet": extract[:500],
                                        "full_content": extract,
                                        "url": f"https://fr.wikipedia.org/wiki/{quote(page_data.get('title', article_title_to_fetch))}",
                                        "source": "Wikipedia FR (Article direct)",
                                    }
                                )
                                break
                except Exception as e:
                    print(f"‚ö†Ô∏è Recherche directe √©chou√©e: {str(e)}")

            response = requests.get(
                api_url, params=search_params, headers=headers, timeout=10
            )

            print(f"üì° Status code: {response.status_code}")

            response.raise_for_status()

            search_data = response.json()

            print(f"üìä R√©ponse JSON re√ßue: {len(str(search_data))} caract√®res")

            if "query" in search_data and "search" in search_data["query"]:
                print(
                    f"üîç Wikipedia FR: {len(search_data['query']['search'])} pages trouv√©es"
                )

                # √âtape 2: Pour chaque page trouv√©e, r√©cup√©rer le contenu complet
                for item in search_data["query"]["search"]:
                    title = item.get("title", "")

                    if not title:
                        continue

                    print(f"üìñ R√©cup√©ration du contenu de: {title}")

                    try:
                        # R√©cup√©rer le contenu complet de la page (pas seulement l'intro)
                        content_params = {
                            "action": "query",
                            "titles": title,
                            "prop": "extracts",
                            "exintro": False,  # R√©cup√©rer l'article complet, pas juste l'intro
                            "explaintext": True,  # Texte brut sans HTML
                            "format": "json",
                            "exsectionformat": "plain",  # Format plat pour toutes les sections
                            # Pas de limite exchars - prendre TOUT le contenu disponible
                        }

                        content_response = requests.get(
                            api_url, params=content_params, headers=headers, timeout=10
                        )
                        content_response.raise_for_status()
                        content_data = content_response.json()

                        # Extraire le contenu
                        pages = content_data.get("query", {}).get("pages", {})
                        for page_data in pages.values():
                            extract = page_data.get("extract", "")

                            if extract and len(extract) > 50:
                                print(f"‚úÖ Contenu r√©cup√©r√©: {len(extract)} caract√®res")
                                results.append(
                                    {
                                        "title": title,
                                        "snippet": extract[
                                            :500
                                        ],  # Premier 500 caract√®res
                                        "full_content": extract,  # Contenu complet
                                        "url": f"https://fr.wikipedia.org/wiki/{quote(title)}",
                                        "source": "Wikipedia FR",
                                    }
                                )
                                break  # Un seul r√©sultat par page
                            else:
                                print(f"‚ö†Ô∏è Contenu vide ou trop court pour: {title}")

                    except Exception as e:
                        print(
                            f"‚ùå Erreur lors de la r√©cup√©ration du contenu de '{title}': {str(e)}"
                        )
                        continue

            print(f"‚úÖ Wikipedia FR: {len(results)} r√©sultats avec contenu complet")

            # üî• NOUVEAU: Filtrer et scorer les r√©sultats par pertinence AVANT de les retourner
            filtered_results = self._filter_and_score_wikipedia_results(results, query)

            return filtered_results[: self.max_results]

        except Exception as e:
            print(f"‚ùå Erreur globale Wikipedia FR: {str(e)}")
            traceback.print_exc()
            return []

    def _filter_and_score_wikipedia_results(
        self, results: List[Dict[str, Any]], query: str
    ) -> List[Dict[str, Any]]:
        """
        Filtre et score les r√©sultats Wikipedia par pertinence √† la query.
        Utilise un scoring RAPIDE et intelligent bas√© sur l'analyse des mots-cl√©s,
        sans appel LLM pour maximiser la performance.

        Args:
            results: Liste des r√©sultats Wikipedia bruts
            query: Query originale de l'utilisateur

        Returns:
            Liste tri√©e des r√©sultats pertinents
        """
        if not results:
            return results

        print(f"üîç Scoring rapide g√©n√©rique de {len(results)} r√©sultats Wikipedia...")

        query_lower = query.lower().strip()
        stopwords = {
            "le",
            "la",
            "les",
            "de",
            "des",
            "du",
            "un",
            "une",
            "et",
            "ou",
            "en",
            "sur",
            "dans",
            "pour",
            "avec",
            "sans",
            "par",
            "que",
            "qui",
            "quoi",
            "comment",
            "cherche",
            "recherche",
            "trouve",
            "internet",
            "web",
            "google",
        }
        query_terms = {
            word
            for word in re.findall(r"\w+", query_lower, flags=re.UNICODE)
            if len(word) > 2 and word not in stopwords
        }

        wants_recent = any(
            word in query_lower
            for word in [
                "dernier",
                "derni√®re",
                "derniers",
                "derni√®res",
                "r√©cent",
                "r√©cente",
                "r√©cents",
                "r√©centes",
                "latest",
                "recent",
            ]
        )
        query_years = {
            int(year) for year in re.findall(r"\b(19\d{2}|20\d{2})\b", query_lower)
        }

        all_tokens = []
        for result in results:
            title = (result.get("title") or "").lower()
            snippet = (result.get("snippet") or "").lower()
            all_tokens.extend(
                set(
                    token
                    for token in re.findall(
                        r"\w+", f"{title} {snippet}", flags=re.UNICODE
                    )
                    if len(token) > 2 and token not in stopwords
                )
            )
        token_frequency = Counter(all_tokens)

        scored_results = []
        for result in results:
            title = result.get("title", "").lower()
            snippet = result.get("snippet", "").lower()

            title_terms = {
                token
                for token in re.findall(r"\w+", title, flags=re.UNICODE)
                if len(token) > 2 and token not in stopwords
            }
            snippet_terms = {
                token
                for token in re.findall(r"\w+", snippet, flags=re.UNICODE)
                if len(token) > 2 and token not in stopwords
            }
            page_terms = title_terms.union(snippet_terms)

            overlap_title = query_terms.intersection(title_terms)
            overlap_all = query_terms.intersection(page_terms)

            coverage = len(overlap_all) / max(len(query_terms), 1)
            title_coverage = len(overlap_title) / max(len(query_terms), 1)

            score = 0.0
            score += coverage * 360
            score += title_coverage * 340
            score += fuzz.partial_ratio(query_lower, title) * 1.5

            rarity_bonus = 0.0
            for token in overlap_title:
                rarity_bonus += 1.0 / max(token_frequency.get(token, 1), 1)
            score += rarity_bonus * 220

            title_years = {
                int(year) for year in re.findall(r"\b(19\d{2}|20\d{2})\b", title)
            }
            if query_years:
                if query_years.intersection(title_years):
                    score += 180
                elif title_years:
                    score -= 120
            elif wants_recent and title_years:
                newest_year = max(title_years)
                score += max(0, (newest_year - 2018) * 16)

            if len(title_terms) < 2 and len(snippet_terms) < 3:
                score -= 60

            # P√©nalit√© pour snippets trop courts (pseudo-pages vides)
            snippet_len = len(snippet)
            if snippet_len < 80:
                score -= 200
            else:
                score += min(snippet_len / 40, 80)

            score = int(score)

            scored_results.append((score, result, title))

        # Trier par score d√©croissant
        scored_results.sort(reverse=True, key=lambda x: x[0])

        # Afficher les scores et filtrer
        filtered = []
        min_score = 180  # Seuil minimum de pertinence (g√©n√©rique)

        for score, result, title in scored_results:
            if score >= min_score:
                print(f"‚úÖ Score {score:4d}: {result.get('title')}")
                filtered.append(result)
            else:
                print(f"‚ùå Score {score:4d}: {result.get('title')} (√©limin√©)")

        # Si aucun r√©sultat au-dessus du seuil, prendre les 3 meilleurs
        if not filtered and scored_results:
            print(f"‚ö†Ô∏è Aucun r√©sultat >= {min_score}, prise des 3 meilleurs")
            filtered = [result for _, result, _ in scored_results[:3]]

        print(f"üìä {len(filtered)} r√©sultats retenus sur {len(results)}")
        return filtered[: self.max_results]

    def _search_wikipedia_en(self, query: str) -> List[Dict[str, Any]]:
        """Recherche sur Wikipedia anglais avec CONTENU COMPLET et correction orthographique"""
        try:
            # Utiliser l'API Wikipedia
            api_url = "https://en.wikipedia.org/w/api.php"

            # √âtape 0: Demander une suggestion orthographique √† Wikipedia
            suggestion_params = {
                "action": "opensearch",
                "search": query,
                "limit": 1,
                "format": "json",
            }

            headers = {"User-Agent": self.user_agent}

            print(f"üîç Recherche de suggestion pour: '{query}'")

            suggestion_response = requests.get(
                api_url, params=suggestion_params, headers=headers, timeout=10
            )
            suggestion_response.raise_for_status()
            suggestion_data = suggestion_response.json()

            # La r√©ponse est [query, [suggestions], [descriptions], [urls]]
            suggested_query = query
            if len(suggestion_data) > 1 and suggestion_data[1]:
                suggested_query = suggestion_data[1][0]  # Premi√®re suggestion
                if suggested_query.lower() != query.lower():
                    print(f"‚úèÔ∏è Wikipedia sugg√®re: '{query}' ‚Üí '{suggested_query}'")

            # √âtape 1: Rechercher les pages pertinentes avec la suggestion
            # D√©tecter les noms propres (mots significatifs en majuscules OU mots de >=4 lettres dans question)
            potential_names = []
            for word in query.split():
                clean_word = word.strip("?.,!;:")
                # Majuscule OU mot long dans question de type "burj khalifa"
                if len(clean_word) >= 4 and clean_word.lower() not in [
                    "what",
                    "which",
                    "size",
                    "height",
                    "tall",
                    "quel",
                    "quelle",
                    "comment",
                    "taille",
                    "hauteur",
                    "fait",
                    "mesure",
                ]:
                    potential_names.append(clean_word)

            has_proper_noun = (
                len(potential_names) >= 2
            )  # Au moins 2 mots significatifs (ex: "burj khalifa")
            search_limit = 5 if has_proper_noun else 3

            if has_proper_noun:
                direct_search_term = " ".join(potential_names)
                print(
                    f"üéØ Entit√© d√©tect√©e: '{direct_search_term}', recherche √©largie √† {search_limit} pages + recherche directe"
                )

            search_params = {
                "action": "query",
                "list": "search",
                "srsearch": suggested_query,
                "format": "json",
                "srlimit": search_limit,  # Ajust√© dynamiquement
            }

            print(f"üåê Requ√™te Wikipedia EN avec: '{suggested_query}'")

            # Initialiser la liste des r√©sultats
            results = []

            # NOUVELLE √âTAPE 1.5: Si entit√© d√©tect√©e, chercher DIRECTEMENT l'article sp√©cifique
            if has_proper_noun:
                direct_article_title = (
                    direct_search_term.title()
                )  # "burj khalifa" ‚Üí "Burj Khalifa"
                print(
                    f"üéØ Tentative de r√©cup√©ration directe de l'article EN: '{direct_article_title}'"
                )

                # D'abord, chercher une suggestion de Wikipedia si l'orthographe est incorrecte
                article_title_to_fetch = direct_article_title

                try:
                    # Utiliser l'API opensearch pour obtenir des suggestions
                    opensearch_params = {
                        "action": "opensearch",
                        "search": direct_search_term,
                        "limit": 5,
                        "namespace": 0,
                        "format": "json",
                    }

                    opensearch_response = requests.get(
                        api_url, params=opensearch_params, headers=headers, timeout=10
                    )
                    opensearch_response.raise_for_status()
                    opensearch_data = opensearch_response.json()

                    # Format de la r√©ponse: [query, [titles], [descriptions], [urls]]
                    if len(opensearch_data) > 1 and opensearch_data[1]:
                        suggestions = opensearch_data[1]
                        if suggestions:
                            # Calculer la similarit√© avec fuzzy matching
                            best_match = None
                            best_score = 0

                            for suggestion in suggestions:
                                score = fuzz.ratio(
                                    direct_search_term.lower(), suggestion.lower()
                                )
                                if score > best_score:
                                    best_score = score
                                    best_match = suggestion

                            # Si on a un bon match (score > 70), l'utiliser
                            if best_match and best_score > 70:
                                if best_match.lower() != direct_article_title.lower():
                                    print(
                                        f"üìù Correction orthographique EN: '{direct_article_title}' ‚Üí '{best_match}' (score: {best_score})"
                                    )
                                    article_title_to_fetch = best_match
                                else:
                                    print(
                                        f"‚úì Orthographe correcte confirm√©e EN (score: {best_score})"
                                    )
                except Exception as e:
                    print(f"‚ö†Ô∏è Suggestion orthographique EN √©chou√©e: {str(e)}")

                try:
                    direct_content_params = {
                        "action": "query",
                        "titles": article_title_to_fetch,
                        "prop": "extracts",
                        "exintro": False,
                        "explaintext": True,
                        "format": "json",
                    }

                    direct_response = requests.get(
                        api_url,
                        params=direct_content_params,
                        headers=headers,
                        timeout=10,
                    )
                    direct_response.raise_for_status()
                    direct_data = direct_response.json()

                    direct_pages = direct_data.get("query", {}).get("pages", {})
                    for page_id, page_data in direct_pages.items():
                        if page_id != "-1":  # -1 signifie page not found
                            extract = page_data.get("extract", "")
                            if extract and len(extract) > 100:
                                print(
                                    f"‚úÖ Article direct EN trouv√©: {len(extract)} caract√®res"
                                )
                                results.append(
                                    {
                                        "title": page_data.get(
                                            "title", article_title_to_fetch
                                        ),
                                        "snippet": extract[:500],
                                        "full_content": extract,
                                        "url": f"https://en.wikipedia.org/wiki/{quote(page_data.get('title', article_title_to_fetch))}",
                                        "source": "Wikipedia EN (Article direct)",
                                    }
                                )
                                break
                except Exception as e:
                    print(f"‚ö†Ô∏è Recherche directe EN √©chou√©e: {str(e)}")

            response = requests.get(
                api_url, params=search_params, headers=headers, timeout=10
            )

            print(f"üì° Status code: {response.status_code}")

            response.raise_for_status()

            search_data = response.json()

            print(f"üìä R√©ponse JSON re√ßue: {len(str(search_data))} caract√®res")

            results = []

            if "query" in search_data and "search" in search_data["query"]:
                print(
                    f"üîç Wikipedia EN: {len(search_data['query']['search'])} pages trouv√©es"
                )

                # √âtape 2: Pour chaque page trouv√©e, r√©cup√©rer le contenu complet
                for item in search_data["query"]["search"]:
                    title = item.get("title", "")

                    if not title:
                        continue

                    print(f"üìñ R√©cup√©ration du contenu de: {title}")

                    try:
                        # R√©cup√©rer le contenu complet de la page (pas seulement l'intro)
                        content_params = {
                            "action": "query",
                            "titles": title,
                            "prop": "extracts",
                            "exintro": False,  # CHANG√â: R√©cup√©rer l'article complet, pas juste l'intro
                            "explaintext": True,  # Texte brut sans HTML
                            "format": "json",
                            "exsectionformat": "plain",  # Format plat pour toutes les sections
                            # Pas de limite exchars - prendre TOUT le contenu disponible
                        }

                        content_response = requests.get(
                            api_url, params=content_params, headers=headers, timeout=10
                        )
                        content_response.raise_for_status()
                        content_data = content_response.json()

                        # Extraire le contenu
                        pages = content_data.get("query", {}).get("pages", {})
                        for _, page_data in pages.items():
                            extract = page_data.get("extract", "")

                            if extract and len(extract) > 50:
                                print(f"‚úÖ Contenu r√©cup√©r√©: {len(extract)} caract√®res")
                                results.append(
                                    {
                                        "title": title,
                                        "snippet": extract[
                                            :500
                                        ],  # Premier 500 caract√®res
                                        "full_content": extract,  # Contenu complet
                                        "url": f"https://en.wikipedia.org/wiki/{quote(title)}",
                                        "source": "Wikipedia EN",
                                    }
                                )
                                break  # Un seul r√©sultat par page
                            else:
                                print(f"‚ö†Ô∏è Contenu vide ou trop court pour: {title}")

                    except Exception as e:
                        print(
                            f"‚ùå Erreur lors de la r√©cup√©ration du contenu de '{title}': {str(e)}"
                        )
                        continue

            print(f"‚úÖ Wikipedia EN: {len(results)} r√©sultats avec contenu complet")
            # Filtrer par pertinence avant de retourner
            filtered_results = self._filter_and_score_wikipedia_results(results, query)
            return filtered_results[: self.max_results]

        except Exception as e:
            print(f"‚ùå Erreur globale Wikipedia EN: {str(e)}")
            traceback.print_exc()
            return []

    def _extract_page_contents(
        self, search_results: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """Extrait le contenu des pages"""
        enriched_results = []

        def extract_single_page(result):
            try:
                if not result.get("url") or not result["url"].startswith("http"):
                    return result

                headers = {"User-Agent": self.user_agent}
                response = requests.get(result["url"], headers=headers, timeout=7)
                response.raise_for_status()

                soup = BeautifulSoup(response.content, "html.parser")

                # Supprimer les scripts et styles
                for element in soup(
                    ["script", "style", "nav", "footer", "header", "aside"]
                ):
                    element.decompose()

                # Extraire le texte principal
                main_content = ""

                # Priorit√© aux balises principales
                for tag in [
                    "main",
                    "article",
                    '[role="main"]',
                    ".content",
                    ".article",
                    ".post",
                ]:
                    content_elem = soup.select_one(tag)
                    if content_elem:
                        main_content = content_elem.get_text(separator=" ", strip=True)
                        break

                # Fallback sur les paragraphes
                if not main_content:
                    paragraphs = soup.find_all("p")
                    main_content = " ".join(
                        [p.get_text(strip=True) for p in paragraphs[:10]]
                    )

                # Limiter la longueur
                if len(main_content) > self.max_content_length:
                    main_content = main_content[: self.max_content_length] + "..."

                result["full_content"] = main_content

            except Exception as e:
                print(
                    f"‚ö†Ô∏è Impossible d'extraire le contenu de {result.get('url', 'URL inconnue')}: {str(e)}"
                )
                result["full_content"] = result.get("snippet", "")

            return result

        # Traitement parall√®le
        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
            enriched_results = list(executor.map(extract_single_page, search_results))

        return enriched_results

    def _generate_cache_key(self, query: str) -> str:
        """G√©n√®re une cl√© de cache pour la requ√™te"""
        return f"search_{hash(query.lower().strip())}"

    def _is_cache_valid(self, cache_key: str) -> bool:
        """V√©rifie si le cache est encore valide"""
        if cache_key not in self.search_cache:
            return False

        cache_time = self.search_cache[cache_key]["timestamp"]
        return (time.time() - cache_time) < self.cache_duration

    def _cache_results(
        self, cache_key: str, summary: str, results: List[Dict[str, Any]]
    ):
        """Met en cache les r√©sultats"""
        self.search_cache[cache_key] = {
            "summary": summary,
            "results": results,
            "timestamp": time.time(),
        }

        # Nettoyer le cache ancien (garder max 20 entr√©es)
        if len(self.search_cache) > 20:
            oldest_key = min(
                self.search_cache.keys(),
                key=lambda k: self.search_cache[k]["timestamp"],
            )
            del self.search_cache[oldest_key]

    def get_search_history(self) -> List[str]:
        """Retourne l'historique des recherches r√©centes"""
        history = []
        for data in sorted(
            self.search_cache.items(), key=lambda x: x[1]["timestamp"], reverse=True
        ):
            if data.get("results") and data["results"]:
                history.append(f"Recherche r√©cente - {len(data['results'])} r√©sultats")

        return history[:10]

    def summarize_url(self, url: str) -> str:
        """
        R√©cup√®re et r√©sume le contenu d'une URL directe

        Args:
            url: L'URL de la page √† r√©sumer

        Returns:
            str: Un r√©sum√© format√© du contenu de la page
        """
        try:
            print(f"üåê R√©cup√©ration de la page: {url}")

            # V√©rifier le cache
            cache_key = f"url_{hash(url)}"
            if self._is_cache_valid(cache_key):
                print("üìã Contenu trouv√© en cache")
                return self.search_cache[cache_key]["summary"]

            # R√©cup√©rer le contenu de la page
            headers = {
                "User-Agent": self.user_agent,
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "fr-FR,fr;q=0.5",
            }

            response = requests.get(url, headers=headers, timeout=self.timeout)
            response.raise_for_status()

            # Parser le HTML
            soup = BeautifulSoup(response.content, "html.parser")

            # Extraire le titre
            title = soup.title.string if soup.title else "Page Web"
            title = self._clean_title(title)

            # Supprimer les √©l√©ments non pertinents
            for element in soup(
                [
                    "script",
                    "style",
                    "nav",
                    "footer",
                    "header",
                    "aside",
                    "iframe",
                    "noscript",
                ]
            ):
                element.decompose()

            # Extraire le contenu principal
            main_content = ""

            # Essayer de trouver le contenu principal avec diff√©rentes strat√©gies
            for selector in [
                "main",
                "article",
                '[role="main"]',
                ".content",
                ".article",
                ".post",
                ".entry-content",
                "#content",
            ]:
                content_elem = soup.select_one(selector)
                if content_elem:
                    main_content = content_elem.get_text(separator="\n", strip=True)
                    break

            # Fallback: extraire tous les paragraphes
            if not main_content or len(main_content) < 100:
                paragraphs = soup.find_all("p")
                main_content = "\n".join(
                    [
                        p.get_text(strip=True)
                        for p in paragraphs
                        if len(p.get_text(strip=True)) > 30
                    ]
                )

            # Si toujours pas de contenu, extraire tout le texte du body
            if not main_content or len(main_content) < 100:
                body = soup.find("body")
                if body:
                    main_content = body.get_text(separator="\n", strip=True)

            # Nettoyer le contenu
            lines = main_content.split("\n")
            cleaned_lines = []
            for line in lines:
                line = line.strip()
                # Garder seulement les lignes avec du contenu significatif
                if len(line) > 20 and not line.startswith(
                    ("¬©", "Cookie", "JavaScript")
                ):
                    cleaned_lines.append(line)

            main_content = "\n".join(cleaned_lines)

            # V√©rifier qu'on a du contenu
            if not main_content or len(main_content) < 50:
                return "‚ùå Impossible d'extraire le contenu de cette page. L'URL est peut-√™tre prot√©g√©e ou le format n'est pas support√©."

            # G√©n√©rer le r√©sum√©
            summary = self._generate_url_summary(title, url, main_content)

            # Mettre en cache
            self.search_cache[cache_key] = {
                "summary": summary,
                "timestamp": time.time(),
            }

            return summary

        except requests.exceptions.Timeout:
            return f"‚è±Ô∏è La requ√™te a pris trop de temps. L'URL ne r√©pond pas assez rapidement: {url}"
        except requests.exceptions.ConnectionError:
            return f"üîå Impossible de se connecter √† cette URL. V√©rifiez votre connexion internet et que l'URL est correcte: {url}"
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 404:
                return f"üîç Page non trouv√©e (erreur 404). Cette URL n'existe pas ou plus: {url}"
            elif e.response.status_code == 403:
                return f"üö´ Acc√®s refus√© (erreur 403). Cette page bloque les acc√®s automatis√©s: {url}"
            else:
                return (
                    f"‚ùå Erreur HTTP {e.response.status_code} lors de l'acc√®s √†: {url}"
                )
        except Exception as e:
            print(f"‚ùå Erreur lors du r√©sum√© de l'URL: {str(e)}")
            return f"‚ùå Une erreur s'est produite lors de la r√©cup√©ration de cette page: {str(e)}"

    def _generate_url_summary(self, title: str, url: str, content: str) -> str:
        """
        G√©n√®re un r√©sum√© structur√© du contenu d'une URL

        Args:
            title: Titre de la page
            url: URL de la page
            content: Contenu extrait de la page

        Returns:
            str: R√©sum√© format√© en markdown
        """
        # Limiter le contenu pour l'analyse
        if len(content) > 5000:
            content_for_analysis = content[:5000]
        else:
            content_for_analysis = content

        # Extraire les points cl√©s
        sentences = re.split(r"[.!?]+", content_for_analysis)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 30]

        # Prendre les premi√®res phrases significatives comme r√©sum√©
        summary_sentences = sentences[:5] if len(sentences) >= 5 else sentences

        # Construire le r√©sum√©
        summary = f"üìÑ **{title}**"
        summary += f" ({url})\n\n"
        summary += "üìù **R√©sum√© du contenu:**\n\n"

        for i, sentence in enumerate(summary_sentences, 1):
            # Nettoyer la phrase
            sentence = self._universal_word_spacing_fix(sentence)
            summary += f"{i}. {sentence}.\n\n"

        # Ajouter statistiques
        word_count = len(content.split())
        summary += f"\nüìä **Statistiques:** {word_count} mots, {len(sentences)} phrases analys√©es\n"

        # Extraire des mots-cl√©s importants
        words = content.lower().split()
        # Filtrer les mots courts et communs
        stop_words = {
            "le",
            "la",
            "les",
            "un",
            "une",
            "des",
            "de",
            "du",
            "et",
            "ou",
            "mais",
            "pour",
            "dans",
            "sur",
            "avec",
            "est",
            "sont",
            "qui",
            "que",
            "ce",
            "se",
            "ne",
            "pas",
            "plus",
            "comme",
            "tout",
            "nous",
            "vous",
            "leur",
            "leurs",
            "son",
            "sa",
            "ses",
        }
        keywords = [w for w in words if len(w) > 4 and w not in stop_words]

        # Compter les occurrences
        keyword_counts = Counter(keywords)
        top_keywords = [kw for kw, count in keyword_counts.most_common(5)]

        if top_keywords:
            summary += f"üè∑Ô∏è **Mots-cl√©s:** {', '.join(top_keywords)}\n\n\n"

        return summary


# Alias pour remplacer l'ancienne classe
InternetSearchEngine = EnhancedInternetSearchEngine
