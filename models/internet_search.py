"""
Module de recherche internet avec rÃ©sumÃ© intelligent
Permet de rechercher des informations sur internet et de les synthÃ©tiser
"""

import requests
import re
import time
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import json
from urllib.parse import quote, urljoin
from bs4 import BeautifulSoup
import concurrent.futures
import threading


class InternetSearchEngine:
    """Moteur de recherche internet avec synthÃ¨se intelligente"""
    
    def __init__(self):
        self.search_apis = {
            "duckduckgo": "https://api.duckduckgo.com/",
            "bing": "https://api.bing.microsoft.com/v7.0/search",
            "google_custom": "https://www.googleapis.com/customsearch/v1"
        }
        
        # Configuration
        self.max_results = 5
        self.max_content_length = 2000
        self.timeout = 10
        self.user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        
        # Cache des rÃ©sultats rÃ©cents
        self.search_cache = {}
        self.cache_duration = 3600  # 1 heure
        
    def search_and_summarize(self, query: str, context: Dict[str, Any] = None) -> str:
        """
        Recherche sur internet et gÃ©nÃ¨re un rÃ©sumÃ© intelligent
        
        Args:
            query: RequÃªte de recherche
            context: Contexte additionnel pour la recherche
            
        Returns:
            str: RÃ©sumÃ© intelligent des rÃ©sultats
        """
        try:
            print(f"ğŸ” Recherche internet pour: '{query}'")
            
            # VÃ©rifier le cache
            cache_key = self._generate_cache_key(query)
            if self._is_cache_valid(cache_key):
                print("ğŸ“‹ RÃ©sultats trouvÃ©s en cache")
                return self.search_cache[cache_key]["summary"]
            
            # Effectuer la recherche
            search_results = self._perform_search(query)
            
            if not search_results:
                return f"âŒ DÃ©solÃ©, je n'ai pas pu trouver d'informations sur '{query}'. VÃ©rifiez votre connexion internet."
            
            # Extraire le contenu des pages
            page_contents = self._extract_page_contents(search_results)
            
            # GÃ©nÃ©rer le rÃ©sumÃ©
            summary = self._generate_intelligent_summary(query, page_contents, search_results)
            
            # Mettre en cache
            self._cache_results(cache_key, summary, search_results)
            
            return summary
            
        except Exception as e:
            print(f"âŒ Erreur lors de la recherche: {str(e)}")
            return f"DÃ©solÃ©, une erreur s'est produite lors de la recherche internet : {str(e)}"
    
    def _perform_search(self, query: str) -> List[Dict[str, Any]]:
        """
        Effectue la recherche sur internet
        
        Args:
            query: RequÃªte de recherche
            
        Returns:
            List[Dict]: Liste des rÃ©sultats de recherche
        """
        # Essayer diffÃ©rentes mÃ©thodes de recherche
        search_methods = [
            self._search_duckduckgo_instant,
            self._search_with_requests,
            self._search_fallback
        ]
        
        for method in search_methods:
            try:
                results = method(query)
                if results:
                    print(f"âœ… {len(results)} rÃ©sultats trouvÃ©s avec {method.__name__}")
                    return results
            except Exception as e:
                print(f"âš ï¸ {method.__name__} a Ã©chouÃ©: {str(e)}")
                continue
        
        return []
    
    def _search_duckduckgo_instant(self, query: str) -> List[Dict[str, Any]]:
        """
        Recherche via l'API instant de DuckDuckGo
        
        Args:
            query: RequÃªte de recherche
            
        Returns:
            List[Dict]: RÃ©sultats de recherche
        """
        url = "https://api.duckduckgo.com/"
        params = {
            "q": query,
            "format": "json",
            "no_redirect": "1",
            "no_html": "1",
            "skip_disambig": "1"
        }
        
        headers = {"User-Agent": self.user_agent}
        
        response = requests.get(url, params=params, headers=headers, timeout=self.timeout)
        response.raise_for_status()
        
        data = response.json()
        results = []
        
        # RÃ©sultat instant
        if data.get("Abstract"):
            results.append({
                "title": data.get("Heading", "Information"),
                "snippet": data.get("Abstract", ""),
                "url": data.get("AbstractURL", ""),
                "source": "DuckDuckGo Instant"
            })
        
        # RÃ©sultats de topics
        for topic in data.get("RelatedTopics", [])[:3]:
            if isinstance(topic, dict) and topic.get("Text"):
                results.append({
                    "title": topic.get("FirstURL", "").split("/")[-1].replace("_", " ").title(),
                    "snippet": topic.get("Text", ""),
                    "url": topic.get("FirstURL", ""),
                    "source": "DuckDuckGo"
                })
        
        return results[:self.max_results]
    
    def _search_with_requests(self, query: str) -> List[Dict[str, Any]]:
        """
        Recherche en scrapant les rÃ©sultats de moteurs de recherche
        
        Args:
            query: RequÃªte de recherche
            
        Returns:
            List[Dict]: RÃ©sultats de recherche
        """
        # Recherche sur DuckDuckGo HTML
        search_url = f"https://html.duckduckgo.com/html/?q={quote(query)}"
        
        headers = {
            "User-Agent": self.user_agent,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "fr-FR,fr;q=0.5",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
        }
        
        response = requests.get(search_url, headers=headers, timeout=self.timeout)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        results = []
        
        # Extraire les rÃ©sultats
        for result_div in soup.find_all('div', class_='result')[:self.max_results]:
            title_elem = result_div.find('a', class_='result__a')
            snippet_elem = result_div.find('a', class_='result__snippet')
            
            if title_elem and snippet_elem:
                title = title_elem.get_text(strip=True)
                url = title_elem.get('href', '')
                snippet = snippet_elem.get_text(strip=True)
                
                if title and snippet:
                    results.append({
                        "title": title,
                        "snippet": snippet,
                        "url": url,
                        "source": "DuckDuckGo"
                    })
        
        return results
    
    def _search_fallback(self, query: str) -> List[Dict[str, Any]]:
        """
        MÃ©thode de recherche de secours - simulation de rÃ©sultats
        
        Args:
            query: RequÃªte de recherche
            
        Returns:
            List[Dict]: RÃ©sultats simulÃ©s
        """
        # En cas d'Ã©chec des autres mÃ©thodes, fournir une rÃ©ponse constructive
        return [{
            "title": f"Recherche sur '{query}'",
            "snippet": f"Je n'ai pas pu accÃ©der aux moteurs de recherche pour '{query}'. VÃ©rifiez votre connexion internet ou reformulez votre question.",
            "url": "",
            "source": "SystÃ¨me local"
        }]
    
    def _extract_page_contents(self, search_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Extrait le contenu des pages des rÃ©sultats de recherche
        
        Args:
            search_results: RÃ©sultats de recherche
            
        Returns:
            List[Dict]: Contenus extraits
        """
        enriched_results = []
        
        def extract_single_page(result):
            try:
                if not result.get("url") or not result["url"].startswith("http"):
                    return result
                
                headers = {"User-Agent": self.user_agent}
                response = requests.get(result["url"], headers=headers, timeout=5)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Supprimer les scripts et styles
                for element in soup(["script", "style", "nav", "footer", "header"]):
                    element.decompose()
                
                # Extraire le texte principal
                main_content = ""
                
                # PrioritÃ© aux balises principales
                for tag in ['main', 'article', 'div[class*="content"]', 'div[class*="article"]']:
                    content_elem = soup.select_one(tag)
                    if content_elem:
                        main_content = content_elem.get_text(separator=" ", strip=True)
                        break
                
                # Fallback sur tout le body
                if not main_content:
                    body = soup.find('body')
                    if body:
                        main_content = body.get_text(separator=" ", strip=True)
                
                # Limiter la longueur
                if len(main_content) > self.max_content_length:
                    main_content = main_content[:self.max_content_length] + "..."
                
                result["full_content"] = main_content
                
            except Exception as e:
                print(f"âš ï¸ Impossible d'extraire le contenu de {result.get('url', 'URL inconnue')}: {str(e)}")
                result["full_content"] = result.get("snippet", "")
            
            return result
        
        # Traitement parallÃ¨le pour accÃ©lÃ©rer l'extraction
        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
            enriched_results = list(executor.map(extract_single_page, search_results))
        
        return enriched_results
    
    def _generate_intelligent_summary(self, query: str, page_contents: List[Dict[str, Any]], 
                                    original_results: List[Dict[str, Any]]) -> str:
        """
        GÃ©nÃ¨re un rÃ©sumÃ© intelligent des rÃ©sultats de recherche
        
        Args:
            query: RequÃªte originale
            page_contents: Contenus extraits des pages
            original_results: RÃ©sultats de recherche originaux
            
        Returns:
            str: RÃ©sumÃ© intelligent
        """
        import re
        # Construire le rÃ©sumÃ©
        summary = f"ğŸ” **RÃ©sultats de recherche pour '{query}'**\n\n"

        # Extraire toutes les phrases des snippets et contenus
        all_sentences = []
        for result in page_contents:
            for field in ["snippet", "full_content"]:
                text = result.get(field, "")
                if text:
                    # DÃ©couper en phrases
                    sentences = re.split(r'(?<=[.!?])\s+', text)
                    all_sentences.extend([s.strip() for s in sentences if len(s.strip()) > 0])

        # Calculer la similaritÃ© entre la question et chaque phrase (heuristique simple)
        def score(sentence, query):
            # BasÃ© sur le nombre de mots en commun (en ignorant la casse et la ponctuation)
            import string
            s_words = set(w.strip(string.punctuation).lower() for w in sentence.split())
            q_words = set(w.strip(string.punctuation).lower() for w in query.split())
            # Bonus si la phrase commence par une rÃ©ponse directe (ex: "La taille de la tour Eiffel est ...")
            direct_answer_bonus = 0
            if any(w in s_words for w in q_words):
                if s_words and list(s_words)[0] in q_words:
                    direct_answer_bonus = 2
            return len(s_words & q_words) + direct_answer_bonus

        # Nouvelle logique : consensus multi-sources
        import string
        from collections import Counter, defaultdict

        def normalize_sentence(s):
            # Minuscule, sans ponctuation, sans espaces multiples
            s = s.lower()
            s = s.translate(str.maketrans('', '', string.punctuation))
            s = ' '.join(s.split())
            return s

        def extract_numbers(s):
            return set(re.findall(r'\d+', s))

        q_words = set(w.lower() for w in query.split() if len(w) > 2)
        # Filtrer les phrases candidates
        candidates = []
        for s in all_sentences:
            if len(s.split()) < 8:
                continue
            if not any(w in s.lower() for w in q_words):
                continue
            candidates.append(s)

        # Grouper par similaritÃ© (mots communs et chiffres communs)
        groups = defaultdict(list)
        for s in candidates:
            norm = normalize_sentence(s)
            nums = tuple(sorted(extract_numbers(norm)))
            # ClÃ© = mots triÃ©s + chiffres trouvÃ©s
            key = tuple(sorted(set(norm.split()) & q_words)) + nums
            groups[key].append(s)

        # Prendre le groupe le plus frÃ©quent (plusieurs sources disent la mÃªme chose)
        if groups:
            best_group = max(groups.values(), key=len)
            # Dans ce groupe, prendre la phrase la plus longue
            best_sentence = max(best_group, key=len)
            summary += f"{best_sentence}\n\n"
        else:
            # Fallback : prendre la phrase la plus longue contenant au moins un mot de la question
            fallback = [s for s in all_sentences if any(w in s.lower() for w in q_words)]
            if fallback:
                best_sentence = max(fallback, key=lambda s: len(s))
                summary += f"{best_sentence}\n\n"
            else:
                summary += "Aucune rÃ©ponse directe trouvÃ©e.\n\n"

        # Ajouter les sources principales
        summary += "ğŸ”— **Sources principales :**\n"
        source_count = 0
        for result in page_contents[:3]:  # Top 3 rÃ©sultats
            if result.get("title") and result.get("snippet"):
                source_count += 1
                title = result["title"][:100] + "..." if len(result["title"]) > 100 else result["title"]
                snippet = result["snippet"][:150] + "..." if len(result["snippet"]) > 150 else result["snippet"]

                # Lien cliquable en Markdown uniquement (pas de HTML)
                url = result.get("url")
                if url and url.startswith("http"):
                    markdown_link = f'[{title}]({url})'
                else:
                    markdown_link = f'**{title}**'

                summary += f"\n{source_count}. {markdown_link}\n"
                summary += f"   {snippet}\n"

        return summary
    
    def _extract_key_information(self, page_contents: List[Dict[str, Any]], query: str) -> str:
        """
        Extrait les informations clÃ©s des contenus de pages
        
        Args:
            page_contents: Contenus des pages
            query: RequÃªte originale
            
        Returns:
            str: Informations clÃ©s extraites
        """
        # Combiner tous les contenus
        all_content = ""
        for content in page_contents:
            snippet = content.get("snippet", "")
            full_content = content.get("full_content", "")
            all_content += f"{snippet} {full_content} "
        
        if not all_content.strip():
            return "Aucune information dÃ©taillÃ©e disponible."
        
        # Extraire les phrases les plus pertinentes
        sentences = re.split(r'[.!?]+', all_content)
        relevant_sentences = []
        
        query_words = set(query.lower().split())
        
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 20:  # Ignorer les phrases trop courtes
                sentence_words = set(sentence.lower().split())
                
                # Calculer la pertinence
                relevance_score = len(query_words.intersection(sentence_words))
                
                if relevance_score > 0:
                    relevant_sentences.append((sentence, relevance_score))
        
        # Trier par pertinence et prendre les meilleures
        relevant_sentences.sort(key=lambda x: x[1], reverse=True)
        
        key_info = ""
        for sentence, score in relevant_sentences[:3]:  # Top 3 phrases
            key_info += f"â€¢ {sentence.strip()}\n"
        
        return key_info if key_info else "Informations gÃ©nÃ©rales trouvÃ©es mais nÃ©cessitent une recherche plus spÃ©cifique."
    
    def _generate_search_suggestions(self, query: str) -> str:
        """
        GÃ©nÃ¨re des suggestions de recherche
        
        Args:
            query: RequÃªte originale
            
        Returns:
            str: Suggestions formatÃ©es
        """
        suggestions = []
        query_lower = query.lower()
        
        # Suggestions basÃ©es sur le type de requÃªte
        if any(word in query_lower for word in ["comment", "how"]):
            suggestions.extend([
                f"{query} Ã©tapes",
                f"{query} tutoriel",
                f"{query} guide complet"
            ])
        elif any(word in query_lower for word in ["qu'est", "what is", "dÃ©finition"]):
            suggestions.extend([
                f"{query} exemples",
                f"{query} explication simple",
                f"avantages {query}"
            ])
        elif any(word in query_lower for word in ["meilleur", "best"]):
            suggestions.extend([
                f"{query} 2025",
                f"{query} comparatif",
                f"{query} avis"
            ])
        else:
            suggestions.extend([
                f"{query} actualitÃ©s",
                f"{query} guide",
                f"{query} exemples"
            ])
        
        if suggestions:
            return "â€¢ " + "\nâ€¢ ".join(suggestions[:3])
        
        return ""
    
    def _generate_cache_key(self, query: str) -> str:
        """GÃ©nÃ¨re une clÃ© de cache pour la requÃªte"""
        return f"search_{hash(query.lower().strip())}"
    
    def _is_cache_valid(self, cache_key: str) -> bool:
        """VÃ©rifie si le cache est encore valide"""
        if cache_key not in self.search_cache:
            return False
        
        cache_time = self.search_cache[cache_key]["timestamp"]
        return (time.time() - cache_time) < self.cache_duration
    
    def _cache_results(self, cache_key: str, summary: str, results: List[Dict[str, Any]]):
        """Met en cache les rÃ©sultats"""
        self.search_cache[cache_key] = {
            "summary": summary,
            "results": results,
            "timestamp": time.time()
        }
        
        # Nettoyer le cache ancien (garder max 20 entrÃ©es)
        if len(self.search_cache) > 20:
            oldest_key = min(self.search_cache.keys(), 
                           key=lambda k: self.search_cache[k]["timestamp"])
            del self.search_cache[oldest_key]
    
    def get_search_history(self) -> List[str]:
        """
        Retourne l'historique des recherches rÃ©centes
        
        Returns:
            List[str]: Historique des recherches
        """
        history = []
        for cache_key, data in sorted(self.search_cache.items(), 
                                    key=lambda x: x[1]["timestamp"], reverse=True):
            # Reconstituer la requÃªte Ã  partir des rÃ©sultats
            if data.get("results") and data["results"]:
                history.append(f"Recherche rÃ©cente - {len(data['results'])} rÃ©sultats")
        
        return history[:10]  # 10 derniÃ¨res recherches
