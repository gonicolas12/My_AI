# Configuration de l'IA Personnelle

# ====================================
# CONFIGURATION G√âN√âRALE
# ====================================
ai:
  name: "My Personal AI"
  version: "6.5.0"
  description: "IA personnelle locale compl√®te"
  
  # Param√®tres g√©n√©raux
  max_tokens: 131072
  temperature: 0.7
  timeout: 120
  
  # Historique des conversations
  conversation_history_limit: 100
  save_conversations: true
  
  # Types de fichiers support√©s
  supported_file_types:
    - ".pdf"
    - ".docx"
    - ".doc"
    - ".txt"
    - ".py"
    - ".html"
    - ".css"
    - ".js"
    - ".json"
    - ".xml"
    - ".md"

# ====================================
# MOD√àLES DE LANGAGE
# ====================================
llm:
  # Backend par d√©faut (local uniquement)
  default_backend: "local"
  
  # Configuration locale
  local:
    model_type: "advanced_local_ai"
    knowledge_base: "advanced_knowledge.json"
    enable_learning: true
    conversation_memory: 10
    base_url: "http://localhost:11434"
    default_model: "llama3.2"
    timeout: 60
    available_models:
      - "llama3.2"
      - "llama3.2:13b"
      - "codellama"
      - "mistral"
  
  # Configuration Transformers
  transformers:
    default_model: "microsoft/DialoGPT-medium"
    device: "auto"  # auto, cpu, cuda
    cache_dir: "models_cache"
    models:
      conversation:
        - "microsoft/DialoGPT-medium"
        - "microsoft/DialoGPT-large"
      code:
        - "Salesforce/codegen-350M-mono"
        - "microsoft/CodeBERT-base"

# ====================================
# TRAITEMENT DE FICHIERS
# ====================================
file_processing:
  # Taille maximale des fichiers (en MB)
  max_file_size_mb: 50
  
  # R√©pertoires
  temp_directory: "temp"
  backup_directory: "backups"
  output_directory: "outputs"
  
  # PDF
  pdf:
    preferred_library: "pymupdf"  # pymupdf, pypdf2
    extract_images: false
    extract_tables: true
  
  # DOCX
  docx:
    preserve_formatting: true
    extract_tables: true
    extract_images: false

# ====================================
# G√âN√âRATION DE CONTENU
# ====================================
generation:
  # R√©pertoire de sortie
  output_directory: "outputs"
  
  # Templates de code
  code_templates:
    python:
      include_docstrings: true
      include_type_hints: true
      style: "pep8"
    
    javascript:
      style: "modern"  # modern, classic
      include_jsdoc: true
    
    html:
      include_css: true
      include_javascript: true
      responsive: true
  
  # Documents
  documents:
    default_format: "pdf"  # pdf, docx, txt
    include_metadata: true
    include_timestamp: true
    
    pdf:
      page_size: "A4"
      margins: 20  # mm
      font_family: "Arial"
      font_size: 12
    
    docx:
      style_template: "default"
      include_toc: false

# ====================================
# INTERFACE UTILISATEUR
# ====================================
ui:
  # CLI
  cli:
    prompt: "ü§ñ MyAI> "
    show_typing_indicator: true
    max_display_width: 80
    colors:
      primary: "blue"
      success: "green"
      error: "red"
      warning: "yellow"
  
  # GUI (futur)
  gui:
    title: "My Personal AI Assistant"
    theme: "light"  # light, dark
    window_size: "800x600"

# ====================================
# S√âCURIT√â
# ====================================
security:
  # Validation des entr√©es
  validate_inputs: true
  max_query_length: 10000
  
  # Fichiers
  scan_uploads: true
  allowed_extensions:
    - ".pdf"
    - ".docx"
    - ".txt"
    - ".py"
    - ".html"
    - ".css"
    - ".js"
  
  # Contenu malveillant
  block_scripts: true
  block_exec_commands: true

# ====================================
# LOGGING
# ====================================
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  directory: "logs"
  max_file_size_mb: 10
  backup_count: 5
  
  # Format des logs
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  date_format: "%Y-%m-%d %H:%M:%S"
  
  # Composants √† logger
  components:
    ai_engine: true
    llm_manager: true
    file_processor: true
    conversation: true

# ====================================
# PERFORMANCE
# ====================================
performance:
  # Cache
  enable_cache: true
  cache_size_mb: 100
  cache_ttl_hours: 24
  
  # Traitement parall√®le
  max_workers: 4
  chunk_size: 1024
  
  # M√©moire
  max_memory_usage_mb: 2048
  gc_threshold: 1000

# ====================================
# D√âVELOPPEMENT
# ====================================
development:
  debug_mode: false
  profiling: false
  
  # Tests
  test_data_directory: "tests/data"
  mock_llm_responses: false
  
  # M√©triques
  collect_metrics: true
  metrics_file: "logs/metrics.json"


# === OPTIMISATIONS AVANCEES ===
optimization:
  # RAG (Retrieval-Augmented Generation)
  rag:
    enabled: true
    chunk_size: 512
    chunk_overlap: 50
    max_retrieved_chunks: 3
    embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
    vector_store_path: "./data/vector_store"
    
  # Context Window Optimization
  context_optimization:
    enabled: true
    max_context_tokens: 8192
    compression_enabled: true
    sliding_window_size: 4096
    memory_efficient_attention: true
    
  # Fine-tuning
  fine_tuning:
    enabled: false  # Activer manuellement
    model_base: "microsoft/DialoGPT-medium"
    lora_r: 8
    lora_alpha: 16
    lora_dropout: 0.1
    learning_rate: 2e-5
    batch_size: 4
    num_epochs: 3
    
  # Performance Monitoring
  monitoring:
    enabled: true
    log_performance_metrics: true
    benchmark_interval: 3600  # 1 heure
    max_memory_usage_mb: 2048
    
  # Cache System
  cache:
    enabled: true
    embedding_cache_size: 1000
    response_cache_size: 500
    cache_ttl: 3600  # 1 heure

# ====================================
# MODEL CONTEXT PROTOCOL (MCP)
# ====================================
mcp:
  # Outils locaux (toujours actifs, pas de d√©pendance externe)
  local_tools:
    web_search: true
    search_memory: true
    read_local_file: true
    list_directory: true
    generate_code: true
    calculate: true

  # Serveurs MCP externes (optionnels, n√©cessitent Node.js via npx ou Python via uvx)
  # Mettre enabled: true pour activer, false pour ignorer
  servers:

    # Acc√®s au syst√®me de fichiers local
    # Requiert: Node.js ‚Äî npx @modelcontextprotocol/server-filesystem
    filesystem:
      enabled: false
      command: "npx"
      args: ["@modelcontextprotocol/server-filesystem", "./"]

    # Acc√®s aux d√©p√¥ts Git (lecture de code, commits, branches)
    # Requiert: Node.js ‚Äî npx @modelcontextprotocol/server-git
    git:
      enabled: false
      command: "npx"
      args: ["@modelcontextprotocol/server-git", "--repository", "."]

    # Acc√®s aux bases SQLite (m√©moire, conversations)
    # Requiert: Node.js ‚Äî npx @modelcontextprotocol/server-sqlite
    sqlite:
      enabled: false
      command: "npx"
      args: ["@modelcontextprotocol/server-sqlite", "memory/vector_store/chroma.db"]

    # Raisonnement structur√© s√©quentiel (cha√Ænes de pens√©e)
    # Requiert: Node.js ‚Äî npx @modelcontextprotocol/server-sequential-thinking
    sequential_thinking:
      enabled: false
      command: "npx"
      args: ["@modelcontextprotocol/server-sequential-thinking"]

    # Recherche Brave (alternative locale √† DuckDuckGo)
    # Requiert: cl√© API Brave + Node.js
    brave_search:
      enabled: false
      command: "npx"
      args: ["@modelcontextprotocol/server-brave-search"]
      env:
        BRAVE_API_KEY: "${BRAVE_API_KEY}"

    # Fetch HTTP (r√©cup√©ration de pages web)
    # Requiert: Node.js ‚Äî npx @modelcontextprotocol/server-fetch
    fetch:
      enabled: false
      command: "npx"
      args: ["@modelcontextprotocol/server-fetch"]

# ====================================
# CL√âS API
# ====================================
github:
  token: "${GITHUB_TOKEN}"
