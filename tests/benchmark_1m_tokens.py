"""
üìä BENCHMARK PROFESSIONNEL SYST√àME 1M TOKENS
My Personal AI Ultra v5.0.0 - Analyse de Performance Compl√®te

Mesures pr√©cises des performances pour validation industrielle
"""

import sys
import time
import json
import statistics
from pathlib import Path
from datetime import datetime
import gc

# Configuration du chemin
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

# Imports du syst√®me Ultra
try:
    from models.ultra_custom_ai import UltraCustomAIModel
    from models.intelligent_context_manager import UltraIntelligentContextManager
    from core.ai_engine import AIEngine
    ULTRA_AVAILABLE = True
except ImportError as e:
    ULTRA_AVAILABLE = False
    print(f"‚ùå Syst√®me Ultra non disponible: {e}")

class Benchmark1M:
    """Benchmark professionnel du syst√®me 1M tokens"""
    
    def __init__(self):
        self.results = {
            "benchmark_date": datetime.now().isoformat(),
            "system_version": "My Personal AI Ultra v5.0.0",
            "test_environment": "Production Ready",
            "performance_metrics": {},
            "scalability_analysis": {},
            "memory_efficiency": {},
            "professional_grade": "PENDING"
        }
    
    def generate_benchmark_content(self, size_tokens: int) -> str:
        """G√©n√®re du contenu optimis√© pour benchmark"""
        # Contenu technique r√©aliste
        base_content = """
        Analyse de performance syst√®me IA ultra-avanc√© avec capacit√© √©tendue de traitement.
        Impl√©mentation d'algorithmes de compression et d'indexation s√©mantique optimis√©s.
        Architecture modulaire permettant scalabilit√© horizontale et verticale.
        Gestionnaire de contexte intelligent avec √©viction automatique des donn√©es anciennes.
        Syst√®me de chunks adaptatifs avec d√©duplication et compression en temps r√©el.
        Indexation vectorielle pour recherche s√©mantique ultra-rapide dans corpus massifs.
        Pipeline de traitement distribu√© avec gestion d'erreurs et r√©cup√©ration automatique.
        Interface API RESTful pour int√©gration enterprise avec authentification et monitoring.
        Base de donn√©es vectorielle optimis√©e pour requ√™tes de similarit√© √† grande √©chelle.
        Machine learning int√©gr√© pour am√©lioration continue des performances syst√®me.
        """
        
        # Calculer le nombre de r√©p√©titions n√©cessaires
        base_tokens = len(base_content.split())
        repetitions = max(1, size_tokens // base_tokens)
        
        return (base_content + " ") * repetitions
    
    def benchmark_storage_performance(self) -> dict:
        """Benchmark des performances de stockage"""
        print("üìä BENCHMARK STOCKAGE")
        print("-" * 30)
        
        storage_results = {}
        test_sizes = [1000, 5000, 10000, 25000, 50000, 100000, 250000]
        
        context_mgr = UltraIntelligentContextManager()
        
        for size in test_sizes:
            print(f"  üì¶ Test {size:,} tokens...")
            
            # G√©n√©rer contenu
            content = self.generate_benchmark_content(size)
            
            # Mesures multiples pour pr√©cision
            times = []
            for i in range(3):
                gc.collect()  # Nettoyage m√©moire pour mesures pr√©cises
                
                start = time.perf_counter()
                chunk_ids = context_mgr.add_ultra_content(
                    content, 
                    content_type=f"benchmark_{size}_{i}",
                    importance_level="medium"
                )
                elapsed = time.perf_counter() - start
                times.append(elapsed)
            
            # Statistiques
            avg_time = statistics.mean(times)
            std_dev = statistics.stdev(times) if len(times) > 1 else 0
            tokens_per_sec = size / avg_time
            
            storage_results[size] = {
                "avg_time_seconds": round(avg_time, 4),
                "std_deviation": round(std_dev, 4),
                "tokens_per_second": round(tokens_per_sec, 0),
                "chunks_created": len(chunk_ids),
                "efficiency_score": round(tokens_per_sec / 1000, 2)  # Score sur 1000 tokens/sec
            }
            
            print(f"    ‚è±Ô∏è  {avg_time:.3f}s ¬± {std_dev:.3f}s")
            print(f"    ‚ö° {tokens_per_sec:,.0f} tokens/sec")
        
        return storage_results
    
    def benchmark_search_performance(self) -> dict:
        """Benchmark des performances de recherche"""
        print("\nüîç BENCHMARK RECHERCHE")
        print("-" * 30)
        
        search_results = {}
        context_mgr = UltraIntelligentContextManager()
        
        # Pr√©parer corpus de test
        corpus_sizes = [10000, 50000, 100000, 500000]
        search_queries = [
            "analyse performance syst√®me",
            "algorithme compression",
            "architecture modulaire",
            "indexation s√©mantique",
            "machine learning"
        ]
        
        for corpus_size in corpus_sizes:
            print(f"  üìö Corpus {corpus_size:,} tokens...")
            
            # Charger corpus
            content = self.generate_benchmark_content(corpus_size)
            context_mgr.add_ultra_content(content, content_type=f"corpus_{corpus_size}")
            
            # Tester recherches
            search_times = []
            result_counts = []
            
            for query in search_queries:
                times_for_query = []
                
                for _ in range(5):  # 5 mesures par requ√™te
                    start = time.perf_counter()
                    results = context_mgr.search_relevant_chunks(query, max_chunks=10)
                    elapsed = time.perf_counter() - start
                    times_for_query.append(elapsed)
                    result_counts.append(len(results))
                
                search_times.extend(times_for_query)
            
            # Statistiques de recherche
            avg_search_time = statistics.mean(search_times)
            avg_results = statistics.mean(result_counts)
            searches_per_sec = 1 / avg_search_time if avg_search_time > 0 else float('inf')
            
            search_results[corpus_size] = {
                "avg_search_time_ms": round(avg_search_time * 1000, 2),
                "searches_per_second": round(searches_per_sec, 1),
                "avg_results_found": round(avg_results, 1),
                "search_efficiency": round(searches_per_sec * avg_results, 1)
            }
            
            print(f"    üîç {avg_search_time*1000:.2f}ms par recherche")
            print(f"    üìà {searches_per_sec:.1f} recherches/sec")
    
        return search_results
    
    def benchmark_memory_efficiency(self) -> dict:
        """Benchmark de l'efficacit√© m√©moire"""
        print("\nüíæ BENCHMARK M√âMOIRE")
        print("-" * 30)
        
        memory_results = {}
        
        try:
            import psutil
            import os
            
            process = psutil.Process(os.getpid())
            
            # Mesure m√©moire initiale
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB
            print(f"  üìä M√©moire initiale: {initial_memory:.1f} MB")
            
            context_mgr = UltraIntelligentContextManager()
            
            # Test progression m√©moire
            test_loads = [50000, 100000, 200000, 500000]
            memory_progression = []
            
            for load in test_loads:
                # Charger donn√©es
                content = self.generate_benchmark_content(load)
                context_mgr.add_ultra_content(content, content_type=f"memory_test_{load}")
                
                # Mesurer m√©moire
                current_memory = process.memory_info().rss / 1024 / 1024
                memory_used = current_memory - initial_memory
                stats = context_mgr.get_stats()
                tokens_loaded = stats.get('total_tokens', 0)
                
                # Calculs d'efficacit√©
                mb_per_1k_tokens = (memory_used / tokens_loaded * 1000) if tokens_loaded > 0 else 0
                
                memory_progression.append({
                    "tokens_loaded": tokens_loaded,
                    "memory_used_mb": round(memory_used, 1),
                    "mb_per_1k_tokens": round(mb_per_1k_tokens, 3),
                    "efficiency_rating": "Excellent" if mb_per_1k_tokens < 0.5 else 
                                       "Good" if mb_per_1k_tokens < 1.0 else
                                       "Moderate" if mb_per_1k_tokens < 2.0 else "Poor"
                })
                
                print(f"  üìà {tokens_loaded:,} tokens: {memory_used:.1f} MB ({mb_per_1k_tokens:.3f} MB/1k tokens)")
            
            # Analyse finale m√©moire
            final_memory = process.memory_info().rss / 1024 / 1024
            total_memory_used = final_memory - initial_memory
            final_stats = context_mgr.get_stats()
            final_tokens = final_stats.get('total_tokens', 0)
            
            memory_results = {
                "initial_memory_mb": round(initial_memory, 1),
                "final_memory_mb": round(final_memory, 1), 
                "total_memory_used_mb": round(total_memory_used, 1),
                "final_tokens_loaded": final_tokens,
                "overall_efficiency_mb_per_1k": round((total_memory_used / final_tokens * 1000), 3) if final_tokens > 0 else 0,
                "memory_progression": memory_progression,
                "efficiency_grade": "A+" if total_memory_used < 100 else
                                   "A" if total_memory_used < 200 else
                                   "B" if total_memory_used < 500 else "C"
            }
            
        except ImportError:
            print("  ‚ö†Ô∏è psutil non disponible - benchmark m√©moire limit√©")
            memory_results = {"status": "unavailable", "reason": "psutil not installed"}
        
        return memory_results
    
    def benchmark_scalability(self) -> dict:
        """Benchmark de scalabilit√©"""
        print("\nüìà BENCHMARK SCALABILIT√â")
        print("-" * 30)
        
        scalability_results = {}
        context_mgr = UltraIntelligentContextManager()
        
        # Test mont√©e en charge progressive
        load_stages = [
            (10000, "Charge l√©g√®re"),
            (50000, "Charge mod√©r√©e"), 
            (100000, "Charge √©lev√©e"),
            (250000, "Charge intensive"),
            (500000, "Charge maximale"),
            (750000, "Stress test"),
            (1000000, "Limite absolue")
        ]
        
        cumulative_tokens = 0
        performance_degradation = []
        
        for tokens, description in load_stages:
            print(f"  üéØ {description}: +{tokens:,} tokens...")
            
            content = self.generate_benchmark_content(tokens)
            
            # Mesurer performance
            start = time.perf_counter()
            chunk_ids = context_mgr.add_ultra_content(content, content_type=f"scalability_{tokens}")
            add_time = time.perf_counter() - start
            
            # Mesurer recherche sous charge
            search_start = time.perf_counter()
            results = context_mgr.search_relevant_chunks("performance test", max_chunks=5)
            search_time = time.perf_counter() - search_start
            
            # Statistiques
            stats = context_mgr.get_stats()
            cumulative_tokens = stats.get('total_tokens', 0)
            tokens_per_sec = tokens / add_time if add_time > 0 else float('inf')
            
            stage_result = {
                "stage": description,
                "tokens_added": tokens,
                "cumulative_tokens": cumulative_tokens,
                "add_time_seconds": round(add_time, 3),
                "search_time_ms": round(search_time * 1000, 2),
                "tokens_per_second": round(tokens_per_sec, 0),
                "chunks_total": stats.get('total_chunks', 0),
                "utilization": stats.get('utilization', '0%')
            }
            
            performance_degradation.append(stage_result)
            
            print(f"    ‚è±Ô∏è  Ajout: {add_time:.3f}s ({tokens_per_sec:,.0f} tokens/sec)")
            print(f"    üîç Recherche: {search_time*1000:.2f}ms")
            print(f"    üìä Total: {cumulative_tokens:,} tokens ({stats.get('utilization', '0%')})")
            
            # Arr√™ter si on atteint la limite ou performance d√©grad√©e
            if cumulative_tokens >= 1000000 or add_time > 1.0:
                if cumulative_tokens >= 1000000:
                    print(f"    üèÜ LIMITE 1M TOKENS ATTEINTE!")
                break
        
        # Analyse de la scalabilit√©
        if len(performance_degradation) > 1:
            first_stage = performance_degradation[0]
            last_stage = performance_degradation[-1]
            
            performance_ratio = last_stage['tokens_per_second'] / first_stage['tokens_per_second']
            scalability_grade = (
                "Excellent" if performance_ratio > 0.8 else
                "Good" if performance_ratio > 0.6 else
                "Moderate" if performance_ratio > 0.4 else
                "Poor"
            )
        else:
            performance_ratio = 1.0
            scalability_grade = "Unknown"
        
        scalability_results = {
            "max_tokens_achieved": cumulative_tokens,
            "stages_completed": len(performance_degradation),
            "performance_ratio": round(performance_ratio, 2),
            "scalability_grade": scalability_grade,
            "stage_details": performance_degradation,
            "linear_scalability": performance_ratio > 0.8
        }
        
        return scalability_results
    
    def calculate_professional_grade(self) -> str:
        """Calcule le grade professionnel du syst√®me"""
        metrics = self.results["performance_metrics"]
        
        # Crit√®res d'√©valuation
        scores = []
        
        # Performance stockage (tokens/sec pour 100k tokens)
        storage = metrics.get("storage_performance", {})
        if 100000 in storage:
            tps = storage[100000].get("tokens_per_second", 0)
            scores.append(min(100, tps / 1000))  # 100k+ tokens/sec = 100 points
        
        # Performance recherche (recherches/sec)
        search = metrics.get("search_performance", {})
        if search:
            avg_searches_per_sec = statistics.mean([
                data.get("searches_per_second", 0) for data in search.values()
            ])
            scores.append(min(100, avg_searches_per_sec * 10))  # 10+ recherches/sec = 100 points
        
        # Scalabilit√©
        scalability = metrics.get("scalability_analysis", {})
        max_tokens = scalability.get("max_tokens_achieved", 0)
        scores.append(min(100, max_tokens / 10000))  # 1M tokens = 100 points
        
        # Efficacit√© m√©moire
        memory = metrics.get("memory_efficiency", {})
        if "overall_efficiency_mb_per_1k" in memory:
            efficiency = memory["overall_efficiency_mb_per_1k"]
            # Moins de m√©moire = meilleur score (invers√©)
            scores.append(max(0, 100 - efficiency * 20))  # < 0.5 MB/1k = score parfait
        
        # Calcul du grade final
        if scores:
            avg_score = statistics.mean(scores)
            if avg_score >= 90:
                return "A+ (Excellence Industrielle)"
            elif avg_score >= 80:
                return "A (Production Haute Performance)"
            elif avg_score >= 70:
                return "B+ (Production Standard)"
            elif avg_score >= 60:
                return "B (Pr√™t Production)"
            else:
                return "C (D√©veloppement/Test)"
        
        return "Non √âvalu√©"
    
    def run_complete_benchmark(self) -> dict:
        """Lance le benchmark complet"""
        print("üìä BENCHMARK PROFESSIONNEL SYST√àME 1M TOKENS")
        print("=" * 60)
        
        if not ULTRA_AVAILABLE:
            print("‚ùå Syst√®me Ultra non disponible")
            return self.results
        
        start_time = time.perf_counter()
        
        # Ex√©cution des benchmarks
        print("üöÄ Lancement des tests de performance...")
        
        self.results["performance_metrics"]["storage_performance"] = self.benchmark_storage_performance()
        self.results["performance_metrics"]["search_performance"] = self.benchmark_search_performance()
        self.results["performance_metrics"]["memory_efficiency"] = self.benchmark_memory_efficiency()
        self.results["performance_metrics"]["scalability_analysis"] = self.benchmark_scalability()
        
        total_time = time.perf_counter() - start_time
        
        # Calcul du grade professionnel
        professional_grade = self.calculate_professional_grade()
        self.results["professional_grade"] = professional_grade
        self.results["total_benchmark_time"] = round(total_time, 2)
        
        # Affichage du r√©sum√©
        self.display_benchmark_summary()
        
        return self.results
    
    def display_benchmark_summary(self):
        """Affiche le r√©sum√© du benchmark"""
        print("\n" + "=" * 60)
        print("üìä R√âSUM√â BENCHMARK PROFESSIONNEL")
        print("=" * 60)
        
        metrics = self.results["performance_metrics"]
        
        # Performance stockage
        storage = metrics.get("storage_performance", {})
        if storage:
            max_tps = max(data.get("tokens_per_second", 0) for data in storage.values())
            print(f"üì¶ Stockage Max: {max_tps:,.0f} tokens/sec")
        
        # Performance recherche
        search = metrics.get("search_performance", {})
        if search:
            avg_search = statistics.mean([
                data.get("searches_per_second", 0) for data in search.values()
            ])
            print(f"üîç Recherche Moy: {avg_search:.1f} recherches/sec")
        
        # Scalabilit√©
        scalability = metrics.get("scalability_analysis", {})
        max_tokens = scalability.get("max_tokens_achieved", 0)
        scalability_grade = scalability.get("scalability_grade", "Unknown")
        print(f"üìà Capacit√© Max: {max_tokens:,} tokens ({scalability_grade})")
        
        # M√©moire
        memory = metrics.get("memory_efficiency", {})
        if "efficiency_grade" in memory:
            print(f"üíæ Efficacit√© M√©moire: Grade {memory['efficiency_grade']}")
        
        # Grade final
        grade = self.results["professional_grade"]
        print(f"\nüèÜ GRADE PROFESSIONNEL: {grade}")
        
        # Temps total
        total_time = self.results.get("total_benchmark_time", 0)
        print(f"‚è±Ô∏è Temps Total: {total_time:.2f} secondes")
        
        # Recommandation
        if "A+" in grade or "A" in grade:
            print(f"\n‚úÖ RECOMMANDATION: D√âPLOIEMENT PRODUCTION IMM√âDIAT")
            print(f"üí° Syst√®me de niveau industriel confirm√©")
        elif "B" in grade:
            print(f"\n‚ö° RECOMMANDATION: SYST√àME PR√äT PRODUCTION")
            print(f"üí° Performance solide pour environnement professionnel")
        else:
            print(f"\n‚ö†Ô∏è RECOMMANDATION: OPTIMISATIONS SUGG√âR√âES")
            print(f"üí° Syst√®me fonctionnel n√©cessitant am√©liorations")

def main():
    """Benchmark principal"""
    print("üìä BENCHMARK PROFESSIONNEL SYST√àME 1M TOKENS")
    print("My Personal AI Ultra v5.0.0")
    print("=" * 60)
    
    print("Ce benchmark va mesurer pr√©cis√©ment les performances")
    print("de votre syst√®me pour validation industrielle.")
    print()
    
    choice = input("Lancer le benchmark complet? (o/n): ").lower().strip()
    
    if choice in ['o', 'oui', 'y', 'yes']:
        benchmark = Benchmark1M()
        results = benchmark.run_complete_benchmark()
        
        # Sauvegarde des r√©sultats
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"benchmark_1m_results_{timestamp}.json"
        
        with open(filename, "w") as f:
            json.dump(results, f, indent=2)
        
        print(f"\nüíæ R√©sultats sauv√©s dans '{filename}'")
        print("üéâ Benchmark professionnel termin√©!")
        
    else:
        print("Benchmark annul√©.")

if __name__ == "__main__":
    main()
