# üöÄ Documentation M√©moire Vectorielle √âtendue - v5.0.0 (3 Septembre 2025)

## üß† Vue d'Ensemble : M√©moire Interne de 1M Tokens

Le syst√®me Ultra de My Personal AI v5.0.0 impl√©mente une **m√©moire vectorielle interne pouvant stocker jusqu'√† 1 million de tokens** (via ChromaDB et SQLite). Il est important de distinguer deux notions diff√©rentes :

> ‚ö†Ô∏è **√Ä ne pas confondre**
> - **M√©moire interne (1M tokens)** : ce que l'IA stocke et indexe en local (historique, documents, contexte cumulatif)
> - **Fen√™tre de contexte LLM (4k‚Äì8k tokens)** : ce qui est r√©ellement envoy√© √† Ollama pour chaque g√©n√©ration de r√©ponse (d√©fini par `num_ctx` dans le `Modelfile`)
>
> Le moteur de recherche s√©mantique s√©lectionne les fragments les plus pertinents dans la m√©moire interne, puis les injecte dans la fen√™tre LLM disponible.

### üéØ Chiffres Cl√©s
- **1 000 000 tokens** de capacit√© de stockage en m√©moire vectorielle interne
- **Fen√™tre LLM effective** : 4 096 tokens (d√©faut `Modelfile`) jusqu'√† 8 192 tokens (`local_llm.py`)
- **Compression intelligente** : 2.4:1 √† 52:1 selon le contenu
- **Persistance SQLite** optimis√©e pour les gros volumes
- **Recherche s√©mantique** ultra-rapide (TF-IDF + cosinus)
- **Architecture 100% locale** sans cloud

## üß† Architecture Technique Ultra

### Composants Cl√©s

#### 1. UltraCustomAI (`models/ultra_custom_ai.py`)
```python
from models.ultra_custom_ai import UltraCustomAIModel
from core.ai_engine import AIEngine

# Initialisation Ultra
base_engine = AIEngine()
ultra_ai = UltraCustomAIModel(base_engine)

# G√©n√©ration avec contexte √©tendu
response = ultra_ai.generate_response("Analyse ce projet complet")
print(response['ultra_stats'])  # Statistiques 1M tokens
```

#### 2. IntelligentContextManager (`models/intelligent_context_manager.py`)
```python
from models.intelligent_context_manager import UltraIntelligentContextManager

# Gestionnaire de contexte intelligent
context_mgr = UltraIntelligentContextManager()

# Ajout de contenu avec chunking intelligent
context_mgr.add_content("Document tr√®s long...", "document")

# Recherche s√©mantique ultra-rapide
results = context_mgr.search_relevant_chunks("ma requ√™te", limit=10)
```

#### 3. MillionTokenContextManager (`models/million_token_context_manager.py`)
```python
from models.million_token_context_manager import MillionTokenContextManager

# Persistance et compression
token_mgr = MillionTokenContextManager()

# Ajout de gros volumes avec compression automatique
token_mgr.add_document("gros_document.pdf", auto_compress=True)

# Statistiques en temps r√©el
stats = token_mgr.get_stats()
print(f"Tokens utilis√©s: {stats['current_tokens']:,} / 1,048,576")
```

## ‚ö° Performances et Optimisations

### Syst√®me de Compression Intelligent

| Type de Contenu | Ratio Moyen | Exemple |
|------------------|-------------|---------|
| Texte r√©p√©titif | 15:1 √† 52:1 | Logs, documentation r√©p√©titive |
| Code Python | 3:1 √† 8:1 | Fichiers sources avec commentaires |
| Documents PDF | 4:1 √† 12:1 | Articles, rapports |
| Conversations | 2:1 √† 4:1 | √âchanges utilisateur-IA |

### Base de Donn√©es SQLite Optimis√©e
```sql
-- Structure de la table de contexte
CREATE TABLE context_chunks (
    id TEXT PRIMARY KEY,
    content_hash TEXT,
    compressed_content BLOB,
    tokens INTEGER,
    importance REAL,
    chunk_type TEXT,
    created_at TIMESTAMP,
    last_accessed TIMESTAMP
);

-- Index pour recherche ultra-rapide
CREATE INDEX idx_importance ON context_chunks(importance DESC);
CREATE INDEX idx_type_tokens ON context_chunks(chunk_type, tokens);
```

### Recherche S√©mantique Ultra-Rapide
- **TF-IDF Vectorization** : Analyse s√©mantique des requ√™tes
- **Similarit√© Cosinus** : Matching pr√©cis des chunks pertinents
- **Fallback Intelligent** : Syst√®me sans sklearn pour compatibilit√© totale
- **Cache Adaptatif** : Acc√©l√©ration des requ√™tes r√©p√©t√©es

## üé® Interface Ultra Moderne

### Fonctionnalit√©s Avanc√©es
- **Coloration Syntaxique Python** : Pygments int√©gr√©
- **Formatage Markdown** : Gras, italique, liens cliquables
- **Blocs de Code** : Support natif ```python```
- **Animation de Frappe** : Optimis√©e pour r√©ponses 1M tokens
- **Hauteur Adaptative** : Auto-ajustement selon le contenu
- **Stats en Temps R√©el** : Monitoring du contexte utilis√©

### Interface Graphique Moderne
```python
# Lancement de l'interface Ultra
from interfaces.gui_modern import ModernAIInterface

# Configuration Ultra automatique
interface = ModernAIInterface()
interface.configure_ultra_mode(max_tokens=1048576)
interface.run()
```

## üîß Configuration et Personnalisation

### Configuration Ultra (ultra_config.py)
```python
ULTRA_CONFIG = {
    'max_context_length': 1000000,  # Capacit√© m√©moire vectorielle interne
    'compression_ratio': 'auto',    # Automatique selon contenu
    'chunk_size': 512,              # Taille des chunks
    'overlap_size': 50,             # Recouvrement entre chunks
    'database_path': 'data/ultra_context.db',
    'enable_semantic_search': True,
    'search_algorithm': 'tfidf_cosine',
    'auto_optimize': True,
    'max_memory_mb': 1024
}
```

### Variables d'Environnement
```bash
# Activation du mode Ultra
export ENABLE_ULTRA_MODE=true
export MAX_CONTEXT_TOKENS=1048576
export ULTRA_DB_PATH="data/ultra_context.db"

# Optimisations performances
export ULTRA_CHUNK_SIZE=512
export ULTRA_COMPRESSION="auto"
export ULTRA_SEARCH_ALGO="tfidf_cosine"
```

## üìä Monitoring et Statistiques

### M√©triques Temps R√©el
```python
from models.ultra_custom_ai import UltraCustomAIModel

ultra_ai = UltraCustomAIModel()
stats = ultra_ai.get_context_stats()

print(f"""
üöÄ STATISTIQUES M√âMOIRE VECTORIELLE :
   üìä Tokens en m√©moire interne : {stats['current_tokens']:,} / {stats['max_context_length']:,}
   ‚ö†Ô∏è  Fen√™tre LLM effective (Ollama) : 4 096 ‚Äì 8 192 tokens
   üìÑ Documents trait√©s : {stats['documents_processed']}
   üß© Chunks cr√©√©s : {stats['chunks_created']}
   üóúÔ∏è  Ratio compression : {stats.get('compression_ratio', 'N/A')}
   ‚ö° Temps recherche moyen : {stats.get('avg_search_time_ms', 'N/A')} ms
   üíæ Taille DB : {stats.get('database_size_mb', 'N/A')} MB
""")
```

### Logs et D√©bogage
```python
import logging

# Configuration logging Ultra
logging.basicConfig(
    level=logging.INFO,
    format='[ULTRA] %(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/ultra_1m_tokens.log'),
        logging.StreamHandler()
    ]
)
```

## üõ†Ô∏è D√©pannage et FAQ

### Probl√®mes Courants

#### "M√©moire insuffisante"
```bash
# Solution : Ajuster la taille des chunks
export ULTRA_CHUNK_SIZE=256  # Plus petits chunks
export ULTRA_MAX_MEMORY_MB=2048  # Plus de RAM autoris√©e
```

#### "Base de donn√©es corrompue"
```bash
# R√©initialisation compl√®te
python -c "from models.million_token_context_manager import MillionTokenContextManager; mgr = MillionTokenContextManager(); mgr.reset_database()"
```

#### "Recherche lente"
```bash
# Optimisation des index
python -c "from models.intelligent_context_manager import UltraIntelligentContextManager; mgr = UltraIntelligentContextManager(); mgr.optimize_database()"
```

### Performance Optimale

#### Configuration Recommand√©e
- **RAM** : 16 GB minimum pour utilisation intensive
- **Stockage** : SSD recommand√© (base SQLite)
- **Processeur** : Multi-core pour compression parall√®le
- **Python** : 3.10+ pour performances optimales

#### Bonnes Pratiques
1. **Chunking adaptatif** : Laisser l'auto-d√©tection
2. **Compression automatique** : Mode auto selon le contenu
3. **Maintenance r√©guli√®re** : Nettoyage p√©riodique des chunks anciens
4. **Monitoring continu** : Surveillance de l'usage m√©moire

---

*Documentation g√©n√©r√©e automatiquement le 3 Septembre 2025 pour My Personal AI Ultra v5.0.0*
