# ðŸ“– Guide d'Installation - My Personal AI ULTRA v5.0.0 (1M Tokens RÃ‰EL)

## ðŸŽ¯ Vue d'Ensemble Ultra

My Personal AI Ultra v5.0.0 introduit le **systÃ¨me rÃ©volutionnaire de 1 Million de tokens RÃ‰EL** fonctionnant **entiÃ¨rement localement** sans dÃ©pendance Ã  des services externes. Cette installation vous guide pour mettre en place votre IA ultra-performante, privÃ©e et sÃ©curisÃ©e avec capacitÃ©s Ã©tendues.

## ðŸš€ Installation Express Ultra (7 minutes)

### 1. PrÃ©requis Pour le SystÃ¨me Ultra

- **Python 3.9+** (REQUIS: Python 3.10+ pour performances optimales 1M tokens)
- **~500 MB d'espace disque** (pour la base SQLite et cache intelligent)
- **8 GB RAM minimum** (16 GB recommandÃ© pour 1M tokens)
- **Windows/Linux/macOS** supportÃ©s
- **Aucune connexion internet requise** aprÃ¨s installation
- **Processeur moderne** (pour compression et recherche sÃ©mantique)

### 2. Installation Automatique Ultra

```bash
# 1. Aller dans le rÃ©pertoire du projet
cd My_AI

# 2. Installer TOUTES les dÃ©pendances Ultra (ML inclus)
pip install -r requirements.txt

# DÃ©pendances Ultra spÃ©cifiques :
pip install scikit-learn customtkinter pdfplumber python-docx requests beautifulsoup4 pygments

# 3. Lancement Ultra immÃ©diat
.\launch_ultra.bat   # ðŸš€ NOUVEAU : Lancement mode Ultra
# OU
python launch_ultra.py
```

### 3. VÃ©rification du SystÃ¨me Ultra

```bash
# Test du systÃ¨me 1M tokens
python launch_ultra.py

# VÃ©rification du statut Ultra
python main_ultra.py status

# Test de traitement de gros documents (NOUVEAU)
python main_ultra.py process gros_document.pdf --ultra

# Statistiques contexte 1M tokens
python -c "from models.ultra_custom_ai import UltraCustomAI; print(UltraCustomAI().get_context_stats())"
```

## ðŸ§  Architecture Ultra 1M Tokens 100% Locale

### Nouveaux Composants Ultra

L'IA Ultra utilise exclusivement des composants locaux rÃ©volutionnaires :

- **Moteur IA Custom** : Logique de raisonnement dÃ©veloppÃ©e spÃ©cialement
- **Reconnaissance d'intentions** : Patterns linguistiques locaux
- **MÃ©moire conversationnelle** : Stockage local des contextes
- **Base de connaissances** : Informations encodÃ©es localement
- **Processeurs de documents** : Traitement PDF/DOCX sans cloud

### Aucun Service Externe

âŒ **Pas de dÃ©pendances externes** :
- Pas d'API OpenAI
- Pas de Claude/Anthropic
- Pas de Google Bard
- Pas de services cloud
- Pas d'envoi de donnÃ©es Ã  l'extÃ©rieur

âœ… **Tout reste sur votre machine** :
- Conversations privÃ©es
- Documents confidentiels
- Code source sÃ©curisÃ©
- Historique local uniquement

## ðŸ”§ Configuration AvancÃ©e

### Personnalisation de l'IA

Ã‰ditez le fichier `config.yaml` pour adapter l'IA Ã  vos besoins :

```yaml
# Configuration personnalisÃ©e
ai:
  name: "Mon IA Personnelle"
  max_tokens: 4096
  temperature: 0.7
  conversation_history_limit: 20
  
  # Types de fichiers supportÃ©s
  supported_file_types:
    - ".pdf"
    - ".docx"
    - ".txt"
    - ".py"
    - ".md"
```

### Optimisation des Performances

#### Pour machines avec peu de RAM (4 GB) :
```yaml
ai:
  max_tokens: 2048
  conversation_history_limit: 5
  enable_learning: false
```

#### Pour machines puissantes (16 GB+) :
```yaml
ai:
  max_tokens: 8192
  conversation_history_limit: 50
  enable_learning: true
  advanced_reasoning: true
```

```bash
# ModÃ¨le recommandÃ© (lÃ©ger et performant)
ollama pull llama3.2

# ModÃ¨les alternatifs
ollama pull mistral
ollama pull codellama
ollama pull llama3.2:13b  # Plus gros, plus performant
```

4. **VÃ©rifier le fonctionnement**:

```bash
ollama list
ollama run llama3.2
```

#### Configuration dans l'IA:

L'IA dÃ©tectera automatiquement Ollama. Pour personnaliser:

```yaml
# Dans config.yaml
llm:
  default_backend: "ollama"
  ollama:
    base_url: "http://localhost:11434"
    default_model: "llama3.2"
```

### Option 2: Hugging Face Transformers

Pour utiliser directement les modÃ¨les Transformers:

```bash
# Installation des dÃ©pendances GPU (optionnel)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

#### Configuration:

```yaml
# Dans config.yaml
llm:
  default_backend: "transformers"
  transformers:
    default_model: "microsoft/DialoGPT-medium"
    device: "auto"  # DÃ©tection automatique CPU/GPU
```

## ðŸ“ Structure des RÃ©pertoires

AprÃ¨s le premier lancement, ces rÃ©pertoires seront crÃ©Ã©s:

```
My_AI/
â”œâ”€â”€ outputs/          # Fichiers gÃ©nÃ©rÃ©s
â”œâ”€â”€ logs/            # Fichiers de log
â”œâ”€â”€ temp/            # Fichiers temporaires
â”œâ”€â”€ backups/         # Sauvegardes
â””â”€â”€ models_cache/    # Cache des modÃ¨les (si Transformers)
```

## âš™ï¸ Configuration AvancÃ©e

### Personnalisation du config.yaml

Le fichier `config.yaml` permet de personnaliser tous les aspects:

```yaml
# Exemple de personnalisation
ai:
  temperature: 0.8        # CrÃ©ativitÃ© (0.0-1.0)
  max_tokens: 8192       # Longueur des rÃ©ponses
  
file_processing:
  max_file_size_mb: 100  # Taille max des fichiers

ui:
  cli:
    prompt: "ðŸ¤– MonIA> " # Personnaliser le prompt
```

### Variables d'Environnement

CrÃ©ez un fichier `.env` pour les configurations sensibles:

```bash
# .env
OLLAMA_BASE_URL=http://localhost:11434
OPENAI_API_KEY=your_key_here  # Si vous voulez utiliser OpenAI en fallback
DEBUG=false
```

## ðŸ› RÃ©solution des ProblÃ¨mes

### ProblÃ¨me: "Aucun modÃ¨le LLM disponible"

**Solutions:**
1. VÃ©rifiez qu'Ollama est dÃ©marrÃ©: `ollama serve`
2. VÃ©rifiez qu'un modÃ¨le est installÃ©: `ollama list`
3. Testez la connexion: `curl http://localhost:11434/api/tags`

### ProblÃ¨me: "Import 'torch' could not be resolved"

**Solutions:**
1. Installez PyTorch: `pip install torch`
2. Pour GPU: `pip install torch --index-url https://download.pytorch.org/whl/cu118`

### ProblÃ¨me: "Permission denied" sur Linux/macOS

**Solutions:**
1. Rendez le script exÃ©cutable: `chmod +x main.py`
2. Utilisez Python explicitement: `python main.py`

### ProblÃ¨me: Erreurs de mÃ©moire

**Solutions:**
1. RÃ©duisez `max_tokens` dans la config
2. Utilisez un modÃ¨le plus petit (llama3.2 au lieu de llama3.2:13b)
3. Fermez les autres applications gourmandes

## ðŸ“Š VÃ©rification de l'Installation

### Test Complet:

```bash
# Test du statut
python main.py status

# Test d'une requÃªte simple
python main.py chat "Dis bonjour"

# Test de lecture de fichier
echo "Hello World" > test.txt
python main.py file read test.txt

# Test de gÃ©nÃ©ration de code
python main.py generate code "fonction qui additionne deux nombres"
```

### Mode Debug:

```bash
# Lancer avec plus de logs
python main.py --verbose

# Lancer en mode interactif avec debug
python main.py --verbose --mode cli
```

## ðŸ”„ Mise Ã  Jour

Pour mettre Ã  jour les dÃ©pendances:

```bash
pip install -r requirements.txt --upgrade
```

Pour mettre Ã  jour les modÃ¨les Ollama:

```bash
ollama pull llama3.2  # Re-tÃ©lÃ©charge la derniÃ¨re version
```

## ðŸŽ¯ Optimisation des Performances

### Pour des Performances Maximales:

1. **Utilisez un GPU** (si disponible):
   ```bash
   pip install torch --index-url https://download.pytorch.org/whl/cu118
   ```

2. **Utilisez des modÃ¨les optimisÃ©s**:
   - `llama3.2` : LÃ©ger, rapide
   - `mistral` : Bon compromis
   - `llama3.2:13b` : Plus performant mais plus lourd

3. **Configurez le cache**:
   ```yaml
   performance:
     enable_cache: true
     cache_size_mb: 500
   ```

### Pour des Performances Minimales (PC faible):

1. **Utilisez des modÃ¨les lÃ©gers**
2. **Limitez les tokens**:
   ```yaml
   ai:
     max_tokens: 1024
     temperature: 0.5
   ```

3. **DÃ©sactivez le cache** si peu de RAM:
   ```yaml
   performance:
     enable_cache: false
   ```

## ðŸ“ž Support

Si vous rencontrez des problÃ¨mes:

1. VÃ©rifiez les logs dans `logs/`
2. Lancez avec `--verbose` pour plus d'informations
3. Consultez les exemples dans `examples/`
4. VÃ©rifiez la configuration dans `config.yaml`

## ðŸŽ‰ Vous Ãªtes prÃªt !

Une fois l'installation terminÃ©e, vous pouvez:

- Lancer l'IA: `python main.py`
- Taper `aide` pour voir toutes les commandes
- Commencer Ã  poser des questions Ã  votre IA personnelle !

Bon codage! ðŸš€
