# üìñ Guide d'Installation - My Personal AI v6.0.0

## üéØ Vue d'Ensemble

My Personal AI v6.0.0 est une **IA 100% locale** avec un syst√®me de contexte de **1 Million de tokens R√âEL** fonctionnant enti√®rement sur votre machine sans d√©pendances cloud obligatoires. Cette installation vous guide pour mettre en place votre IA priv√©e et s√©curis√©e.

## ‚ö° Installation Rapide (5 minutes)

### 1. Pr√©requis Syst√®me

**Configuration minimale:**
- **Python 3.8+** (Python 3.10+ recommand√© pour performances optimales)
- **4 GB RAM** (8 GB recommand√©, 16 GB id√©al pour 1M tokens)
- **500 MB d'espace disque** (1 GB recommand√© pour cache et documents)
- **Windows/Linux/macOS** support√©s
- **Connexion internet** (installation uniquement, optionnelle ensuite pour recherche web)

**Configuration recommand√©e:**
- Python 3.10 ou sup√©rieur
- 16 GB RAM
- 2 GB d'espace disque
- Processeur multi-core (pour op√©rations parall√®les)
- GPU (optionnel, pour acc√©l√©ration PyTorch)
- **Ollama** (optionnel mais recommand√© pour des r√©ponses de qualit√© LLM)

### 2. Installation Compl√®te

```bash
# 1. Cloner ou t√©l√©charger le projet
cd My_AI

# 2. Cr√©er un environnement virtuel (recommand√©)
python -m venv venv

# Activer l'environnement virtuel:
# Windows:
venv\Scripts\activate
# Linux/macOS:
source venv/bin/activate

# 3. Installer toutes les d√©pendances
pip install -r requirements.txt

# 4. V√©rifier l'installation
python -c "import customtkinter; print('Installation r√©ussie!')"
```

### 3. Installation Ollama (Optionnel mais Recommand√©)

Ollama permet d'avoir des r√©ponses g√©n√©r√©es par un vrai LLM local (llama3.1:8b).

**√âtape 1 : Installer Ollama**
```bash
# T√©l√©charger depuis https://ollama.com/download
# Installer selon votre OS (Windows, macOS, Linux)
```

**√âtape 2 : T√©l√©charger un mod√®le**
```bash
# Mod√®le recommand√© pour 16 GB RAM
ollama pull llama3.1:8b

# OU mod√®le plus l√©ger pour 8 GB RAM
ollama pull llama3.2
```

**√âtape 3 : Cr√©er le mod√®le personnalis√©**
```bash
# Dans le r√©pertoire My_AI
.\create_custom_model.bat
```

**V√©rifier l'installation :**
```bash
ollama list  # Voir les mod√®les install√©s
ollama run llama3.1:8b "Bonjour"  # Tester
```

> **Note:** Si Ollama n'est pas install√©, l'IA utilisera automatiquement le mode fallback (CustomAIModel avec patterns).

### 4. Lancement Rapide

```bash
# Lancement GUI (recommand√©)
python launch_unified.py

# OU via main.py
python main.py --mode gui

# Lancement CLI
python main.py
```

## üì¶ D√©pendances Principales

### Core Dependencies (Requis)
```
click>=8.0.0              # CLI interface
aiohttp>=3.8.0            # Async HTTP
requests>=2.31.0          # HTTP requests
beautifulsoup4>=4.12.0    # Web scraping
pyyaml>=6.0               # Configuration
python-dotenv>=1.0.0      # Environment variables
```

### Document Processing (Requis)
```
PyMuPDF>=1.23.0          # PDF processing (primaire)
PyPDF2>=3.0.0            # PDF processing (fallback)
python-docx>=0.8.11      # DOCX processing
openpyxl>=3.1.0          # Excel files
python-pptx>=0.6.21      # PowerPoint files
reportlab>=4.0.0         # PDF generation
```

### GUI Interface (Requis pour GUI)
```
customtkinter>=5.2.0     # Modern UI framework
tkinterdnd2>=0.3.0       # Drag-and-drop support
pillow>=10.0.0           # Image processing
pygments                 # Code syntax highlighting
```

### Machine Learning (Requis)
```
torch>=2.0.0             # PyTorch
transformers>=4.30.0     # Hugging Face transformers
scikit-learn>=1.3.0      # ML algorithms
sentence-transformers>=2.2.0  # Embeddings
rapidfuzz                # Fuzzy matching
tiktoken                 # Token counting
```

### Advanced Features (Optionnel)
```
faiss-cpu>=1.7.4         # Semantic search
peft>=0.4.0              # LoRA fine-tuning
bitsandbytes>=0.40.0     # Quantization
sqlalchemy>=2.0.0        # Database ORM
diskcache>=5.6.0         # Caching
cryptography>=41.0.0     # Encryption
```

### Development Tools (Optionnel)
```
pytest>=7.0.0            # Testing
black>=23.0.0            # Code formatting
flake8>=5.5.0            # Linting
isort>=5.12.0            # Import sorting
```

## üèóÔ∏è Structure Projet Post-Installation

Apr√®s le premier lancement, ces r√©pertoires seront cr√©√©s automatiquement:

```
My_AI/
‚îú‚îÄ‚îÄ core/                  # Modules core syst√®me
‚îú‚îÄ‚îÄ models/                # Mod√®les IA
‚îú‚îÄ‚îÄ processors/            # Processeurs documents
‚îú‚îÄ‚îÄ generators/            # G√©n√©rateurs contenu
‚îú‚îÄ‚îÄ interfaces/            # Interfaces utilisateur
‚îú‚îÄ‚îÄ tools/                 # Outils sp√©cialis√©s
‚îú‚îÄ‚îÄ utils/                 # Utilitaires
‚îú‚îÄ‚îÄ data/                  # Donn√©es et enrichissements
‚îÇ   ‚îú‚îÄ‚îÄ enrichissement/    # FAQ et connaissances
‚îÇ   ‚îú‚îÄ‚îÄ context_storage/   # Storage contexte 1M tokens (cr√©√© auto)
‚îÇ   ‚îú‚îÄ‚îÄ outputs/           # Documents g√©n√©r√©s (cr√©√© auto)
‚îÇ   ‚îú‚îÄ‚îÄ temp/              # Fichiers temporaires (cr√©√© auto)
‚îÇ   ‚îú‚îÄ‚îÄ backups/           # Sauvegardes (cr√©√© auto)
‚îÇ   ‚îî‚îÄ‚îÄ logs/              # Logs application (cr√©√© auto)
‚îú‚îÄ‚îÄ docs/                  # Documentation
‚îú‚îÄ‚îÄ examples/              # Exemples utilisation
‚îú‚îÄ‚îÄ tests/                 # Tests unitaires
‚îú‚îÄ‚îÄ config.yaml            # Configuration (cr√©√© auto si absent)
‚îú‚îÄ‚îÄ .env                   # Variables environnement (optionnel)
‚îú‚îÄ‚îÄ requirements.txt       # D√©pendances Python
‚îú‚îÄ‚îÄ launch_unified.py      # Launcher principal
‚îî‚îÄ‚îÄ main.py               # Entry point CLI
```

## ‚öôÔ∏è Configuration

### Fichier config.yaml (Optionnel)

Un fichier `config.yaml` sera cr√©√© automatiquement au premier lancement avec les valeurs par d√©faut. Vous pouvez le personnaliser:

```yaml
# config.yaml - Configuration My Personal AI

# Configuration IA
ai:
  name: "My Personal AI"
  version: "6.0.0"
  max_tokens: 4096          # Max tokens standard
  ultra_max_tokens: 1048576 # Max tokens ultra mode (1M)
  temperature: 0.7
  conversation_history_limit: 10

# Types fichiers support√©s
file_processing:
  supported_types:
    - ".pdf"
    - ".docx"
    - ".txt"
    - ".py"
    - ".js"
    - ".md"
    - ".json"
    - ".csv"
  max_file_size_mb: 100

# Configuration UI
ui:
  cli:
    prompt: "üí¨ Vous> "
    ai_prompt: "ü§ñ IA> "
  gui:
    theme: "dark"
    font_family: "Segoe UI"
    font_size: 11

# R√©pertoires
directories:
  output: "data/outputs"
  temp: "data/temp"
  logs: "data/logs"
  backups: "data/backups"
  context_storage: "data/context_storage"
```

### Variables d'Environnement (.env)

Pour des configurations sensibles ou sp√©cifiques, cr√©ez un fichier `.env`:

```bash
# .env - Variables environnement

# GitHub token pour code generation (optionnel)
GITHUB_TOKEN=your_github_personal_access_token

# Debug mode
DEBUG=false

# Chemins personnalis√©s (optionnel)
DATA_DIR=./data
OUTPUT_DIR=./data/outputs
```

**Note:** Le token GitHub est optionnel. Il am√©liore les capacit√©s de recherche de code mais n'est pas requis pour le fonctionnement de base.

### Configuration Ollama (Modelfile)

Le fichier `Modelfile` √† la racine du projet configure le mod√®le personnalis√© :

```dockerfile
# Modelfile pour My_AI
FROM llama3.1:8b

# Param√®tres
PARAMETER temperature 0.7
PARAMETER num_ctx 8192    # Fen√™tre de contexte

# System prompt personnalis√©
SYSTEM """
Tu es My_AI, un assistant IA personnel expert et bienveillant.
R√©ponds toujours en fran√ßais par d√©faut.
"""
```

**Param√®tres `num_ctx` recommand√©s selon RAM :**
| RAM | num_ctx | Usage |
|-----|---------|-------|
| 8 GB | 4096 | Conversations courtes |
| 16 GB | 8192 | Conversations moyennes |
| 32 GB | 16384 | Gros documents |

## üöÄ Options de Lancement

### 1. GUI Modern (Recommand√©)

Interface graphique moderne avec th√®me sombre style Claude:

```bash
# Lancement direct
python launch_unified.py

# OU via main.py
python main.py --mode gui
```

**Features:**
- Interface chat moderne
- Code syntax highlighting
- Drag-and-drop fichiers
- Timestamps
- Commandes help/status int√©gr√©es

### 2. CLI Enhanced

Interface ligne de commande am√©lior√©e:

```bash
# Mode interactif
python main.py

# OU explicitement
python main.py --mode cli
```

**Commandes disponibles:**
- Requ√™tes normales ‚Üí envoy√©es √† l'IA
- `aide` ou `help` ‚Üí afficher commandes
- `quitter` ou `exit` ‚Üí fermer
- `statut` ou `status` ‚Üí √©tat syst√®me
- `historique` ou `history` ‚Üí voir conversations
- `fichier <path>` ‚Üí traiter fichier
- `generer <type> <desc>` ‚Üí g√©n√©rer contenu

### 3. Direct Queries

Requ√™tes directes sans mode interactif:

```bash
# Chat direct
python main.py chat "Bonjour, comment √ßa va?"

# Analyser fichier
python main.py file analyze path/to/document.pdf

# G√©n√©rer code
python main.py generate code "fonction pour trier une liste"

# Afficher statut
python main.py status
```

## üîß Configuration Avanc√©e

### Installation GPU (CUDA) - Optionnel

Pour acc√©l√©rer les op√©rations PyTorch avec GPU NVIDIA:

```bash
# D√©sinstaller torch CPU
pip uninstall torch torchvision torchaudio

# Installer torch GPU (CUDA 11.8)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# V√©rifier installation GPU
python -c "import torch; print(f'CUDA disponible: {torch.cuda.is_available()}')"
```

**Note:** CUDA doit √™tre install√© sur votre syst√®me. Visitez https://developer.nvidia.com/cuda-downloads

### Optimisation M√©moire

#### Pour machines 4-8 GB RAM:
```yaml
# Dans config.yaml
ai:
  max_tokens: 2048
  conversation_history_limit: 5
  ultra_max_tokens: 524288  # 512K au lieu de 1M
```

#### Pour machines 16+ GB RAM:
```yaml
# Dans config.yaml
ai:
  max_tokens: 8192
  conversation_history_limit: 20
  ultra_max_tokens: 1048576  # Full 1M tokens
```

### Configuration Enrichissement FAQ

Les fichiers d'enrichissement dans `data/enrichissement/` sont charg√©s par priorit√©:

1. `enrichissement_culture.jsonl` (Priorit√© 1)
2. `enrichissement_informatique.jsonl` (Priorit√© 2)
3. `enrichissement_g√©n√©ral.jsonl` (Priorit√© 3)
4. `enrichissement_exemples.jsonl` (Priorit√© 4)

Format JSONL:
```json
{"input": "Question ici", "target": "R√©ponse ici"}
{"input": "Autre question", "target": "Autre r√©ponse"}
```

Pour ajouter vos propres connaissances, √©ditez ces fichiers ou cr√©ez-en de nouveaux.

## üß™ V√©rification Installation

### Test Basique

```bash
# Test imports
python -c "from core.ai_engine import AIEngine; print('Core OK')"
python -c "from models.custom_ai_model import CustomAIModel; print('Models OK')"
python -c "from interfaces.gui_modern import ModernAIGUI; print('GUI OK')"

# Test complet
python tests/test_imports.py
```

### Test Fonctionnel

```bash
# Test requ√™te simple
python main.py chat "Bonjour"

# Test traitement fichier (cr√©er un test.txt d'abord)
echo "Contenu test" > test.txt
python main.py file analyze test.txt

# Test g√©n√©ration code
python main.py generate code "fonction addition"

# Test statut syst√®me
python main.py status
```

### Test Contexte 1M Tokens

```bash
# Test capacit√© contexte
python tests/test_real_1m_tokens.py

# Benchmark performance
python tests/benchmark_1m_tokens.py

# Demo interactive
python tests/demo_1m_tokens.py
```

## üêõ R√©solution Probl√®mes

### Probl√®me: ModuleNotFoundError

**Erreur:** `ModuleNotFoundError: No module named 'customtkinter'`

**Solutions:**
```bash
# V√©rifier environnement virtuel activ√©
# Windows:
venv\Scripts\activate
# Linux/macOS:
source venv/bin/activate

# R√©installer d√©pendances
pip install -r requirements.txt

# Installer module sp√©cifique
pip install customtkinter
```

### Probl√®me: Tkinter non disponible

**Erreur:** `No module named 'tkinter'` ou `_tkinter`

**Solutions:**
- **Windows:** R√©installer Python avec option "tcl/tk" coch√©e
- **Ubuntu/Debian:** `sudo apt-get install python3-tk`
- **macOS:** `brew install python-tk`
- **Fallback:** Utiliser CLI mode ‚Üí `python main.py --mode cli`

### Probl√®me: PyMuPDF installation failed

**Erreur:** Probl√®me compilation PyMuPDF/fitz

**Solutions:**
```bash
# Essayer version sp√©cifique
pip install PyMuPDF==1.23.8

# OU utiliser wheel pre-compil√©
pip install --upgrade pip
pip install --upgrade PyMuPDF

# En dernier recours, PyPDF2 sera utilis√© en fallback
pip install PyPDF2
```

### Probl√®me: M√©moire insuffisante

**Erreur:** `MemoryError` ou application lente

**Solutions:**
1. R√©duire `max_tokens` dans config.yaml
2. Limiter `conversation_history_limit`
3. Fermer applications gourmandes en RAM
4. Utiliser mode CLI au lieu de GUI (moins de RAM)
5. Red√©marrer l'application r√©guli√®rement

### Probl√®me: Erreurs CUDA/GPU

**Erreur:** CUDA errors ou GPU non d√©tect√©

**Solutions:**
```bash
# V√©rifier version CUDA
nvcc --version

# R√©installer torch pour CPU seulement
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio

# L'IA fonctionnera sur CPU (plus lent mais stable)
```

### Probl√®me: Permission denied (Linux/macOS)

**Erreur:** `Permission denied` lors de l'√©criture

**Solutions:**
```bash
# Donner permissions sur r√©pertoires
chmod -R u+w data/
chmod +x launch_unified.py
chmod +x main.py

# OU utiliser sudo (non recommand√©)
sudo python main.py
```

### Probl√®me: Port d√©j√† utilis√©

**Erreur:** Si int√©gration web future - port occup√©

**Solutions:**
```bash
# Trouver processus sur port
# Windows:
netstat -ano | findstr :8000
# Linux/macOS:
lsof -i :8000

# Tuer processus ou changer port dans config
```

## üìä Test Installation Compl√®te

Script de test automatique:

```bash
# Cr√©er test_installation.py
cat > test_installation.py << 'EOF'
#!/usr/bin/env python3
"""Test complet installation My Personal AI"""

def test_imports():
    print("üß™ Test imports modules...")
    try:
        from core.ai_engine import AIEngine
        from models.custom_ai_model import CustomAIModel
        from processors.pdf_processor import PDFProcessor
        from interfaces.cli import EnhancedCLI
        print("‚úÖ Imports OK")
        return True
    except Exception as e:
        print(f"‚ùå Erreur imports: {e}")
        return False

def test_gpu():
    print("\nüß™ Test GPU disponibilit√©...")
    try:
        import torch
        cuda_available = torch.cuda.is_available()
        if cuda_available:
            print(f"‚úÖ GPU disponible: {torch.cuda.get_device_name(0)}")
        else:
            print("‚ö†Ô∏è  GPU non disponible (CPU sera utilis√©)")
        return True
    except:
        print("‚ö†Ô∏è  PyTorch non install√©")
        return True

def test_directories():
    print("\nüß™ Test cr√©ation r√©pertoires...")
    from pathlib import Path
    dirs = ["data/outputs", "data/temp", "data/logs", "data/backups"]
    for dir_path in dirs:
        Path(dir_path).mkdir(parents=True, exist_ok=True)
    print("‚úÖ R√©pertoires OK")
    return True

if __name__ == "__main__":
    print("=" * 50)
    print("  TEST INSTALLATION MY PERSONAL AI v6.0.0")
    print("=" * 50)

    tests = [test_imports(), test_gpu(), test_directories()]

    print("\n" + "=" * 50)
    if all(tests):
        print("‚úÖ Installation compl√®te et fonctionnelle!")
    else:
        print("‚ö†Ô∏è  Probl√®mes d√©tect√©s, voir d√©tails ci-dessus")
    print("=" * 50)
EOF

# Ex√©cuter test
python test_installation.py
```

## üîÑ Mise √† Jour

### Mise √† jour d√©pendances

```bash
# Mettre √† jour toutes les d√©pendances
pip install -r requirements.txt --upgrade

# Mettre √† jour package sp√©cifique
pip install --upgrade customtkinter
pip install --upgrade transformers
```

### Mise √† jour projet

```bash
# Si Git repository
git pull origin main

# R√©installer d√©pendances si requirements.txt chang√©
pip install -r requirements.txt --upgrade
```

## üéØ Optimisation Performances

### Pour Performances Maximales

1. **Utiliser GPU si disponible:**
```bash
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

2. **Augmenter limites config:**
```yaml
ai:
  max_tokens: 8192
  conversation_history_limit: 20
```

3. **Activer cache:**
```yaml
performance:
  enable_cache: true
  cache_size_mb: 1000
```

### Pour Machines Limit√©es

1. **R√©duire consommation m√©moire:**
```yaml
ai:
  max_tokens: 2048
  conversation_history_limit: 5
```

2. **Utiliser CLI au lieu de GUI:**
```bash
python main.py --mode cli
```

3. **D√©sactiver features avanc√©es:**
```yaml
features:
  enable_internet_search: false
  enable_advanced_code_gen: false
```

## üåê Configuration R√©seau (Optionnel)

Pour activer la recherche internet (DuckDuckGo):

```yaml
# Dans config.yaml
internet_search:
  enabled: true
  max_results: 8
  cache_duration: 3600  # 1 heure
  timeout: 10  # secondes
```

**Note:** La recherche internet est optionnelle. L'IA fonctionne 100% localement sans connexion.

## üì± Interface Web (Future/Exp√©rimental)

Pour l'interface web Streamlit (en d√©veloppement):

```bash
# Installer Streamlit
pip install streamlit>=1.25.0

# Lancer interface web
streamlit run interfaces/web_interface.py
```

**Note:** Interface web en cours de d√©veloppement, CLI et GUI sont recommand√©es.

## üéâ Installation Termin√©e!

Une fois l'installation compl√®te, vous pouvez:

1. **Lancer l'IA:**
   ```bash
   python launch_unified.py  # GUI
   python main.py            # CLI
   ```

2. **Tester les capacit√©s:**
   - Poser des questions g√©n√©rales
   - Traiter des documents PDF/DOCX
   - G√©n√©rer du code
   - Rechercher sur internet
   - Utiliser la m√©moire 1M tokens

3. **Explorer les exemples:**
   ```bash
   cd examples
   python basic_usage.py
   python file_processing.py
   ```

4. **Consulter la documentation:**
   - `docs/USAGE.md` - Guide utilisation
   - `docs/ARCHITECTURE.md` - D√©tails architecture
   - `docs/OPTIMIZATION.md` - Optimisations
   - `docs/FAQ.md` - Questions fr√©quentes

## üìû Support

Si vous rencontrez des probl√®mes:

1. **V√©rifier les logs:** `data/logs/`
2. **Mode verbose:** `python main.py --verbose`
3. **Tests diagnostiques:** `python tests/test_imports.py`
4. **Consulter FAQ:** `docs/FAQ.md`
5. **Exemples:** Voir `examples/` pour patterns d'utilisation

---

**Bon codage avec My Personal AI! üöÄ**

*Version: 6.0.0 | Architecture: 100% locale | Capacit√©: 1M tokens*
