#!/usr/bin/env python3
"""
üéØ Benchmark de Fen√™tre de Contexte - My_AI Project
Test sp√©cialis√© pour mesurer les limites et performances du contexte
"""

import os
import sys
import time
import json
import psutil
import gc
from typing import Dict, Any, List, Tuple
from pathlib import Path
from datetime import datetime
import matplotlib.pyplot as plt
import numpy as np

# Ajout du path pour imports locaux
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from models.custom_ai_model import CustomAIModel
from models.conversation_memory import ConversationMemory

class ContextBenchmark:
    """Benchmark sp√©cialis√© pour la fen√™tre de contexte"""
    
    def __init__(self):
        self.results = {}
        self.model = None
        
    def run_comprehensive_benchmark(self) -> Dict[str, Any]:
        """Lance un benchmark complet de la fen√™tre de contexte"""
        print("üéØ Benchmark Complet de la Fen√™tre de Contexte")
        print("=" * 60)
        
        # 1. Test de mont√©e en charge progressive
        self.results["scale_test"] = self._test_progressive_scaling()
        
        # 2. Test de types de contenu diff√©rents
        self.results["content_type_test"] = self._test_content_types()
        
        # 3. Test de performance m√©moire
        self.results["memory_performance"] = self._test_memory_efficiency()
        
        # 4. Test de stabilit√© long-terme
        self.results["stability_test"] = self._test_long_term_stability()
        
        # 5. G√©n√©rer recommandations
        self.results["recommendations"] = self._generate_context_recommendations()
        
        # 6. Sauvegarder et visualiser
        self._save_benchmark_results()
        self._generate_performance_plots()
        
        return self.results
    
    def _test_progressive_scaling(self) -> Dict[str, Any]:
        """Test de mont√©e en charge progressive du contexte"""
        print("üìà Test de mont√©e en charge progressive...")
        
        self.model = CustomAIModel(ConversationMemory())
        
        # Tailles de test exponentielles
        test_sizes = [128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]
        results = []
        
        base_text = "L'intelligence artificielle √©volue rapidement. "
        
        for size in test_sizes:
            print(f"  üîÑ Test {size} tokens...")
            
            # G√©n√©rer le texte de test
            repetitions = max(1, size // len(base_text))
            test_content = base_text * repetitions
            
            # Monitoring des ressources
            process = psutil.Process()
            cpu_before = process.cpu_percent()
            memory_before = process.memory_info().rss / (1024 * 1024)  # MB
            
            start_time = time.time()
            gc_before = len(gc.get_objects())
            
            try:
                response = self.model.generate_response(
                    f"Fais un r√©sum√© structur√© de ce texte: {test_content}", 
                    {}
                )
                
                end_time = time.time()
                processing_time = end_time - start_time
                
                # Mesures post-traitement
                cpu_after = process.cpu_percent()
                memory_after = process.memory_info().rss / (1024 * 1024)  # MB
                gc_after = len(gc.get_objects())
                
                # Calculer les m√©triques
                tokens_per_second = size / processing_time if processing_time > 0 else 0
                memory_efficiency = size / (memory_after - memory_before) if memory_after > memory_before else float('inf')
                
                result = {
                    "context_size": size,
                    "actual_tokens": len(test_content.split()),
                    "processing_time": processing_time,
                    "tokens_per_second": tokens_per_second,
                    "memory_before_mb": memory_before,
                    "memory_after_mb": memory_after,
                    "memory_delta_mb": memory_after - memory_before,
                    "memory_efficiency": memory_efficiency,
                    "cpu_usage": cpu_after,
                    "gc_objects_delta": gc_after - gc_before,
                    "response_length": len(response) if response else 0,
                    "success": True,
                    "quality_score": self._assess_response_quality(response, test_content)
                }
                
                results.append(result)
                print(f"    ‚úÖ {processing_time:.2f}s, {tokens_per_second:.0f} tok/s, {memory_after - memory_before:.1f}MB")
                
                # Nettoyage m√©moire
                gc.collect()
                
            except Exception as e:
                result = {
                    "context_size": size,
                    "processing_time": -1,
                    "success": False,
                    "error": str(e),
                    "memory_before_mb": memory_before
                }
                results.append(result)
                print(f"    ‚ùå √âchec: {str(e)}")
                break
        
        # Analyser les r√©sultats
        successful_results = [r for r in results if r["success"]]
        
        analysis = {
            "max_successful_size": max([r["context_size"] for r in successful_results]) if successful_results else 0,
            "optimal_size": self._find_optimal_context_size(successful_results),
            "performance_curve": successful_results,
            "bottleneck_analysis": self._analyze_bottlenecks(successful_results)
        }
        
        return analysis
    
    def _test_content_types(self) -> Dict[str, Any]:
        """Test avec diff√©rents types de contenu"""
        print("üìù Test de types de contenu diff√©rents...")
        
        content_types = {
            "code": {
                "content": "def fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\n\n" * 100,
                "description": "Code Python r√©p√©titif"
            },
            "natural_text": {
                "content": "L'intelligence artificielle est une technologie r√©volutionnaire qui transforme notre soci√©t√©. Elle permet d'automatiser des t√¢ches complexes et d'am√©liorer notre productivit√©. " * 100,
                "description": "Texte naturel"
            },
            "structured_data": {
                "content": '{"name": "Jean", "age": 30, "skills": ["Python", "JavaScript", "React"]}\n' * 200,
                "description": "Donn√©es JSON structur√©es"
            },
            "mixed_content": {
                "content": "# Documentation\n\nVoici du code:\n```python\nprint('hello')\n```\n\nEt du texte explicatif.\n" * 50,
                "description": "Contenu mixte (markdown, code, texte)"
            }
        }
        
        results = {}
        
        for content_type, data in content_types.items():
            print(f"  üîç Test {content_type}...")
            
            start_time = time.time()
            memory_before = psutil.Process().memory_info().rss / (1024 * 1024)
            
            try:
                response = self.model.generate_response(
                    f"Analyse ce contenu {data['description']}: {data['content']}", 
                    {}
                )
                
                end_time = time.time()
                memory_after = psutil.Process().memory_info().rss / (1024 * 1024)
                
                results[content_type] = {
                    "content_length": len(data["content"]),
                    "estimated_tokens": len(data["content"].split()),
                    "processing_time": end_time - start_time,
                    "memory_delta_mb": memory_after - memory_before,
                    "response_length": len(response) if response else 0,
                    "success": True,
                    "efficiency": len(data["content"]) / (end_time - start_time) if end_time > start_time else 0
                }
                
                print(f"    ‚úÖ {end_time - start_time:.2f}s, {memory_after - memory_before:.1f}MB")
                
            except Exception as e:
                results[content_type] = {
                    "success": False,
                    "error": str(e)
                }
                print(f"    ‚ùå Erreur: {str(e)}")
        
        return results
    
    def _test_memory_efficiency(self) -> Dict[str, Any]:
        """Test d'efficacit√© m√©moire avec monitoring d√©taill√©"""
        print("üíæ Test d'efficacit√© m√©moire...")
        
        # Test avec contexte croissant
        memory_timeline = []
        context_accumulation = ""
        
        for i in range(10):
            # Ajouter du contenu au contexte
            new_content = f"Partie {i+1}: L'IA doit comprendre et retenir cette information importante. " * 50
            context_accumulation += new_content
            
            # Mesurer avant traitement
            memory_before = psutil.Process().memory_info().rss / (1024 * 1024)
            
            start_time = time.time()
            
            try:
                response = self.model.generate_response(
                    f"R√©sume tout le contexte accumul√©: {context_accumulation}", 
                    {}
                )
                
                end_time = time.time()
                memory_after = psutil.Process().memory_info().rss / (1024 * 1024)
                
                memory_timeline.append({
                    "iteration": i + 1,
                    "context_size": len(context_accumulation),
                    "estimated_tokens": len(context_accumulation.split()),
                    "memory_before_mb": memory_before,
                    "memory_after_mb": memory_after,
                    "memory_delta_mb": memory_after - memory_before,
                    "processing_time": end_time - start_time,
                    "response_length": len(response) if response else 0
                })
                
                print(f"  Iteration {i+1}: {memory_after - memory_before:.1f}MB delta")
                
                # Forcer le garbage collection
                gc.collect()
                
            except Exception as e:
                memory_timeline.append({
                    "iteration": i + 1,
                    "error": str(e),
                    "context_size": len(context_accumulation)
                })
                break
        
        # Analyser les fuites m√©moire potentielles
        successful_iterations = [m for m in memory_timeline if "error" not in m]
        
        if len(successful_iterations) > 3:
            memory_growth_rate = (
                successful_iterations[-1]["memory_after_mb"] - 
                successful_iterations[0]["memory_after_mb"]
            ) / len(successful_iterations)
        else:
            memory_growth_rate = 0
        
        return {
            "memory_timeline": memory_timeline,
            "memory_growth_rate_mb": memory_growth_rate,
            "memory_leak_detected": memory_growth_rate > 10,  # Plus de 10MB par it√©ration
            "total_iterations": len(memory_timeline),
            "max_stable_context": self._find_max_stable_context(memory_timeline)
        }
    
    def _test_long_term_stability(self) -> Dict[str, Any]:
        """Test de stabilit√© sur de longues sessions"""
        print("üîÑ Test de stabilit√© long-terme...")
        
        stability_results = []
        
        # Simuler une session longue avec diff√©rents types de requ√™tes
        test_scenarios = [
            "Bonjour, comment √ßa va ?",
            "Explique-moi les algorithmes de tri",
            "G√©n√®re du code Python pour un serveur web",
            "Quelle est la diff√©rence entre liste et dictionnaire ?",
            "Raconte-moi une blague",
            "Analyse ce code: def test(): return 'hello'",
            "Recherche sur internet les nouvelles en IA",
            "R√©sume un document technique complexe sur l'apprentissage automatique"
        ]
        
        # R√©p√©ter le cycle plusieurs fois
        for cycle in range(5):
            print(f"  üîÑ Cycle {cycle + 1}/5...")
            
            cycle_start_time = time.time()
            cycle_memory_start = psutil.Process().memory_info().rss / (1024 * 1024)
            
            for i, scenario in enumerate(test_scenarios):
                try:
                    start_time = time.time()
                    response = self.model.generate_response(scenario, {})
                    end_time = time.time()
                    
                    stability_results.append({
                        "cycle": cycle + 1,
                        "scenario": i + 1,
                        "scenario_text": scenario[:50] + "...",
                        "processing_time": end_time - start_time,
                        "response_length": len(response) if response else 0,
                        "success": True
                    })
                    
                except Exception as e:
                    stability_results.append({
                        "cycle": cycle + 1,
                        "scenario": i + 1,
                        "error": str(e),
                        "success": False
                    })
            
            cycle_end_time = time.time()
            cycle_memory_end = psutil.Process().memory_info().rss / (1024 * 1024)
            
            print(f"    Cycle {cycle + 1}: {cycle_end_time - cycle_start_time:.1f}s, "
                  f"Œîmem: {cycle_memory_end - cycle_memory_start:.1f}MB")
            
            # Pause entre cycles
            time.sleep(1)
        
        # Analyser la stabilit√©
        successful_tests = [r for r in stability_results if r["success"]]
        
        if successful_tests:
            processing_times = [r["processing_time"] for r in successful_tests]
            avg_time = sum(processing_times) / len(processing_times)
            time_variance = np.var(processing_times) if len(processing_times) > 1 else 0
            
            stability_score = 1.0 / (1.0 + time_variance)  # Score de stabilit√©
        else:
            avg_time = 0
            time_variance = 0
            stability_score = 0
        
        return {
            "total_tests": len(stability_results),
            "successful_tests": len(successful_tests),
            "success_rate": len(successful_tests) / len(stability_results) if stability_results else 0,
            "avg_processing_time": avg_time,
            "time_variance": time_variance,
            "stability_score": stability_score,
            "detailed_results": stability_results
        }
    
    def _test_content_types(self) -> Dict[str, Any]:
        """Test avec diff√©rents types de contenu pour identifier les goulots d'√©tranglement"""
        print("üìã Test de types de contenu sp√©cialis√©s...")
        
        content_templates = {
            "repetitive_code": {
                "template": "def function_{i}():\n    return {i} * 2\n\n",
                "multiplier": 100,
                "description": "Code r√©p√©titif"
            },
            "diverse_text": {
                "template": "Chapitre {i}: L'intelligence artificielle dans le domaine num√©ro {i} r√©volutionne les m√©thodes traditionnelles. ",
                "multiplier": 50,
                "description": "Texte diversifi√©"
            },
            "json_data": {
                "template": '{{"id": {i}, "data": "information_{i}", "value": {i}}},\n',
                "multiplier": 200,
                "description": "Donn√©es JSON"
            },
            "mixed_markdown": {
                "template": "## Section {i}\n\n```python\nprint('code_{i}')\n```\n\nTexte explicatif {i}.\n\n",
                "multiplier": 30,
                "description": "Markdown mixte"
            }
        }
        
        results = {}
        
        for content_type, config in content_templates.items():
            print(f"  üìÑ Test {content_type}...")
            
            # G√©n√©rer le contenu
            content = ""
            for i in range(config["multiplier"]):
                content += config["template"].format(i=i)
            
            # Test de performance
            start_time = time.time()
            memory_before = psutil.Process().memory_info().rss / (1024 * 1024)
            
            try:
                response = self.model.generate_response(
                    f"Analyse ce {config['description']}: {content}", 
                    {}
                )
                
                end_time = time.time()
                memory_after = psutil.Process().memory_info().rss / (1024 * 1024)
                
                results[content_type] = {
                    "content_length": len(content),
                    "estimated_tokens": len(content.split()),
                    "processing_time": end_time - start_time,
                    "tokens_per_second": len(content.split()) / (end_time - start_time) if end_time > start_time else 0,
                    "memory_delta_mb": memory_after - memory_before,
                    "response_length": len(response) if response else 0,
                    "compression_ratio": len(response) / len(content) if response and content else 0,
                    "success": True
                }
                
                print(f"    ‚úÖ {end_time - start_time:.2f}s, compression: {len(response) / len(content):.2%}")
                
            except Exception as e:
                results[content_type] = {
                    "success": False,
                    "error": str(e),
                    "content_length": len(content)
                }
                print(f"    ‚ùå Erreur: {str(e)}")
        
        return results
    
    def _test_memory_efficiency(self) -> Dict[str, Any]:
        """Test approfondi de l'efficacit√© m√©moire"""
        print("üß† Test d'efficacit√© m√©moire avanc√©...")
        
        # Test de charge m√©moire avec monitoring continu
        memory_snapshots = []
        context_sizes = [1024, 2048, 4096, 8192]
        
        for size in context_sizes:
            # Cr√©er un contexte de taille sp√©cifique
            test_text = "Information importante num√©ro " + " ".join([str(i) for i in range(size)])
            
            # Prendre plusieurs snapshots pendant le traitement
            snapshots = []
            
            def memory_monitor():
                for _ in range(20):  # Monitor pendant 2 secondes
                    snapshots.append({
                        "timestamp": time.time(),
                        "memory_mb": psutil.Process().memory_info().rss / (1024 * 1024)
                    })
                    time.sleep(0.1)
            
            # D√©marrer monitoring en parall√®le
            import threading
            monitor_thread = threading.Thread(target=memory_monitor)
            
            start_time = time.time()
            monitor_thread.start()
            
            try:
                response = self.model.generate_response(f"Traite ce contexte: {test_text}", {})
                end_time = time.time()
                success = True
            except Exception as e:
                end_time = time.time()
                success = False
                response = None
            
            monitor_thread.join()
            
            memory_snapshots.append({
                "context_size": size,
                "success": success,
                "processing_time": end_time - start_time,
                "memory_timeline": snapshots,
                "peak_memory": max([s["memory_mb"] for s in snapshots]) if snapshots else 0,
                "memory_growth": snapshots[-1]["memory_mb"] - snapshots[0]["memory_mb"] if len(snapshots) > 1 else 0
            })
            
            print(f"    Context {size}: {'‚úÖ' if success else '‚ùå'} "
                  f"Peak: {max([s['memory_mb'] for s in snapshots]) if snapshots else 0:.1f}MB")
        
        return {
            "memory_snapshots": memory_snapshots,
            "memory_leak_detected": self._detect_memory_leaks(memory_snapshots),
            "optimal_memory_usage": self._calculate_optimal_memory_usage(memory_snapshots)
        }
    
    def _test_long_term_stability(self) -> Dict[str, Any]:
        """Test de stabilit√© sur une longue p√©riode"""
        print("‚è∞ Test de stabilit√© long-terme...")
        
        # Simuler 100 interactions cons√©cutives
        interactions = []
        base_queries = [
            "Salut",
            "Comment faire une boucle en Python?",
            "G√©n√®re du code HTML",
            "Explique les classes Python"
        ]
        
        for i in range(100):
            query = base_queries[i % len(base_queries)] + f" (it√©ration {i+1})"
            
            start_time = time.time()
            memory_before = psutil.Process().memory_info().rss / (1024 * 1024)
            
            try:
                response = self.model.generate_response(query, {})
                end_time = time.time()
                memory_after = psutil.Process().memory_info().rss / (1024 * 1024)
                
                interactions.append({
                    "iteration": i + 1,
                    "processing_time": end_time - start_time,
                    "memory_mb": memory_after,
                    "memory_delta": memory_after - memory_before,
                    "success": True
                })
                
                if (i + 1) % 20 == 0:
                    print(f"    ‚úÖ {i + 1}/100 interactions, m√©moire: {memory_after:.1f}MB")
                
            except Exception as e:
                interactions.append({
                    "iteration": i + 1,
                    "error": str(e),
                    "success": False
                })
                print(f"    ‚ùå √âchec √† l'it√©ration {i + 1}: {str(e)}")
        
        # Analyser la stabilit√©
        successful_interactions = [i for i in interactions if i["success"]]
        
        if successful_interactions:
            # Tendance de performance
            early_times = [i["processing_time"] for i in successful_interactions[:20]]
            late_times = [i["processing_time"] for i in successful_interactions[-20:]]
            
            performance_degradation = (
                (sum(late_times) / len(late_times)) - (sum(early_times) / len(early_times))
            ) / (sum(early_times) / len(early_times)) if early_times else 0
            
            # Tendance m√©moire
            memory_values = [i["memory_mb"] for i in successful_interactions]
            memory_trend = (memory_values[-1] - memory_values[0]) / len(memory_values) if len(memory_values) > 1 else 0
        else:
            performance_degradation = 0
            memory_trend = 0
        
        return {
            "total_interactions": len(interactions),
            "successful_interactions": len(successful_interactions),
            "success_rate": len(successful_interactions) / len(interactions) if interactions else 0,
            "performance_degradation": performance_degradation,
            "memory_trend_mb_per_interaction": memory_trend,
            "stability_rating": self._calculate_stability_rating(interactions)
        }
    
    def _assess_response_quality(self, response: str, original_content: str) -> float:
        """√âvalue la qualit√© de la r√©ponse (0-1)"""
        if not response:
            return 0.0
        
        # M√©triques simples de qualit√©
        length_ratio = len(response) / len(original_content) if original_content else 0
        
        # Score bas√© sur la longueur relative et la pr√©sence de contenu structur√©
        quality_indicators = 0
        if 0.1 <= length_ratio <= 0.8:  # Bon ratio de compression
            quality_indicators += 0.3
        if len(response.split()) > 10:  # R√©ponse substantielle
            quality_indicators += 0.3
        if any(marker in response for marker in ["**", "*", "\n", ":"]):  # Formatage
            quality_indicators += 0.2
        if len(response.split('.')) > 2:  # Phrases multiples
            quality_indicators += 0.2
        
        return min(1.0, quality_indicators)
    
    def _find_optimal_context_size(self, results: List[Dict]) -> int:
        """Trouve la taille de contexte optimale bas√©e sur le ratio performance/m√©moire"""
        if not results:
            return 0
        
        # Calculer un score d'efficacit√© pour chaque taille
        efficiency_scores = []
        
        for result in results:
            if result["success"]:
                # Score bas√© sur tokens/seconde et efficacit√© m√©moire
                speed_score = result["tokens_per_second"] / 1000  # Normaliser
                memory_score = 1.0 / (result["memory_delta_mb"] + 1)  # Inverser (moins = mieux)
                
                efficiency_score = (speed_score + memory_score) / 2
                efficiency_scores.append((result["context_size"], efficiency_score))
        
        if efficiency_scores:
            # Retourner la taille avec le meilleur score d'efficacit√©
            optimal = max(efficiency_scores, key=lambda x: x[1])
            return optimal[0]
        
        return 0
    
    def _analyze_bottlenecks(self, results: List[Dict]) -> Dict[str, Any]:
        """Analyse les goulots d'√©tranglement"""
        if not results:
            return {}
        
        # Identifier o√π les performances se d√©gradent
        bottlenecks = {
            "memory_bottleneck": None,
            "speed_bottleneck": None,
            "critical_size": None
        }
        
        # D√©tecter la d√©gradation de vitesse
        for i in range(1, len(results)):
            prev = results[i-1]
            curr = results[i]
            
            if prev["tokens_per_second"] > 0 and curr["tokens_per_second"] > 0:
                speed_degradation = (prev["tokens_per_second"] - curr["tokens_per_second"]) / prev["tokens_per_second"]
                
                if speed_degradation > 0.3 and not bottlenecks["speed_bottleneck"]:  # 30% de d√©gradation
                    bottlenecks["speed_bottleneck"] = curr["context_size"]
            
            # D√©tecter l'explosion m√©moire
            if curr["memory_delta_mb"] > prev["memory_delta_mb"] * 2 and not bottlenecks["memory_bottleneck"]:
                bottlenecks["memory_bottleneck"] = curr["context_size"]
        
        return bottlenecks
    
    def _find_max_stable_context(self, memory_timeline: List[Dict]) -> int:
        """Trouve la taille de contexte stable maximale"""
        stable_iterations = [m for m in memory_timeline if m.get("success") and m.get("memory_delta_mb", 0) < 50]
        
        if stable_iterations:
            return max(m["estimated_tokens"] for m in stable_iterations)
        return 0
    
    def _detect_memory_leaks(self, snapshots: List[Dict]) -> bool:
        """D√©tecte les fuites m√©moire potentielles"""
        successful_snapshots = [s for s in snapshots if s["success"]]
        
        if len(successful_snapshots) < 3:
            return False
        
        # V√©rifier si la m√©moire continue de cro√Ætre
        memory_growth = successful_snapshots[-1]["peak_memory"] - successful_snapshots[0]["peak_memory"]
        return memory_growth > 100  # Plus de 100MB de croissance = fuite potentielle
    
    def _calculate_optimal_memory_usage(self, snapshots: List[Dict]) -> Dict[str, float]:
        """Calcule l'utilisation m√©moire optimale"""
        successful_snapshots = [s for s in snapshots if s["success"]]
        
        if not successful_snapshots:
            return {}
        
        # Efficacit√© m√©moire = tokens trait√©s / MB utilis√©
        efficiency_ratios = []
        for snapshot in successful_snapshots:
            if snapshot["memory_growth"] > 0:
                efficiency = snapshot["context_size"] / snapshot["memory_growth"]
                efficiency_ratios.append(efficiency)
        
        return {
            "avg_efficiency": sum(efficiency_ratios) / len(efficiency_ratios) if efficiency_ratios else 0,
            "best_efficiency": max(efficiency_ratios) if efficiency_ratios else 0,
            "recommended_chunk_size": self._recommend_chunk_size(successful_snapshots)
        }
    
    def _recommend_chunk_size(self, snapshots: List[Dict]) -> int:
        """Recommande une taille de chunk optimale"""
        # Trouver le sweet spot entre performance et m√©moire
        if not snapshots:
            return 2048  # Valeur par d√©faut
        
        # Chercher la taille o√π le ratio performance/m√©moire est optimal
        best_ratio = 0
        best_size = 2048
        
        for snapshot in snapshots:
            if snapshot["memory_growth"] > 0 and snapshot["processing_time"] > 0:
                ratio = snapshot["context_size"] / (snapshot["memory_growth"] * snapshot["processing_time"])
                if ratio > best_ratio:
                    best_ratio = ratio
                    best_size = snapshot["context_size"]
        
        return best_size
    
    def _calculate_stability_rating(self, interactions: List[Dict]) -> str:
        """Calcule une note de stabilit√©"""
        if not interactions:
            return "Unknown"
        
        success_rate = len([i for i in interactions if i["success"]]) / len(interactions)
        
        if success_rate >= 0.95:
            return "Excellent"
        elif success_rate >= 0.85:
            return "Good"
        elif success_rate >= 0.70:
            return "Fair"
        else:
            return "Poor"
    
    def _generate_context_recommendations(self) -> Dict[str, Any]:
        """G√©n√®re des recommandations sp√©cifiques au contexte"""
        recommendations = {
            "immediate_actions": [],
            "optimization_techniques": [],
            "implementation_priority": []
        }
        
        # Analyser les r√©sultats pour g√©n√©rer des recommandations
        scale_test = self.results.get("scale_test", {})
        memory_test = self.results.get("memory_performance", {})
        
        # Recommandations bas√©es sur la taille maximale
        max_size = scale_test.get("max_successful_size", 0)
        if max_size < 4096:
            recommendations["immediate_actions"].append(
                f"Contexte limit√© √† {max_size} tokens - Impl√©menter RAG prioritaire"
            )
        elif max_size < 8192:
            recommendations["immediate_actions"].append(
                "Contexte mod√©r√© - Optimiser avec chunking intelligent"
            )
        
        # Recommandations bas√©es sur la m√©moire
        if memory_test.get("memory_leak_detected"):
            recommendations["immediate_actions"].append(
                "Fuite m√©moire d√©tect√©e - Impl√©menter garbage collection optimis√©"
            )
        
        # Techniques d'optimisation recommand√©es
        recommendations["optimization_techniques"] = [
            "RAG avec FAISS pour recherche s√©mantique",
            "Context sliding window avec r√©sum√©s",
            "Chunking adaptatif bas√© sur la s√©mantique",
            "Cache des embeddings pour r√©utilisation",
            "Compression de contexte avec mod√®les d√©di√©s"
        ]
        
        # Priorit√©s d'impl√©mentation
        recommendations["implementation_priority"] = [
            {"technique": "RAG Pipeline", "priority": 1, "impact": "High", "effort": "Medium"},
            {"technique": "Context Chunking", "priority": 2, "impact": "High", "effort": "Low"},
            {"technique": "Memory Optimization", "priority": 3, "impact": "Medium", "effort": "Low"},
            {"technique": "FlashAttention", "priority": 4, "impact": "High", "effort": "High"},
            {"technique": "Model Quantization", "priority": 5, "impact": "Medium", "effort": "Medium"}
        ]
        
        return recommendations
    
    def _save_benchmark_results(self):
        """Sauvegarde les r√©sultats d√©taill√©s"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Sauvegarde JSON d√©taill√©e
        report_path = f"context_benchmark_{timestamp}.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"üìÑ Benchmark sauvegard√©: {report_path}")
    
    def _generate_performance_plots(self):
        """G√©n√®re des graphiques de performance"""
        try:
            scale_test = self.results.get("scale_test", {})
            performance_curve = scale_test.get("performance_curve", [])
            
            if not performance_curve:
                print("‚ö†Ô∏è Pas de donn√©es pour les graphiques")
                return
            
            # Cr√©er les graphiques
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
            
            sizes = [r["context_size"] for r in performance_curve]
            times = [r["processing_time"] for r in performance_curve]
            tokens_per_sec = [r["tokens_per_second"] for r in performance_curve]
            memory_deltas = [r.get("memory_delta_mb", 0) for r in performance_curve]
            
            # Graphique 1: Temps de traitement vs taille contexte
            ax1.plot(sizes, times, 'b-o', linewidth=2, markersize=6)
            ax1.set_xlabel('Taille du Contexte (tokens)')
            ax1.set_ylabel('Temps de Traitement (s)')
            ax1.set_title('Performance vs Taille du Contexte')
            ax1.grid(True, alpha=0.3)
            
            # Graphique 2: Throughput
            ax2.plot(sizes, tokens_per_sec, 'g-o', linewidth=2, markersize=6)
            ax2.set_xlabel('Taille du Contexte (tokens)')
            ax2.set_ylabel('Tokens/Seconde')
            ax2.set_title('Throughput du Mod√®le')
            ax2.grid(True, alpha=0.3)
            
            # Graphique 3: Utilisation m√©moire
            ax3.plot(sizes, memory_deltas, 'r-o', linewidth=2, markersize=6)
            ax3.set_xlabel('Taille du Contexte (tokens)')
            ax3.set_ylabel('Delta M√©moire (MB)')
            ax3.set_title('Consommation M√©moire')
            ax3.grid(True, alpha=0.3)
            
            # Graphique 4: Efficacit√© (tokens/s par MB)
            efficiency = [t/m if m > 0 else 0 for t, m in zip(tokens_per_sec, memory_deltas)]
            ax4.plot(sizes, efficiency, 'm-o', linewidth=2, markersize=6)
            ax4.set_xlabel('Taille du Contexte (tokens)')
            ax4.set_ylabel('Efficacit√© (tokens/s/MB)')
            ax4.set_title('Efficacit√© Globale')
            ax4.grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # Sauvegarder
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            plot_path = f"context_performance_{timestamp}.png"
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            print(f"üìä Graphiques sauvegard√©s: {plot_path}")
            
        except ImportError:
            print("‚ö†Ô∏è matplotlib non disponible - pas de graphiques g√©n√©r√©s")
        except Exception as e:
            print(f"‚ùå Erreur g√©n√©ration graphiques: {str(e)}")

def main():
    """Point d'entr√©e principal du benchmark"""
    benchmark = ContextBenchmark()
    results = benchmark.run_comprehensive_benchmark()
    
    # Affichage du r√©sum√©
    print("\n" + "=" * 60)
    print("üìä R√âSUM√â DU BENCHMARK CONTEXTE")
    print("=" * 60)
    
    # R√©sultats de mont√©e en charge
    scale_test = results.get("scale_test", {})
    max_size = scale_test.get("max_successful_size", 0)
    optimal_size = scale_test.get("optimal_size", 0)
    
    print(f"üéØ Taille maximale test√©e avec succ√®s: {max_size:,} tokens")
    print(f"üèÜ Taille optimale recommand√©e: {optimal_size:,} tokens")
    
    # Performance par type de contenu
    content_test = results.get("content_type_test", {})
    if content_test:
        print(f"\nüìã Performance par type de contenu:")
        for content_type, data in content_test.items():
            if data.get("success"):
                print(f"   {content_type}: {data['tokens_per_second']:.0f} tok/s")
    
    # Stabilit√©
    stability = results.get("stability_test", {})
    success_rate = stability.get("success_rate", 0)
    stability_rating = stability.get("stability_rating", "Unknown")
    
    print(f"\nüîÑ Stabilit√©: {success_rate:.1%} succ√®s, Note: {stability_rating}")
    
    # Recommandations
    recs = results.get("recommendations", {})
    immediate = recs.get("immediate_actions", [])
    if immediate:
        print(f"\nüö® Actions Imm√©diates Recommand√©es:")
        for i, action in enumerate(immediate, 1):
            print(f"   {i}. {action}")
    
    return results

if __name__ == "__main__":
    main()
